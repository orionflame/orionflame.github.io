========================================

Pragmatic VEX: Volume 1 | Codex

========================================





----------------------------------------------------------------------------------------------------

00 - Installation

----------------------------------------------------------------------------------------------------



00:00:02,660 --> 00:00:06,379
I will show you how to download the files
using the custom Python script I wrote.


00:00:07,630 --> 00:00:12,380
One thing to note is, at least on Windows,
you have to make sure you have Python installed


00:00:12,380 --> 00:00:14,800
along with the pip module.


00:00:14,800 --> 00:00:19,510
Upon completing your purchase, you will be
given 2 Google Drive links.


00:00:19,510 --> 00:00:23,830
One of them is a text file that contains all
of the Google Drive links.


00:00:23,830 --> 00:00:28,470
Here I renamed it to my_download_links, but
each buyer will have a different text file


00:00:28,470 --> 00:00:30,189
with a unique name.


00:00:30,189 --> 00:00:35,819
The script processes the first text file it
finds within the same folder, and as such


00:00:35,819 --> 00:00:39,429
it will work regardless of the name of the
text file.


00:00:39,429 --> 00:00:44,339
It also briefly mentions this method along
with another method that involves using a


00:00:44,339 --> 00:00:47,489
download manager like jdownloader.


00:00:47,489 --> 00:00:51,769
If you can download the files using the latter
method, then you don't have to use this method


00:00:51,769 --> 00:00:53,239
described here.


00:00:53,239 --> 00:00:57,679
But if you are having issues with your download
manager to download files from Google Drive,


00:00:57,679 --> 00:01:00,519
then please follow the steps described in
this video.


00:01:00,519 --> 00:01:05,470
As you can see, the text file includes all
of the Google Drive links included in the


00:01:05,470 --> 00:01:12,200
course, as each file is individually generated
to embed unique watermark data for each buyer.


00:01:12,200 --> 00:01:17,689
As the files are not located inside a single
folder, it's not possible to download everything


00:01:17,689 --> 00:01:19,460
from a single folder.


00:01:19,460 --> 00:01:24,400
Here I removed most of the links, so the download
process finishes faster.


00:01:24,400 --> 00:01:28,710
Otherwise it would take quite a long time
to download the entire course.


00:01:28,710 --> 00:01:32,380
So please be patient while the download process
is running.


00:01:32,380 --> 00:01:37,619
The second file is the zip file that contains
the Python script and the credentials file


00:01:37,619 --> 00:01:40,140
to download the course files.


00:01:40,140 --> 00:01:45,340
First extract the zip file and then copy the
3 files inside any folder, where you want


00:01:45,340 --> 00:01:47,909
the course files to be downloaded.


00:01:47,909 --> 00:01:53,470
Here I copy everything under N:/Tutorials,
which is where the course files will be saved.


00:01:53,470 --> 00:01:58,569
All you have to do is to run the Python script,
which will download all the required Python


00:01:58,569 --> 00:02:02,780
modules if they are missing.


00:02:02,780 --> 00:02:06,780
Upon completion, you will be promoted to login
to your Google account.


00:02:06,780 --> 00:02:12,209
This is necessary to be able to download files
from Google Drive, because Google limits external


00:02:12,209 --> 00:02:16,650
downloads for users that are not logged into
their Google account.


00:02:16,650 --> 00:02:20,750
Because this script is not verified, you have
to proceed as shown.


00:02:20,750 --> 00:02:25,099
The verification process is a long process,
which is why I didn't bother to go through


00:02:25,099 --> 00:02:26,209
with it.


00:02:26,209 --> 00:02:30,860
But you can read the source code of the Python
script if you are concerned about security.


00:02:30,860 --> 00:02:35,730
After giving the script the permission, you
will see that only read only access is needed


00:02:35,730 --> 00:02:37,620
for downloading, nothing more.


00:02:37,620 --> 00:02:41,840
When you complete this step, you should see
a message like this, upon which you can close


00:02:41,840 --> 00:02:42,840
it.


00:02:42,840 --> 00:02:46,569
As soon as the permission is granted, the
script already started the download process.


00:02:46,569 --> 00:02:51,519
You can see the name of the file being downloaded
as well as the overall process.


00:02:51,519 --> 00:02:54,590
The files are being downloaded one by one.


00:02:54,590 --> 00:03:00,420
As mentioned before, please be patient as
the large files can take a while to download.


00:03:00,420 --> 00:03:05,320
After all the files finished downloading,
you can press Enter to exit.


00:03:09,880 --> 00:03:16,579
As you can see, the video file plays fine.


00:03:16,579 --> 00:03:21,079
You can extra the zip file into the Scenes
folder directly so the HIP files can read


00:03:21,079 --> 00:03:24,379
the required geometry and image files correctly.


00:03:24,379 --> 00:03:28,780
If you were not able to download all the files
for any reason, you can re-run the script,


00:03:28,780 --> 00:03:31,200
and it will continue downloading the missing
files.


00:03:31,200 --> 00:03:36,260
So I am just going to delete some of the files
to show this.


00:03:41,049 --> 00:03:45,879
The token file generated contains the permission
you granted to the script.


00:03:45,879 --> 00:03:48,860
So you don't have to do this step each time
you run the script.


00:03:48,860 --> 00:03:54,110
As soon as we run the script again, you can
see the existing files are being skipped.


00:03:54,110 --> 00:03:57,400
And the missing files start to download again.


00:03:57,400 --> 00:04:02,340
If you prematurely terminate the script, make
sure to delete the file being download as


00:04:02,340 --> 00:04:04,960
it won't be complete.


00:04:07,880 --> 00:04:12,799
When we run the script again, this time there
is nothing to download, and as such all the


00:04:12,799 --> 00:04:15,549
files will be skipped.


00:04:15,549 --> 00:04:17,280
And as always, thanks for watching.






----------------------------------------------------------------------------------------------------

01 - Introduction

----------------------------------------------------------------------------------------------------



00:00:03,700 --> 00:00:06,940
Hello and welcome to Pragmatic VEX: Volume 1.


00:00:06,940 --> 00:00:13,340
Some of the topics we will cover include
point clouds, smooth point cloud filtering,


00:00:13,340 --> 00:00:21,580
camera-based occlusion, point relaxation,
half-edges, hash functions, k-depth neighbours,


00:00:21,580 --> 00:00:28,560
k-means clustering, quadtrees,
convolution kernels, shaping functions,


00:00:28,560 --> 00:00:36,640
computing the gradient of an attribute, gradient ascends,
gradient descents, contour lines, flowlines


00:00:36,640 --> 00:00:44,829
on arbitrary geometry via geometric advection,
caustics, adaptive subdivision, bilinear subdivision,


00:00:44,829 --> 00:00:49,320
all the way to fully implementing a custom
catmull-clark subdivision, based on OpenSubdiv,


00:00:49,320 --> 00:00:54,980
where we will tap into the power of the subdivision
limit surface to create isolines directly


00:00:54,980 --> 00:00:59,530
on the subdivision limit surface, and sample
arbitrary attributes from the subdivision


00:00:59,530 --> 00:01:02,179
limit surface, and a lot more.


00:01:02,179 --> 00:01:06,740
Dedicated chapters for aggressive performance
optimizations, showing every trick in the


00:01:06,740 --> 00:01:11,970
book, including implementing a custom fuse
operation using VEX, replacing many of the


00:01:11,970 --> 00:01:16,979
common SOPs that turn out to be bottlenecks
in performance, with our custom VEX based


00:01:16,980 --> 00:01:20,840
implementations, and see how far we 
can push the limits of Houdini,


00:01:20,840 --> 00:01:22,880
far beyond the available tools.


00:01:22,940 --> 00:01:27,440
This is a practical course, rather than an
academic one, and as such, every topic is


00:01:27,440 --> 00:01:33,260
demystified from its formal definitions, explained
intuitively through concept drawings and


00:01:33,260 --> 00:01:35,240
demonstrated with practical examples.


00:01:36,380 --> 00:01:41,280
From the beginning until the end, the entire course
is filled with production-proven tips and tricks.


00:01:42,640 --> 00:01:48,800
We will also learn how to read technical papers,
analyse and break down formulas, solve equations,


00:01:48,810 --> 00:01:53,380
translate and transform them in a way that's
applicable to our implementation, and when


00:01:53,380 --> 00:01:57,170
there is not enough information available,
we will reverse engineer the available tools


00:01:57,170 --> 00:02:01,570
in Houdini, to fully match their behaviour
in VEX.


00:02:01,570 --> 00:02:05,869
This course is aimed to increase your technical
capacity to tackle more complex production


00:02:05,869 --> 00:02:11,260
problems on your own, with complete ease and
control by acquiring a deeper technical understanding


00:02:11,260 --> 00:02:16,080
of how things work in Houdini at the lowest
level with a strong applied focus on high-end


00:02:16,080 --> 00:02:18,330
feature film visual effects production.


00:02:19,600 --> 00:02:23,260
And as always, thanks for watching.






----------------------------------------------------------------------------------------------------

02 - Point Clouds - Introduction

----------------------------------------------------------------------------------------------------



00:00:05,160 --> 00:00:10,629
Point clouds are a fundamental part of general
Houdini workflow, whether you are creating


00:00:10,629 --> 00:00:18,090
procedural modeling assets for use in games
or complex dynamics setups for use in VFX.


00:00:18,090 --> 00:00:24,800
Internally they use acceleration data structures
for max performance over naive brute force methods.


00:00:25,740 --> 00:00:30,300
We use acceleration data structures for various
computer science problems.


00:00:31,060 --> 00:00:37,980
For example, we use a KD-tree to perform nearest
neighbour searches or multidimensional key queries.


00:00:38,340 --> 00:00:46,440
There are two common ways to approach accelerator
creation, object subdivision and space subdivision.


00:00:46,440 --> 00:00:53,120
In Bounding Volume Hierarchy (BVH) we are
subdividing objects into smaller pieces.


00:00:53,120 --> 00:01:02,410
In contrast, Octrees/K-d trees and other space
subdivision methods, divide the space recursively.


00:01:02,410 --> 00:01:06,460
There is volumes of information on these
subjects online, should you want to dive into


00:01:06,460 --> 00:01:10,960
the deepest end, but if there is a single
piece of information that you should take


00:01:10,960 --> 00:01:19,240
from this video, it's that pcopen uses a kd-tree
structure and pcfind uses a BVH structure.


00:01:20,420 --> 00:01:22,860
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

03 - Point Clouds - pcopen() vs pcfind() vs nearpoints() - Introduction

----------------------------------------------------------------------------------------------------



00:00:05,279 --> 00:00:11,340
So the 2 most common point cloud lookup functions
are pcopen and pcfind.


00:00:11,340 --> 00:00:15,560
On the surface, they might look the same but
they are quite different the way they are used.


00:00:16,080 --> 00:00:21,840
pcopen feels a bit arcane the way you are
using memory handles and such, but if you


00:00:21,849 --> 00:00:26,540
are used to C style languages it might be
familiar to you.


00:00:26,540 --> 00:00:31,852
The whole idea with pcopen is you perform
a query that returns a handle, and you use


00:00:31,852 --> 00:00:36,010
this handle to iterate through the queue of
points and perform the operations you need


00:00:36,010 --> 00:00:37,609
for each point.


00:00:37,609 --> 00:00:44,359
With pcfind, you immediately get an array
of point indices where you can work with them


00:00:44,359 --> 00:00:46,179
in any way you want.


00:00:46,179 --> 00:00:51,450
pcopen also has some magical syntax like
if you want to get the point number or the


00:00:51,450 --> 00:00:57,859
point distance, you have to import special
attributes called "point.number" and "point.distance"


00:00:57,859 --> 00:01:01,170
respectively using the pcimport function.


00:01:01,170 --> 00:01:06,740
With pcfind, because you already have an array
of the point indices, you are free to use


00:01:06,740 --> 00:01:11,410
any VEX function where you can use point indices.


00:01:11,410 --> 00:01:16,250
It used to be faster to access the distances
with pcopen where they already had this info


00:01:16,250 --> 00:01:22,020
in memory but now we have an overload in pcfind
that can return you the point distances array


00:01:22,020 --> 00:01:28,780
directly where you can use min/max/average
and sum functions on it directly.


00:01:28,780 --> 00:01:33,830
This saves you the trouble of populating an
array of distances manually.


00:01:33,830 --> 00:01:38,640
Beyond the style and the magical syntax, the
most important factor in my opinion is the


00:01:38,640 --> 00:01:43,859
performance and pcfind is significantly faster
than pcopen.


00:01:43,860 --> 00:01:49,390
We will demonstrate this using a practical
example in Houdini where we will color points


00:01:49,390 --> 00:01:53,280
using density, which is how many 
points are found for each point


00:01:53,440 --> 00:01:55,740
relative to the max number of points queried.






----------------------------------------------------------------------------------------------------

04 - Point Clouds - pcopen() vs pcfind() vs nearpoints() - Implementation

----------------------------------------------------------------------------------------------------



00:00:05,660 --> 00:00:08,860
First, create a geometry node and bring a geometry.


00:00:08,860 --> 00:00:11,040
I am gonna use this model.


00:00:12,000 --> 00:00:14,100
We will create some kind of noise.


00:00:18,200 --> 00:00:20,240
I am going to use turbulent noise.


00:00:27,660 --> 00:00:31,440
Maybe use simplex noise, and increase frequency.


00:00:41,600 --> 00:00:44,920
Let's fit it between -0.1, and 0.5.


00:00:49,860 --> 00:00:51,740
Then we will scatter some points.


00:00:58,580 --> 00:00:59,720
1 million should be enough.


00:01:01,940 --> 00:01:04,920
Now we can write the VEX code that shows both
approaches.


00:01:06,320 --> 00:01:08,320
We can start with pcopen first.


00:01:35,200 --> 00:01:37,280
This should give us the density attribute.


00:01:39,280 --> 00:01:42,960
So let's just create a color sop and visualize the density.


00:01:46,880 --> 00:01:48,120
Create the parameters.


00:01:48,660 --> 00:01:51,460
But before this let's just open the performance monitor.


00:01:53,940 --> 00:01:56,260
Set the radius first and record it now.


00:01:58,240 --> 00:02:00,400
Let's say 120 points.


00:02:03,520 --> 00:02:05,400
It took about 1.7 seconds.


00:02:06,680 --> 00:02:07,800
We can stop the timer.


00:02:10,200 --> 00:02:13,680
Now create the second wrangle to implement
the same using pcfind.


00:02:17,040 --> 00:02:19,760
We can copy the parameter declarations to
save time.


00:02:36,920 --> 00:02:39,080
Let's visualize the same density.


00:02:43,720 --> 00:02:45,600
Set the radius to the same radius.


00:02:46,680 --> 00:02:51,960
Start the timer and then set the point number
to 120.


00:02:53,280 --> 00:02:55,600
It look a bit less than 0.8 seconds.


00:02:56,320 --> 00:03:00,600
As you can see pcfind is more than twice as
fast as pcopen.


00:03:00,600 --> 00:03:06,920
So it's much better to use pcfind rather than
pcopen, it's more user friendly and much faster,


00:03:07,520 --> 00:03:11,560
and speed is an important factor 
often times when working inside Houdini,


00:03:11,600 --> 00:03:14,240
particularly with large data sets.


00:03:15,080 --> 00:03:18,840
Another thing to mention for completeness
is nearpoints.


00:03:19,320 --> 00:03:24,520
Contrary to popular belief, this function is
just a simpler wrapper around pcfind


00:03:24,520 --> 00:03:26,880
that allows you to type a few less parameters.


00:03:27,360 --> 00:03:29,360
It is not a different implementation.


00:03:29,520 --> 00:03:34,080
If you ever see it being slightly faster than
pcfind, it's just thermal noise.


00:03:35,200 --> 00:03:40,840
In the next chapter, I will talk about pcfilter
that still gives pcopen relevance,


00:03:41,520 --> 00:03:47,069
but it's still something that can be implemented
using pcfind, it's just that it's more work and


00:03:47,069 --> 00:03:51,920
requires due diligence if you want to
handle all attribute types generically.


00:03:52,640 --> 00:03:55,240
And with that said, see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

05 - Point Clouds - pcfilter() Implementation for pcfind()

----------------------------------------------------------------------------------------------------



00:00:05,040 --> 00:00:10,400
In SOPs pcfilter is the only function that
still makes pcopen relevant.


00:00:11,400 --> 00:00:17,140
It's a function that smoothly interpolates attribute
values of the points found by the point cloud lookup.


00:00:18,420 --> 00:00:20,120
Let's demonstrate it in Houdini.


00:00:22,300 --> 00:00:24,580
Let's first copy the code from the help file.


00:00:25,900 --> 00:00:27,280
We can use the same scene.


00:00:27,540 --> 00:00:30,620
I am just gonna delete some of the nodes we
don't need.


00:00:33,940 --> 00:00:35,500
Let's first subdivide the model.


00:00:42,140 --> 00:00:45,200
Scatter some points, maybe like 200.


00:00:49,580 --> 00:00:53,560
We can create Attribute Randomize to randomize
the color.


00:00:59,040 --> 00:01:03,680
Make it a bit brighter and use it as the second
input.


00:01:14,880 --> 00:01:18,160
So instead of density, we are just gonna filter
the color.


00:01:26,480 --> 00:01:35,760
We have to change input to 2, and radius has
to be larger, and only 3 points.


00:01:36,880 --> 00:01:40,160
As you can see pcfilter is quite straightforward.


00:01:41,040 --> 00:01:43,280
You can also change the seed to alternate.


00:01:45,040 --> 00:01:47,200
You can also reduce the brightness.


00:01:47,920 --> 00:01:53,680
With pcfilter you don't have to do anything
special, you just pass the handle and the


00:01:53,760 --> 00:01:57,440
attribute name, then it will do the smooth
filtering for you.


00:01:58,560 --> 00:02:01,360
Now let's implement it from scratch.


00:02:01,360 --> 00:02:06,480
I am just gonna copy-paste this node and instead
of using the pcfilter, we will paste the function


00:02:06,480 --> 00:02:08,240
we copied from the help card.


00:02:09,680 --> 00:02:11,360
So this is the weight,


00:02:14,960 --> 00:02:17,280
and we are gonna replace this one.


00:02:19,680 --> 00:02:22,080
maxd is the maximum distance.


00:02:23,360 --> 00:02:27,760
You can get this using the pcfarthest 
function by passing the handle.


00:02:29,680 --> 00:02:31,760
So this is the maximum distance.


00:02:36,720 --> 00:02:38,640
The distance is already here.


00:02:40,320 --> 00:02:45,920
This is a float version of the function, we
will implement a vector version for the color.


00:02:46,600 --> 00:02:48,120
So this is a vector.


00:02:53,520 --> 00:02:58,240
And the sum should also be initialized, otherwise
you will get a warning.


00:02:59,200 --> 00:03:03,840
Here you are already accumulating but accumulating
an uninitialized variable.


00:03:04,640 --> 00:03:10,500
So this should work but we cannot use the
same name as the pcfilter function,


00:03:10,500 --> 00:03:12,460
so let's make it pcfilter2.


00:03:21,760 --> 00:03:23,800
As you can see the result is identical.


00:03:28,320 --> 00:03:32,320
Now let's implement the exact same thing to
be able to use it with pcfind.


00:03:33,840 --> 00:03:37,000
So we can just copy the same code, as it's
already modified.


00:03:37,700 --> 00:03:39,900
But this time we have to change more things.


00:03:41,180 --> 00:03:44,360
So instead of handle, we have to pass an input,


00:03:46,480 --> 00:03:47,880
an array of points,


00:03:51,520 --> 00:03:52,800
and an array of distances.


00:03:58,380 --> 00:04:01,640
Because we don't have pcfarthest function
for pcfind,


00:04:01,900 --> 00:04:04,560
we are just gonna get the last item of the distance array.


00:04:05,900 --> 00:04:07,600
And we don't have pciterate.


00:04:21,200 --> 00:04:23,200
We don't need the point distance import.


00:04:24,140 --> 00:04:25,740
We can just use the distance array.


00:04:28,560 --> 00:04:32,500
And for the attribute value, we are just gonna
use the point function.


00:04:46,620 --> 00:04:48,520
So now the function should be complete.


00:04:50,100 --> 00:04:52,200
We have to initialize the distance array.


00:05:15,480 --> 00:05:17,880
Set the same radius and number of points.


00:05:20,800 --> 00:05:24,320
As you can see, it's matching perfectly to
the other 2 methods.


00:05:26,380 --> 00:05:29,860
So you can use this method when you need to
smooth filter a vector attribute


00:05:29,900 --> 00:05:32,360
using pcfind instead of pcopen.


00:05:33,800 --> 00:05:38,400
It actually makes sense to invest some time
to implement this function to take any floating


00:05:38,420 --> 00:05:45,320
attribute type from float to vector2, vector
to vector4, matrix2, matrix3, matrix to a float array.


00:05:47,020 --> 00:05:52,780
The time you spend will be well worth it considering
the speed and ease of use of pcfind over pcopen.


00:05:54,720 --> 00:05:57,200
And with that said, see you guys 
in the next lesson.






----------------------------------------------------------------------------------------------------

06 - Point Clouds - pgfind()

----------------------------------------------------------------------------------------------------



00:00:06,180 --> 00:00:10,840
When the performance is of top importance
and you want to squeeze out every last drop


00:00:10,840 --> 00:00:16,139
of performance out of your computer, there
is another function you can consider using,


00:00:16,139 --> 00:00:18,410
and that is pgfind.


00:00:18,410 --> 00:00:22,060
pgfind is a hash-based bucketing technique


00:00:22,080 --> 00:00:27,040
that was significantly faster than KD-tree based approaches.


00:00:27,050 --> 00:00:31,600
It is now actually identical to the VDB Point
Grid sorting technique.


00:00:31,600 --> 00:00:36,480
SideFX had their own version before they switched
to VDB's when they made theirs.


00:00:36,480 --> 00:00:42,910
With the correctly tuned particle separation
size, pgfind was 2 times faster than pcfind.


00:00:42,910 --> 00:00:48,260
But nowadays if you look around Houdini, you
will see instances of this function in pieces


00:00:48,260 --> 00:00:51,270
of commented out code citing stability issues.


00:00:51,270 --> 00:00:52,270
(showing code from Vellum node)


00:00:52,270 --> 00:00:53,950
So what happened?


00:00:53,950 --> 00:01:00,360
As of February 2020, I have talked to Jeff
Lait, a senior mathematician at SideFX and


00:01:00,360 --> 00:01:08,310
according to him, they made pcfind faster
since then and pgfind while faster than pcfind,


00:01:08,310 --> 00:01:13,960
it tended to react badly when NANs or very
large numbers like 10 billion-plus showed up,


00:01:14,000 --> 00:01:16,580
so they kept having to turn it off for
robustness.


00:01:17,540 --> 00:01:19,740
This is the current situation.


00:01:19,750 --> 00:01:26,090
Note that this may change in the future, but
for now, I would recommend using pcfind as


00:01:26,090 --> 00:01:28,640
it's quite fast and very robust.


00:01:29,740 --> 00:01:31,500
See you guys in the next lesson.






----------------------------------------------------------------------------------------------------

07 - Point Clouds - pcfind_radius()

----------------------------------------------------------------------------------------------------



00:00:08,440 --> 00:00:14,600
pcfind_radius is another useful function to
introduce variable pscale in your point cloud lookups.


00:00:18,120 --> 00:00:19,780
Let's see how it works in Houdini.


00:00:24,580 --> 00:00:27,600
So let's just scatter some points like 15
points.


00:00:30,100 --> 00:00:31,860
Then randomize their pscale.


00:00:36,540 --> 00:00:40,840
And just copy some spheres on the points so
we can see their scale.


00:00:56,600 --> 00:01:00,260
Maybe increase the minimum pscale, and randomize.


00:01:02,700 --> 00:01:07,880
We also need to randomize the color, so we
can create another attribute randomize.


00:01:17,880 --> 00:01:19,520
Now let's write the code to do it.


00:01:45,580 --> 00:01:51,040
Here what we say is to color any point if
they are within the pscale of the nearest point


00:01:51,040 --> 00:01:52,160
from the second input.


00:02:04,600 --> 00:02:09,360
So let's just set max points to 1 point and
radius scale to also 1.


00:02:09,900 --> 00:02:11,300
And radius is 0,


00:02:12,080 --> 00:02:16,000
because we don't want to dilute the 
original lookup distance away from the pscale.


00:02:17,800 --> 00:02:20,120
Also instead of connecting to the Copy to Points SOP,


00:02:20,200 --> 00:02:22,840
we can just connect to this node where we have color.


00:02:23,500 --> 00:02:26,540
No need to copy color back onto points for this example.


00:02:30,120 --> 00:02:32,420
As you can see, the colors show up.


00:02:33,700 --> 00:02:39,380
But what we wanna do is not to have a solid
color but rather have what you see become


00:02:39,380 --> 00:02:42,700
hollow that goes further with a color using a fixed width,


00:02:43,840 --> 00:02:46,080
which will look like hollow circles on the surface.


00:02:47,860 --> 00:02:50,140
So we have to do another point cloud look up.


00:02:51,200 --> 00:02:52,660
We can just copy this one.


00:02:54,040 --> 00:02:57,720
And create a width parameter for the second
point cloud look up.


00:02:59,720 --> 00:03:04,880
The new requirement is that we will only color
a point if it can not be found within the


00:03:04,920 --> 00:03:11,400
first pscale, rather it can only be found
within the dilated pscale with the additional width.


00:03:18,280 --> 00:03:21,160
You can now see hollow circles.


00:03:22,420 --> 00:03:25,340
We also need to change the color look up to pts2.


00:03:27,860 --> 00:03:29,200
Now the colors are correct.


00:03:30,600 --> 00:03:35,180
You can also vary the width per point or interpolate
between multiple colors if you want.


00:03:36,220 --> 00:03:37,620
But this is the main idea.


00:03:39,280 --> 00:03:40,880
See you guys in the next lesson.






----------------------------------------------------------------------------------------------------

08 - Point Clouds - Excluding the Current Point & Ad-Hoc Groups

----------------------------------------------------------------------------------------------------



00:00:04,820 --> 00:00:09,820
One of the common operations wrt to point
clouds is to remove the current point from


00:00:09,820 --> 00:00:10,800
the search result.


00:00:12,080 --> 00:00:14,420
There are many approaches to accomplish this,


00:00:17,600 --> 00:00:19,380
So let's just scatter some points.


00:00:23,940 --> 00:00:26,160
We will do a regular point cloud lookup.


00:00:40,100 --> 00:00:41,820
I also assign the result,


00:00:42,480 --> 00:00:46,820
otherwise, VEX will optimize out the function 
call since the result is not used.


00:00:48,220 --> 00:00:53,399
And all point cloud functions support ad hoc
groups, meaning they support literal and on


00:00:53,400 --> 00:00:57,780
the fly groups that you can form on the spot
using some form of expression.


00:00:59,360 --> 00:01:02,560
One way people use these is 
by dynamically generating them


00:01:02,760 --> 00:01:05,160
to exclude the current point in literal form.


00:01:07,160 --> 00:01:09,260
Let's open the performance monitor first.


00:01:10,720 --> 00:01:11,800
Press record.


00:01:13,040 --> 00:01:15,260
I will show you a way you should have used first.


00:01:20,680 --> 00:01:25,960
What this code tells is to include all the
points except the point number that comes


00:01:25,960 --> 00:01:28,720
after the caret character that looks like a hat.


00:01:30,860 --> 00:01:33,480
As you can see it's significantly slower.


00:01:34,840 --> 00:01:39,720
What happens under the hood is, for every
dynamically generated ad hoc group that's


00:01:39,720 --> 00:01:44,960
unique, you create a new acceleration data
structure, which is very expensive,


00:01:46,580 --> 00:01:49,780
and this causes you to do this for each point essentially,


00:01:50,200 --> 00:01:52,880
negating all the benefits of having this sort


00:01:52,880 --> 00:01:55,020
of acceleration data structure in the first
place.


00:01:56,180 --> 00:02:00,600
So in this example we created 10 thousand
acceleration data structures.


00:02:01,500 --> 00:02:02,960
Let's remove the ad hoc group.


00:02:04,300 --> 00:02:10,420
The most common method people use is to either
remove the first element or skip the first element.


00:02:12,840 --> 00:02:16,720
removeindex removes an item from an array
by using an index.


00:02:19,140 --> 00:02:22,660
As you can see, it's much faster than the
ad hoc approach.


00:02:24,200 --> 00:02:26,540
But there is something you should know about
this technique.


00:02:27,380 --> 00:02:32,580
Even though it should work in most cases,
it's actually not the correct way to deal with this.


00:02:33,880 --> 00:02:38,120
Point cloud functions do not have intrinsic
knowledge about the current point.


00:02:39,140 --> 00:02:41,540
They are only concerned about point positions.


00:02:42,800 --> 00:02:47,700
So if we imagine a case where there are multiple
points that overlap at the same position as


00:02:47,700 --> 00:02:48,700
the current point.


00:02:49,380 --> 00:02:53,620
There is no guarantee that the first point
that's returned will be the point itself.


00:02:54,880 --> 00:02:58,980
So if you are writing code for correctness,
this is how you should do it.


00:03:00,640 --> 00:03:04,940
Instead of removeindex, you should use removevalue.


00:03:04,960 --> 00:03:07,880
And remove the current point from the search
result.


00:03:09,120 --> 00:03:13,820
This will ensure the current point will be
removed from the array, regardless of where


00:03:13,820 --> 00:03:15,080
it may be in the array.


00:03:16,640 --> 00:03:23,100
To summarize, you should know that anytime
you change the P channel, the pscale channel,


00:03:23,100 --> 00:03:28,180
and the ad hoc group, you will cause the acceleration
structures to be rebuilt.


00:03:28,180 --> 00:03:33,240
Not when you change the radius or the max
points returned, which is another misconception


00:03:33,240 --> 00:03:34,930
about point clouds.


00:03:34,930 --> 00:03:37,620
So keep this in mind when using them.


00:03:38,260 --> 00:03:42,940
And of course, if you change the input, the
acceleration data structures will also be


00:03:42,940 --> 00:03:44,100
rebuilt.


00:03:45,480 --> 00:03:48,820
And with that said, see you guys in the next
lesson.






----------------------------------------------------------------------------------------------------

09 - Point Clouds - Finding Min & Max Neighbour Points

----------------------------------------------------------------------------------------------------



00:00:04,880 --> 00:00:08,380
Point cloud functions return you 
a max number of points,


00:00:08,920 --> 00:00:12,380
but sometimes you want a minimum guaranteed number of points.


00:00:13,740 --> 00:00:18,960
So you want to gather X number of points between
a minimum and maximum number of points.


00:00:20,600 --> 00:00:22,780
There are a few ways to solve this sort of problem.


00:00:23,760 --> 00:00:28,560
One of them is a sort of adaptive search where
you continually adjust the radius.


00:00:29,580 --> 00:00:31,580
That's one option you can use,


00:00:32,580 --> 00:00:36,360
because just changing the radius 
doesn't rebuild the acceleration structures.


00:00:38,180 --> 00:00:41,380
The technique I am gonna show you is different
than the adaptive search.


00:00:44,940 --> 00:00:46,380
So let's bring a model to use.


00:00:47,940 --> 00:00:49,420
I am gonna use the same model.


00:00:52,540 --> 00:00:54,240
And create fog volume.


00:00:57,980 --> 00:01:01,980
We are gonna modify the density using a noise
inside a volume vop.


00:01:02,460 --> 00:01:04,160
I will use turbulent noise,


00:01:07,500 --> 00:01:08,980
and multiply the density.


00:01:17,720 --> 00:01:20,920
Increase the frequency, and 
find a pattern that you like.


00:01:43,080 --> 00:01:44,580
Scatter some points.


00:01:49,920 --> 00:01:52,340
And create a wrangle to do the lookup.


00:01:59,140 --> 00:02:01,720
So the same usual radius and max points.


00:02:14,600 --> 00:02:16,540
We also want min points.


00:02:25,100 --> 00:02:26,240
Now do the lookup.


00:02:34,360 --> 00:02:39,520
But instead of using the radius parameter,
we will use a very large radius to find all the points.


00:02:42,180 --> 00:02:43,980
And using the minimum points first.


00:02:46,740 --> 00:02:52,660
So what we have to do is: take the last point
returned and compute the distance between


00:02:52,660 --> 00:02:54,720
this point and the current point.


00:02:55,200 --> 00:03:00,140
And if the distance is greater than the radius,
this is what we return.


00:03:01,120 --> 00:03:04,960
What this means is, because we want to
have a minimum number of points


00:03:05,660 --> 00:03:08,360
and we already surpassed the radius,


00:03:08,360 --> 00:03:10,860
we can not increase the radius anymore


00:03:11,520 --> 00:03:14,400
so this is all the points we can get at the minimum.


00:03:40,000 --> 00:03:42,960
So I am just gonna store the number of points
as the result.


00:03:47,840 --> 00:03:52,520
If this condition is not met, we will have
to do another point cloud lookup,


00:03:53,200 --> 00:03:57,380
but this time using the radius 
and the max points parameter.


00:03:58,520 --> 00:04:03,460
It basically means, the furthest point we
found in the previous search doesn't meet


00:04:03,460 --> 00:04:09,180
the distance requirements, so we now need
to use the maximum radius that we can using


00:04:09,180 --> 00:04:14,360
the radius parameter and cap the number of
points using the max points parameter.


00:04:16,000 --> 00:04:18,080
So we are just gonna do it like this.


00:04:40,160 --> 00:04:42,080
We can also clean up the code a bit.


00:04:57,640 --> 00:05:00,380
Change the radius to maybe 0.05.


00:05:01,400 --> 00:05:04,620
And maybe between 50 and 250 points.


00:05:07,120 --> 00:05:11,480
We can check the geometry spreadsheet, and
color the points using the count attribute.


00:05:37,980 --> 00:05:41,420
Now you can actually see how the min and max
points are gathered.


00:05:48,140 --> 00:05:53,180
There is another performance improvement you
can use, something that I learnt in real-time


00:05:53,180 --> 00:05:55,780
graphics programming when 
I was in game development.


00:05:56,460 --> 00:06:01,300
Instead of length or distance functions, you
should always try to use the squared versions.


00:06:01,900 --> 00:06:06,020
This is to avoid computing the square root
of values, which is very expensive.


00:06:06,340 --> 00:06:09,500
But you might wonder when is the right time
to do such a thing.


00:06:10,180 --> 00:06:14,560
As it's a common source of confusion among
many artists I have worked with in VFX


00:06:14,800 --> 00:06:18,200
as the requirements are not as strict as real
time game development.


00:06:18,900 --> 00:06:22,840
If they are faster then why use length or
distance in the first place.


00:06:23,500 --> 00:06:24,920
The answer is actually quite simple.


00:06:25,580 --> 00:06:29,340
Anytime you are doing a comparison, you can
use the squared versions.


00:06:29,860 --> 00:06:32,520
Because you only care about the relative differences,


00:06:32,900 --> 00:06:35,520
you don't care about the actual length or distance.


00:06:36,540 --> 00:06:41,060
If you need the actual length or the actual
distance, you have to use the regular versions.


00:06:42,260 --> 00:06:45,920
In VEX this is length2 and distance2,


00:06:46,720 --> 00:06:50,540
in VOPs, there are no such equivalent, 
so you either have to


00:06:50,540 --> 00:06:54,680
make your own like me, or just use the snippet
VOP to call them.


00:06:55,740 --> 00:06:59,560
So for our code, we have to change 
the distance to distance2,


00:07:00,240 --> 00:07:04,300
but we have to make sure everything
we compare against is also squared.


00:07:04,700 --> 00:07:08,680
And that means, taking radius and multiplying it by itself,


00:07:09,200 --> 00:07:11,180
and now the comparison will be valid.


00:07:12,160 --> 00:07:14,780
As you can see the result stays the same.


00:07:15,780 --> 00:07:18,460
There is yet another performance lesson you
can learn here.


00:07:18,960 --> 00:07:24,300
Instead of multiplying radius by itself, why
not use the power function to do the same?


00:07:24,780 --> 00:07:26,840
Might even look more elegant in the code.


00:07:28,180 --> 00:07:33,700
Multiplying a value by itself is much faster
than the power function, as the latter has


00:07:33,709 --> 00:07:39,309
to deal with the problem in its general case,
dealing with fractional powers and other issues,


00:07:39,309 --> 00:07:45,940
while (radius * radius) would just take a few
multiply instructions, so it's much more performant.


00:07:45,940 --> 00:07:50,980
These tricks will definitely give you a performance
improvement especially when you are working


00:07:50,980 --> 00:07:56,740
with very large data sets, or if you are making
low-level operators that will be part of much


00:07:56,780 --> 00:07:59,120
larger networks such as dynamic solvers.


00:08:00,520 --> 00:08:03,400
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

10 - Point Clouds - Unique Pair Matching - Concept

----------------------------------------------------------------------------------------------------



00:00:05,460 --> 00:00:11,240
Another common problem that comes up in production
is to uniquely match a point cloud to another


00:00:11,240 --> 00:00:16,740
point cloud, which means to find the nearest
unique match between 2 point clouds where


00:00:16,740 --> 00:00:21,740
no point in the source point cloud has duplicate
matches in the target point cloud.


00:00:22,620 --> 00:00:25,320
So to illustrate we can have a point cloud like this.


00:00:26,980 --> 00:00:29,800
And a target point cloud, maybe something like this.


00:00:35,520 --> 00:00:38,240
And we can match them using the nearest point.


00:00:40,080 --> 00:00:44,380
This point would also normally take this one
but since it's already taken,


00:00:44,920 --> 00:00:46,760
it will take the second nearest.


00:00:47,940 --> 00:00:52,540
If there are less points in the target point
cloud then there are in the source point cloud,


00:00:52,760 --> 00:00:58,160
some points won't have matches, so we can
have a value like -1 to identify them.


00:00:59,000 --> 00:01:03,420
But if there are the same number or more in
the target point cloud than there are in the


00:01:03,420 --> 00:01:06,880
source point cloud, then each point will have
a unique match.


00:01:08,060 --> 00:01:13,200
And there are many ways to solve this problem,
I will show you one that's simple to implement


00:01:13,200 --> 00:01:14,740
and quite performant.


00:01:15,300 --> 00:01:17,820
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

11 - Point Clouds - Unique Pair Matching - Implementation

----------------------------------------------------------------------------------------------------



00:00:03,460 --> 00:00:05,380
Let's set this example aside.


00:00:06,100 --> 00:00:11,940
We will first implement it on a visual simpler
geometry so we can identify it easier


00:00:11,940 --> 00:00:15,600
and then once it works, 
we can try it on a different model.


00:00:15,980 --> 00:00:18,360
So we could create something like a circle.


00:00:22,240 --> 00:00:23,280
Make it an arc.


00:00:25,460 --> 00:00:27,460
I will use something like 80 points.


00:00:28,400 --> 00:00:30,380
And copy-paste to make a larger one.


00:00:31,020 --> 00:00:32,320
We will use both of them.


00:00:36,880 --> 00:00:38,880
We could say this is the source.


00:00:42,320 --> 00:00:43,960
And this is the target..


00:00:47,380 --> 00:00:51,000
First, we need to create some groups to include
all points.


00:00:51,580 --> 00:00:53,140
Call it not found.


00:00:55,740 --> 00:00:58,260
As you can see we have all the points.


00:00:59,680 --> 00:01:04,820
So the first thing we want to do is to find
the nearest point for each point in the source


00:01:04,820 --> 00:01:07,440
point cloud from the target point cloud.


00:01:08,080 --> 00:01:10,640
Let's just create an attribute wrangle.


00:01:11,180 --> 00:01:16,720
But instead of doing the look upon all points,
we want to limit it to the not found group.


00:01:19,560 --> 00:01:23,260
And a very large radius, and only 1 match.


00:01:24,160 --> 00:01:27,520
We also want to store the result to check
for duplicates.


00:01:27,880 --> 00:01:29,840
We can not do this in the same wrangle.


00:01:30,740 --> 00:01:32,460
So we have to create another wrangle node.


00:01:33,520 --> 00:01:39,360
There is a very useful function for this, called
findattribval which will give us the index


00:01:39,360 --> 00:01:42,860
of an element with a particular string or
integer value.


00:01:43,720 --> 00:01:48,540
We are gonna check if that index matches the
current point number and if it does, that


00:01:48,540 --> 00:01:54,240
means we are the first match, and so we want
to exclude that point from the not found group.


00:01:54,460 --> 00:01:58,700
So that in the next iteration it's not included
in the point cloud search.


00:01:59,460 --> 00:02:01,980
To do this, just create another wrangle node.


00:02:08,540 --> 00:02:15,460
It's a point attribute called match and the
value is match, and we want the first match.


00:02:21,160 --> 00:02:27,140
This is a shorthand form for groups, where
we write group, followed by an underscore


00:02:27,140 --> 00:02:30,980
and the group name and an integer value that specifies


00:02:30,980 --> 00:02:34,440
whether the current element is included or excluded.


00:02:34,640 --> 00:02:36,940
0 means exclude from the group.


00:02:39,060 --> 00:02:44,300
So if we check, you can see, everything already
found a match in a single iteration,


00:02:44,520 --> 00:02:48,430
because there is only one nearest for 
each point in the source group.


00:02:49,240 --> 00:02:51,200
We can apply something like jitter.


00:02:54,260 --> 00:02:57,680
Actually, let me use standard jitter so everyone
can replicate it.


00:03:00,400 --> 00:03:03,140
Reduce the scale to have a little bit of jitter.


00:03:07,820 --> 00:03:12,040
Now as you can see, there are still 57 points
that couldn't find a match


00:03:12,640 --> 00:03:14,780
because other points found it first.


00:03:15,260 --> 00:03:19,920
There is another optimization we will perform,
and that is, instead of searching the whole


00:03:19,920 --> 00:03:23,960
target point cloud each time, we will only
search the remaining points.


00:03:24,780 --> 00:03:29,600
We are gonna use a similar function, 
but instead of using findattribval,


00:03:29,600 --> 00:03:32,540
we are gonna use findattribvalcount.


00:03:33,460 --> 00:03:35,840
Normally we will do it before these 2 wrangles


00:03:36,920 --> 00:03:38,860
but because this is the first iteration,


00:03:39,220 --> 00:03:43,660
I'll do it afterwards to demonstrate it working
and then we will move it up.


00:03:53,740 --> 00:04:00,920
This time we are using ptnum, because we are
in the target point cloud and here ptnum is


00:04:00,920 --> 00:04:01,820
the match itself.


00:04:10,240 --> 00:04:15,080
As you can see, we got the same number of
points in the source point cloud that couldn't


00:04:15,080 --> 00:04:19,220
find a match as the remaining number of points
in the target point cloud.


00:04:21,000 --> 00:04:25,040
This optimization will basically speed up
this operation.


00:04:26,020 --> 00:04:30,700
Another thing we have to do is to limit the
input group to also not_found group


00:04:30,700 --> 00:04:33,060
so we reduce the number of lookups.


00:04:34,360 --> 00:04:38,140
Now move this node before the 2 wrangles just like this,


00:04:39,580 --> 00:04:41,620
and connect it as the second input.


00:04:49,940 --> 00:04:53,420
And now we have to put this entire thing inside
a for a loop.


00:04:54,400 --> 00:04:56,060
We don't need the iteration.


00:05:06,420 --> 00:05:08,840
We want to make it feedback by count,


00:05:10,440 --> 00:05:12,200
and also fetch feedback.


00:05:14,820 --> 00:05:17,620
Let's rename these just to make it more clear.


00:05:26,640 --> 00:05:31,000
As you can see, we cooked 10 iterations and
it found all the matches.


00:05:32,080 --> 00:05:36,600
But of course, we want the for loop to terminate
in the least number of iterations


00:05:37,620 --> 00:05:41,460
and to do that we are gonna use this feature called
stop condition.


00:05:42,780 --> 00:05:48,440
So we are gonna check if the number of points
in the not_found group in Block Begin node


00:05:48,440 --> 00:05:53,360
is the same as the number of points in the
same group in Block End node.


00:05:54,660 --> 00:06:00,440
If that is the case, that means in the last
iteration we couldn't find any more matches


00:06:01,240 --> 00:06:04,120
and so there is no need to continue anymore.


00:06:06,040 --> 00:06:09,820
First, add a spare parameter that references
Block Begin.


00:06:11,180 --> 00:06:17,960
We will use 2 expressions, pointlist and argc,
where pointlist returns a space-separated


00:06:17,960 --> 00:06:24,440
list of indices from a point group and argc,
which is arg count, counts the number of items


00:06:24,440 --> 00:06:26,000
in a string list like that.


00:06:37,540 --> 00:06:43,300
And compare it with Block Begin which is -1
as it's a spare parameter.


00:06:44,420 --> 00:06:48,220
As you can see it returns 1 in iteration 8.


00:06:49,380 --> 00:06:53,200
So we can increase the number of iterations
to a very large number like this.


00:06:53,820 --> 00:06:56,600
It won't matter because it will automatically
terminate.


00:06:57,780 --> 00:07:03,040
Now the matching is done we can create polygons
between the pairs to better see the result.


00:07:04,380 --> 00:07:07,740
We have to create the same attribute on target points.


00:07:19,480 --> 00:07:24,640
As you can see, every point has a match but
it's still hard to read.


00:07:25,520 --> 00:07:27,120
Let's just bypass the jitter.


00:07:28,280 --> 00:07:30,720
Now every point found a perfect match.


00:07:32,440 --> 00:07:37,180
You can increase the number of points in the
target point cloud, creating abundance.


00:07:39,660 --> 00:07:41,800
Or you can decrease the number of points.


00:07:41,800 --> 00:07:44,760
In this case, not every point finds a match.


00:07:48,100 --> 00:07:50,020
You can delete these points if you want.


00:07:50,860 --> 00:07:52,540
So it would be something like this.


00:07:54,520 --> 00:07:56,720
Let's run this on the statue model.


00:07:59,940 --> 00:08:01,180
So delete the jitter.


00:08:13,740 --> 00:08:17,700
We can alter the noise offset to have a different
distribution of points.


00:08:42,900 --> 00:08:47,620
I will also color the points by bounding box
so we can read the connections better.


00:08:55,740 --> 00:08:59,880
So now you can see some of the points are
going quite far to find a match


00:09:00,200 --> 00:09:02,420
because other points found them first.


00:09:05,360 --> 00:09:07,580
Increase the number of points like double,


00:09:09,240 --> 00:09:12,740
now you can see they find a match much closer than before,


00:09:13,300 --> 00:09:15,440
therefore creating shorter lines.


00:09:16,740 --> 00:09:19,800
If you had less, then most points wouldn't
find a match.


00:09:23,820 --> 00:09:25,960
And now they have so much abundance.


00:09:26,300 --> 00:09:27,980
They get shorter and shorter.


00:09:33,020 --> 00:09:35,280
This is 1 to 1 match, same as before.


00:09:36,440 --> 00:09:39,620
Most people take a sequential approach to
solve this problem.


00:09:40,500 --> 00:09:44,860
But the approached I showed you is parallel
and converges really fast


00:09:44,860 --> 00:09:46,480
and quite simple to implement.


00:09:47,940 --> 00:09:51,320
You can also compile the for loop network
to make it even faster


00:09:51,620 --> 00:09:54,720
as then the entire thing would cook like a single entity.


00:09:56,680 --> 00:09:59,460
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

12 - Point Clouds - Camera Based Occlusion with Variable pscale - Concept

----------------------------------------------------------------------------------------------------



00:00:05,300 --> 00:00:08,680
This is another common problem that often
comes up in production.


00:00:09,480 --> 00:00:13,280
To cull points with variable radius using
occlusion from the camera.


00:00:14,140 --> 00:00:17,580
We first have to project the 
points onto the camera plane,


00:00:17,980 --> 00:00:20,280
do the point cloud lookup in this space,


00:00:20,580 --> 00:00:24,900
and instead of using the points
that are sorted by the distance to the current point,


00:00:25,420 --> 00:00:29,120
we will sort them based on how far
they are to the camera plane.


00:00:33,560 --> 00:00:35,660
So let's say we have the camera here.


00:00:38,300 --> 00:00:43,180
In effect, this will allow us to do the point
cloud search in the projected space


00:00:43,660 --> 00:00:47,700
but have the points sorted based on their proximity
to the camera plane.


00:00:51,480 --> 00:00:57,660
So we take a position in the camera space,
and we take another position along the camera.


00:00:58,140 --> 00:01:01,700
Let's call them p0 and p1.


00:01:03,420 --> 00:01:07,380
And we compute a direction vector using those,
let's call it N.


00:01:08,600 --> 00:01:14,540
What we have to do to project our points on
the camera plane is, take any point on the


00:01:14,540 --> 00:01:21,540
camera plane, like p0, subtract the current
point position, take the dot product between this


00:01:23,520 --> 00:01:27,280
and the normal, multiply it by the normal.


00:01:28,860 --> 00:01:30,500
This is the displacement vector.


00:01:30,600 --> 00:01:31,880
Let's call it q.


00:01:33,800 --> 00:01:36,280
Add this onto the current point position.


00:01:37,360 --> 00:01:41,660
And the length of this displacement gives
you the distance to the camera plane.


00:01:42,720 --> 00:01:45,820
d is the length of q.


00:01:48,360 --> 00:01:51,700
This is the value we will use instead of the
actual distances.


00:01:51,700 --> 00:01:55,560
So we can sort them based
on how far from the camera plane.


00:01:56,920 --> 00:01:59,420
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

13 - Point Clouds - Camera Based Occlusion with Variable pscale - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,400 --> 00:00:05,980
Let's bring a model in.


00:00:06,960 --> 00:00:08,380
Create a fog volume.


00:00:14,180 --> 00:00:18,260
Scatter some points, and we will randomize the scale.


00:00:19,600 --> 00:00:21,640
So create attribute randomize.


00:00:28,140 --> 00:00:29,720
I will use a custom ramp.


00:00:35,440 --> 00:00:38,860
We can also instance some spheres so we can
see the pscale.


00:00:40,180 --> 00:00:44,040
Just try to get a cluster of points with a
lot of overlapping pscale,


00:00:44,040 --> 00:00:48,800
but not very large individual points
as that might remove too many points.


00:00:50,080 --> 00:00:53,700
Let's create a camera and color the points by pscale.


00:01:08,940 --> 00:01:11,880
We can check the range in the Geometry Spreadsheet.


00:01:12,920 --> 00:01:14,400
So that seems to be right.


00:01:19,100 --> 00:01:21,720
We should make the smallest pscale not 0.


00:01:23,280 --> 00:01:27,560
The first thing we have to do is to delete
any points outside the camera frustum.


00:01:28,640 --> 00:01:35,040
I have an OTL to do this but for this lesson,
I will show you how to do this using standard Houdini tools.


00:01:36,940 --> 00:01:39,020
So we can just create a UV Texture SOP.


00:01:48,560 --> 00:01:51,900
Set the camera and use Point attribute.


00:01:53,400 --> 00:01:58,960
After this we will delete the points that
have UV values outside the 0 1 range.


00:02:03,840 --> 00:02:06,580
We will use a very small value instead of 0.


00:02:17,940 --> 00:02:21,860
As you can see the points outside the camera
frustum are gone.


00:02:22,720 --> 00:02:24,020
This is the first step.


00:02:25,060 --> 00:02:28,040
Now we will project the points onto the camera plane.


00:02:29,040 --> 00:02:31,220
So let's create another wrangle node.


00:02:37,280 --> 00:02:38,940
Get the camera first.


00:02:39,840 --> 00:02:44,600
We will use NDC functions to create points
in the camera space for demonstration.


00:02:52,380 --> 00:02:57,800
This will create a point at the center of
the camera, and we can offset it along the


00:02:57,800 --> 00:02:59,500
camera for the second point.


00:03:01,200 --> 00:03:05,300
Let's make it large for visualization 
and set it to detail.


00:03:12,680 --> 00:03:14,360
Just create some points.


00:03:16,040 --> 00:03:18,060
So now we have 2 points.


00:03:18,740 --> 00:03:22,840
We also need to create a camera, 
but to maintain a live reference,


00:03:22,840 --> 00:03:24,300
we should create an operator path.


00:03:34,060 --> 00:03:35,120
Now pick the camera.


00:03:37,200 --> 00:03:42,600
As you can see, the points are in the camera
space, and the second one offset along the


00:03:42,600 --> 00:03:46,140
camera path, from the first point which is
at the center of the camera.


00:03:47,740 --> 00:03:50,320
The values are negative along the camera path.


00:03:50,940 --> 00:03:53,820
So you can use larger distances to push it further.


00:03:55,100 --> 00:03:57,600
But we are gonna use something really small.


00:03:58,360 --> 00:04:00,280
Because we just wanna create a normal.


00:04:01,260 --> 00:04:02,740
Now we can delete these,


00:04:03,600 --> 00:04:05,300
and revert back to points


00:04:08,360 --> 00:04:09,959
and look through the camera.


00:04:10,540 --> 00:04:12,220
Also connect the wrangle back.


00:04:14,160 --> 00:04:15,700
Now we just create the normal.


00:04:20,560 --> 00:04:28,200
The projected point displacement is, dot product
between p0 minus current point position and


00:04:28,860 --> 00:04:31,520
the normal times the normal.


00:04:33,720 --> 00:04:38,140
And the length of this displacement is the
distance to the projected camera plane.


00:04:41,340 --> 00:04:47,740
Finally, we are not gonna modify P, but rather
store the projected point position in a different attribute


00:04:48,240 --> 00:04:53,180
and do the lookup there so we don't
have to revert back to P in the world space


00:04:53,200 --> 00:04:54,300
after the culling of points.


00:04:56,060 --> 00:04:57,080
Let's call it Q.


00:05:02,540 --> 00:05:06,420
We can also store the normal so we can copy
another object like a circle


00:05:06,420 --> 00:05:08,520
that would be aligned to the camera plane.


00:05:10,660 --> 00:05:12,720
We can read the intersections better.


00:05:14,400 --> 00:05:17,920
Now the final step is to cull the points using
camera occlusion.


00:05:18,880 --> 00:05:24,640
We will just do a regular pcfind lookup, but
this time we will use pcfind_radius.


00:05:25,680 --> 00:05:27,780
Instead of P we use Q.


00:05:28,420 --> 00:05:30,520
pscale is our radius channel.


00:05:31,380 --> 00:05:34,280
Radius scale is 1, we don't want to alter it.


00:05:34,540 --> 00:05:36,280
Current projected position.


00:05:38,800 --> 00:05:42,800
Current search radius, and max number of points.


00:05:45,700 --> 00:05:51,320
One thing that's important is that we have to use
the pscale as the radius for the point cloud lookup,


00:05:52,280 --> 00:05:55,080
because we don't want to just use 
the pscale of other points


00:05:55,460 --> 00:05:58,340
we also need to account for the current point's pscale.


00:05:59,200 --> 00:06:05,280
Now rather than gathering the distances in an array,
we want to gather the projected point distances.


00:06:06,700 --> 00:06:08,240
So I am just gonna create an array.


00:06:23,700 --> 00:06:28,040
So now we gathered all the projected point distances
from the camera plane.


00:06:29,020 --> 00:06:33,040
So instead of using the points array that
are sorted from the current point position,


00:06:33,480 --> 00:06:37,860
we will sort them using the distances from
the camera plane that we have just gathered.


00:06:39,040 --> 00:06:43,040
In effect this allows us to do the point cloud
look up in the camera plane,


00:06:43,300 --> 00:06:47,080
but have them sorted using the distance from the actual point position


00:06:47,080 --> 00:06:49,320
in the world space to the camera plane.


00:06:50,800 --> 00:06:55,420
So we can use the reorder function to sort
them using the sorted distances array.


00:06:55,820 --> 00:07:00,000
What argsort does is, instead of giving us
the sorted results,


00:07:00,000 --> 00:07:02,380
it returns the indices of the sorted array


00:07:03,000 --> 00:07:06,920
so we can use this to
sort our points using the reorder function.


00:07:08,000 --> 00:07:12,640
Now that we have the sorted array, what we
want to do is to delete the current point


00:07:12,840 --> 00:07:15,360
if it's not the first point in our sorted points.


00:07:16,800 --> 00:07:20,280
if we have points in our point cloud look up that means


00:07:20,280 --> 00:07:25,700
they are overlapping with each other, and
out of these points we only want to keep the


00:07:25,700 --> 00:07:27,400
one that's closest to the camera.


00:07:28,380 --> 00:07:31,900
That's why we delete the current point if
it's not the first point.


00:07:32,860 --> 00:07:37,020
Because the nearest point to the camera in
the same cluster will have a similar list


00:07:37,020 --> 00:07:41,640
of points where it will survive the deletion,
as it's the closest point to the camera.


00:07:43,420 --> 00:07:46,020
Let's set this to a large number like 10 thousand.


00:07:49,580 --> 00:07:52,320
As you can see there are no overlaps.


00:07:52,720 --> 00:07:57,560
But if you look carefully you can see some
points look like they are overlapping,


00:07:57,900 --> 00:07:59,300
like here and here.


00:08:00,600 --> 00:08:02,600
This is because of the perspective distortion.


00:08:03,840 --> 00:08:08,380
We can create a visualization mode where we
project the points onto the camera plane


00:08:08,380 --> 00:08:10,640
with a certain distance from the camera


00:08:11,880 --> 00:08:13,720
So we can add another option here.


00:08:16,080 --> 00:08:17,800
If this preview mode is on,


00:08:21,260 --> 00:08:22,480
we will set P


00:08:24,940 --> 00:08:29,380
to Q plus normal times a scaling factor


00:08:29,380 --> 00:08:32,400
for moving the camera plane 
along the camera direction.


00:08:33,200 --> 00:08:36,020
Create the parameters and set it to toggle.


00:08:42,740 --> 00:08:46,100
Now you can see nothing is overlapping or intersecting.


00:08:47,020 --> 00:08:50,620
They might be touching edge to edge, like
this one or this one,


00:08:52,240 --> 00:08:54,580
but nothing is overlapping or intersecting.


00:08:55,940 --> 00:08:58,380
We can test this by increasing the density of points.


00:08:59,960 --> 00:09:02,540
So instead of 10 thousand, maybe 100 thousand.


00:09:03,960 --> 00:09:08,540
But because we now have a lot of points with
relatively large pscale values, we get more


00:09:08,540 --> 00:09:10,740
of a shuffling rather than a denser packing.


00:09:11,480 --> 00:09:14,580
We can achieve this by lowering the overall pscale.


00:09:18,240 --> 00:09:21,240
Now you can see the points 
are much more densely packed,


00:09:22,160 --> 00:09:26,880
and still, nothing is actually ever
intersecting, only touching by edges.


00:09:29,500 --> 00:09:31,540
Like this one is touching edge to edge.


00:09:36,420 --> 00:09:41,660
And you can look from the camera and toggle
it on and off just for debugging purposes.


00:09:43,380 --> 00:09:48,700
We used a handful of simple
concepts like dot product, point plane projection,


00:09:48,700 --> 00:09:53,900
point plane distance and combined it with
a point cloud look up and achieved a fairly


00:09:53,900 --> 00:09:57,260
complex operation that could save you some
serious performance gains


00:09:57,260 --> 00:09:58,680
in a real production environment.


00:09:59,780 --> 00:10:02,740
I actually used this technique on the movie
Alien Covenant.


00:10:03,460 --> 00:10:05,980
So I hope you can put it to good use as well.


00:10:07,620 --> 00:10:09,420
See you guys in the next lesson.






----------------------------------------------------------------------------------------------------

14 - Point Clouds - Uniform Point Distribution Over Polygonal Surfaces [Point Relaxation] - Concept

----------------------------------------------------------------------------------------------------



00:00:04,700 --> 00:00:09,800
In this lesson we will create uniformly distributed
points using an iterative approach.


00:00:11,100 --> 00:00:14,440
Scatter SOP already has a method to accomplish this,


00:00:14,900 --> 00:00:18,440
but this is just to show how such an
 algorithm could be implemented.


00:00:22,080 --> 00:00:23,680
So let's draw some points.


00:00:25,400 --> 00:00:27,580
And our world is aligned like this.


00:00:30,880 --> 00:00:36,320
From the current point, we find the average
of all the nearby points, like so.


00:00:37,160 --> 00:00:42,660
And instead of pushing the current point towards
the center, we push it away from the center.


00:00:43,640 --> 00:00:49,240
So this is P, this is Q, and let's call this
vector N.


00:00:50,140 --> 00:00:52,900
N is Q minus P.


00:00:53,860 --> 00:00:56,520
We have to add minus Q onto P.


00:00:58,580 --> 00:01:01,200
Minus Q plus P.


00:01:02,460 --> 00:01:03,700
P minus Q.


00:01:05,460 --> 00:01:07,560
We have to do this at every iteration.


00:01:08,540 --> 00:01:10,360
But just doing this is not enough.


00:01:11,100 --> 00:01:13,460
We also want to keep the point on the surface.


00:01:14,280 --> 00:01:18,580
So at each iteration, we also have to project
the points back onto the surface.


00:01:19,640 --> 00:01:22,420
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

15 - Point Clouds - Uniform Point Distribution Over Polygonal Surfaces [Point Relaxation] - Implementation

----------------------------------------------------------------------------------------------------



00:00:05,240 --> 00:00:09,280
Bring a model you want to use, and scatter
some points.


00:00:10,460 --> 00:00:12,760
Let's create the for loop feedback first.


00:00:20,040 --> 00:00:21,780
And create an attribute wrangle.


00:00:23,040 --> 00:00:25,440
So we are just gonna do a regular point cloud look up.


00:00:29,200 --> 00:00:31,020
Use a very large radius again.


00:00:36,860 --> 00:00:41,280
We also need to bring in the pcfilter function
that we wrote in the pcfilter chapter.


00:00:42,380 --> 00:00:44,020
I already copied it in memory.


00:00:44,160 --> 00:00:45,760
So just paste it here.


00:00:47,980 --> 00:00:51,720
It's better to use this function over standard
mean with equal weights,


00:00:52,620 --> 00:00:55,520
as the weights in pcfilter are more sensible.


00:00:57,100 --> 00:00:59,280
We also need to provide a distance array.


00:01:04,980 --> 00:01:06,520
Let's call the center Q.


00:01:14,140 --> 00:01:16,200
We need to add it back onto P.


00:01:20,580 --> 00:01:23,320
And now project it back onto the original geometry.


00:01:24,300 --> 00:01:28,280
I am gonna use the minpos function to find
the nearest position on the surface.


00:01:33,000 --> 00:01:35,840
And connect the second input to the original geometry.


00:01:37,920 --> 00:01:40,960
We have to specify the number of 
nearby points to consider.


00:01:41,760 --> 00:01:44,700
6 is a good number that yields optimal results.


00:01:50,700 --> 00:01:54,440
Let's copy some spheres on the points for
better visualization.


00:02:11,120 --> 00:02:13,400
So we get a fairly good distribution.


00:02:21,560 --> 00:02:23,200
But notice this artifact.


00:02:26,480 --> 00:02:27,880
Let's say this point.


00:02:30,900 --> 00:02:34,980
As I increase the iterations, you can see
some points are going back and forth,


00:02:35,200 --> 00:02:36,480
never settling.


00:02:38,260 --> 00:02:43,620
In one iteration it's going one direction,
in the next iteration, it's going the other way,


00:02:43,880 --> 00:02:46,580
in an endless loop, never converging.


00:02:48,120 --> 00:02:50,720
To fix this we have to do something different.


00:02:51,840 --> 00:02:53,480
I am gonna set Houdini to manual.


00:02:54,580 --> 00:03:01,480
So what we have to do is, keep the first wrangle,
but apply the same wrangle after the first one,


00:03:02,340 --> 00:03:08,580
therefore have both iterations, and then
split the difference, meaning take the average of both.


00:03:10,000 --> 00:03:13,840
We will also move the projection back onto
the original geometry to the last step.


00:03:15,940 --> 00:03:18,860
So I am gonna remove this and set it to p.


00:03:20,620 --> 00:03:22,560
Just copy paste the wrangle,


00:03:24,660 --> 00:03:27,260
and we are gonna average these in another wrangle.


00:03:29,740 --> 00:03:32,980
This time connect the original 
geometry to the third input.


00:03:52,400 --> 00:03:54,440
We are just doing a simple averaging.


00:04:19,440 --> 00:04:22,960
As you can see we have eliminated 
that back and forth motion,


00:04:23,740 --> 00:04:26,340
and now the points converge much faster.


00:04:27,360 --> 00:04:29,600
We can set the iterations to a large number.


00:04:32,540 --> 00:04:34,900
This should give us a very uniform distribution.


00:04:37,720 --> 00:04:42,360
This is not something you necessarily need
to implement but it's still useful to know


00:04:42,360 --> 00:04:47,620
how to implement such an algorithm and it
can help you to solve some complex problems


00:04:47,620 --> 00:04:49,080
in a production environment.


00:04:52,500 --> 00:04:54,420
See you guys in the next lesson.






----------------------------------------------------------------------------------------------------

16 - Decoupling Operators

----------------------------------------------------------------------------------------------------



00:00:14,719 --> 00:00:19,119
Up until here, we looked at point
clouds, and later on we will look


00:00:19,119 --> 00:00:22,800
at point neighbors using edges and primitives


00:00:22,800 --> 00:00:26,480
via connectivity.
Regardless of the type of neighbor


00:00:26,480 --> 00:00:31,119
elements, whether it is based on
proximity, connectivity


00:00:31,119 --> 00:00:36,000
or another method like what can be
output from Find Shortest Path SOP,


00:00:36,000 --> 00:00:40,020
the key is to decouple discrete
operations from each other.


00:00:41,680 --> 00:00:45,440
When you are designing tools and
workflows, ideally


00:00:45,440 --> 00:00:49,960
one thing should not depend on another
thing, but rather be agnostic.


00:00:51,660 --> 00:00:55,920
My rule of thumb is, an operator 
should only do one thing and


00:00:55,920 --> 00:00:59,680
one thing only, and any other option it has should only


00:00:59,680 --> 00:01:02,980
help to achieve or expand on that operation.


00:01:04,960 --> 00:01:08,320
Here we are separating the gathering of
neighbor elements


00:01:08,320 --> 00:01:13,780
from convolution itself, so that it can
be used with any elements with any weights.


00:01:15,500 --> 00:01:17,840
The computation of weights themselves,


00:01:17,840 --> 00:01:21,640
often times require intrinsic 
knowledge about the type of elements.


00:01:23,360 --> 00:01:26,960
For example, with point neighbours using primitives,


00:01:26,960 --> 00:01:30,799
it's important to consider the depth of
the neighbour as well as whether it's an


00:01:30,799 --> 00:01:34,479
edge based neighbour or a primitive based neighbour, so each of


00:01:34,480 --> 00:01:36,700
them could be weighted
properly.


00:01:38,500 --> 00:01:42,720
That's also why we will go to all the
trouble of implementing these generically,


00:01:42,720 --> 00:01:46,640
so that our convolution could run on these,
 and anything else,


00:01:46,640 --> 00:01:50,320
as long as we provide an array of 
indices for input elements


00:01:50,320 --> 00:01:52,000
and an array of weights.


00:01:53,240 --> 00:01:55,840
Convolution doesn't
need to know anything else,


00:01:55,840 --> 00:01:59,759
and even the weights normalization we
will do later on,


00:01:59,760 --> 00:02:02,280
can be done before the convolution.


00:02:04,560 --> 00:02:08,020
And with that said, see you guys in the
next lesson.






----------------------------------------------------------------------------------------------------

17 - Convolution Kernels - Introduction

----------------------------------------------------------------------------------------------------



00:00:04,720 --> 00:00:06,680
What is a convolution kernel?


00:00:06,680 --> 00:00:12,140
Convolution is the treatment of a matrix by
another one which is called "kernel".


00:00:12,800 --> 00:00:18,160
In the case of images, which are bi-dimensional
collections of pixels in rectangular coordinates,


00:00:18,740 --> 00:00:21,220
the convolution operation uses a kernel.


00:00:22,060 --> 00:00:24,260
The used kernel depends on the effect you
want.


00:00:25,520 --> 00:00:28,580
Convolution can be generalized to higher dimensions,


00:00:28,580 --> 00:00:36,120
not just 1D or 2D, but also 3D,
in the case of voxels for volumes and 4D and beyond.


00:00:37,160 --> 00:00:40,640
A convolution is very useful for signal
processing in general.


00:00:41,440 --> 00:00:45,160
There is a lot of complex mathematical theory
available for convolutions.


00:00:46,000 --> 00:00:49,900
For digital image processing, you don't have
to understand all of that.


00:00:49,900 --> 00:00:55,560
You can use a simple matrix as an image convolution
kernel and do some interesting things!


00:00:56,440 --> 00:01:02,240
An image kernel is a small matrix used to apply effects
like the ones you might find in Photoshop,


00:01:02,240 --> 00:01:06,920
such as blurring, sharpening, outlining or
embossing.


00:01:08,400 --> 00:01:10,800
Let's look at the concept of convolution.


00:01:15,140 --> 00:01:20,720
On the left, is the image matrix: each pixel
is marked with its value.


00:01:21,840 --> 00:01:25,400
The initial pixel is at the center with a
value of 5.


00:01:26,400 --> 00:01:29,120
The kernel action area is a 3x3 matrix.


00:01:30,240 --> 00:01:34,820
In the middle is the kernel and, on the right
is the convolution result.


00:01:36,320 --> 00:01:37,460
In this example,


00:01:37,780 --> 00:01:43,240
for each 3x3 block of pixels in the image
on the left, we multiply each pixel by the


00:01:43,240 --> 00:01:46,820
corresponding entry of the kernel and then
take the sum.


00:01:48,220 --> 00:01:51,620
That sum becomes a new pixel in the image
on the right.


00:01:53,360 --> 00:02:05,940
So 1 times 1, plus 2 times 1, plus 3 times
1, plus 4 times 1, plus 5 times 1, plus


00:02:05,940 --> 00:02:17,120
6 times 1, plus 7 times 1, plus 8 times 1, plus 9 times 1 equals = 45


00:02:19,100 --> 00:02:24,120
The kernel used in this example is a box blur,
which is a simple averaging kernel.


00:02:27,760 --> 00:02:33,260
Note that in this example, we will either
pre-normalize the weights so that their sum


00:02:33,260 --> 00:02:37,000
is already 1, by dividing each weight by the
sum of the weights.


00:02:38,280 --> 00:02:44,540
Or divide the sum of the products, i.e. the
result, by the sum of the weights, which is 9.


00:02:47,480 --> 00:02:50,360
In the upcoming lessons, 
we will pre-normalize the weights


00:02:50,460 --> 00:02:53,160
so that the convolution code could be simpler.


00:02:55,320 --> 00:02:58,480
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

18 - Convolution Kernels - Border Handling

----------------------------------------------------------------------------------------------------



00:00:05,180 --> 00:00:09,940
Kernel convolution usually requires values
from outside of the boundaries.


00:00:10,440 --> 00:00:15,680
In the case of images, these would be pixels
outside the image boundaries.


00:00:16,340 --> 00:00:20,180
The same concept also applies 
to volumes as 3d images.


00:00:21,380 --> 00:00:25,820
But when we are talking about arbitrary geometry, 
things are a bit different.


00:00:27,200 --> 00:00:31,140
For example, if you think about point clouds,
there are no borders.


00:00:32,380 --> 00:00:37,979
If you think about point neighbours using
connectivity, then you can consider open edges


00:00:37,980 --> 00:00:42,760
as borders, i.e. edges that are only shared
by a single polygon.


00:00:44,180 --> 00:00:50,480
But the same image-based border handling behaviours
are not clearly defined here, for all the cases.


00:00:51,520 --> 00:00:57,460
What we can do here as a meaningful operation
is, to exclude the points on open edges.


00:00:58,060 --> 00:01:03,500
For modifying point positions, this would
have the effect of constraining these points.


00:01:05,000 --> 00:01:10,200
As for image-based edges, there are a variety
of methods for handling these:


00:01:11,480 --> 00:01:18,300
Zero Padding: Any pixels outside
the borders would be filled with 0 values.


00:01:21,480 --> 00:01:29,140
Extend: The nearest border pixels are conceptually
extended as far as necessary to provide values


00:01:29,140 --> 00:01:30,380
for the convolution.


00:01:31,360 --> 00:01:35,260
Corner pixels are extended in 90 degree wedges.


00:01:35,760 --> 00:01:38,880
Other edge pixels are extended in lines.


00:01:42,100 --> 00:01:49,320
Wrap: The image is conceptually wrapped (or
tiled) and values are taken from the opposite


00:01:49,320 --> 00:01:50,800
edge or corner.


00:01:53,920 --> 00:01:58,920
Mirror: The image is conceptually mirrored
at the edges.


00:01:59,440 --> 00:02:06,460
For example, attempting to read a pixel 3
units outside an edge, reads the pixel 3 units


00:02:06,460 --> 00:02:08,040
inside the edge instead.


00:02:09,860 --> 00:02:12,840
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

19 - Convolution Kernels - Connectivity & k-Depth Point Neighbours Using Edges - Introduction

----------------------------------------------------------------------------------------------------



00:00:05,549 --> 00:00:10,330
The upcoming concepts I will show are more
useful when using connectivity information


00:00:10,330 --> 00:00:12,570
rather than proximity.


00:00:12,570 --> 00:00:18,539
The idea is if you have a polygonal geometry,
it's often better to use connectivity information


00:00:18,539 --> 00:00:23,240
unless you want to transfer arbitrary data
from one geometry onto another where there


00:00:23,240 --> 00:00:28,289
is no topological correspondence between them,
or you want to discard the topological structure


00:00:28,289 --> 00:00:30,520
of the geometry in your operation.


00:00:31,300 --> 00:00:36,440
In operations such as blurring attributes,
sharpening attributes, computing the gradient


00:00:36,440 --> 00:00:41,489
from an attribute or similar operations, using
the connectivity information creates more


00:00:41,489 --> 00:00:44,240
accurate results that eliminates ambiguity.


00:00:45,180 --> 00:00:48,680
To illustrate this, we can look at the Attribute Blur SOP.


00:00:50,040 --> 00:00:51,780
Let's bring in some geometry.


00:00:53,920 --> 00:00:55,500
Just create attribute blur.


00:00:56,420 --> 00:00:58,200
We are gonna use proximity first.


00:00:59,100 --> 00:01:00,340
Maybe 10 iterations.


00:01:03,640 --> 00:01:05,720
And compare this to connectivity.


00:01:06,880 --> 00:01:08,730
The result is wildly different.


00:01:09,180 --> 00:01:14,420
That's why for these types of operations,
it's much better to use connectivity rather


00:01:14,420 --> 00:01:15,640
than proximity.


00:01:16,700 --> 00:01:21,480
When you switch to connectivity, this is more
inline to what you would expect from such


00:01:21,480 --> 00:01:21,980
an operation.


00:01:23,160 --> 00:01:28,220
In VEX there are 2 functions to obtain
connectivity information using points,


00:01:28,220 --> 00:01:30,940
neighbour and neighbours functions.


00:01:31,740 --> 00:01:37,780
In all of my tools I tend to provide both
options, as Proximity, i.e. point clouds,


00:01:37,780 --> 00:01:38,880
and Connectivity.


00:01:39,620 --> 00:01:44,440
I abstracted these as separate operators that
makes it much easier to set these up.


00:01:45,480 --> 00:01:51,720
It also particularly helps when writing OpenCL
kernels, since we don't have dedicated functions


00:01:51,720 --> 00:01:55,480
for these in OpenCL, these have to be pre-computed.


00:01:56,270 --> 00:02:02,180
By using these dedicated SOPs, we can get
nearby points using proximity or connectivity


00:02:02,180 --> 00:02:04,440
very easily with 0 zero setup time.


00:02:05,540 --> 00:02:10,500
On top of this, I also implemented a depth
functionality for neighbour elements which


00:02:10,500 --> 00:02:14,860
allows gathering neighbours of neighbours
up to K-depth, which we will implement in


00:02:14,860 --> 00:02:15,620
this lesson.


00:02:16,220 --> 00:02:20,580
The reason I am showing these is to give you
a general idea of a generic implementation


00:02:20,580 --> 00:02:22,920
that you can use in your own workflow.


00:02:23,760 --> 00:02:28,160
We won't have to dive in this deep for this
course but we will use some of these concepts


00:02:28,160 --> 00:02:29,520
for the upcoming lessons.


00:02:30,460 --> 00:02:33,040
First off let's get an overview of the algorithm.






----------------------------------------------------------------------------------------------------

20 - Convolution Kernels - Connectivity & k-Depth Point Neighbours Using Edges - Concept

----------------------------------------------------------------------------------------------------



00:00:03,840 --> 00:00:06,700
Let's say we have a polygonal mesh like this.


00:00:11,440 --> 00:00:15,320
So we want to get the neighbours from this
point using edges.


00:00:16,020 --> 00:00:18,780
Let's use different colors to identify them easier.


00:00:20,320 --> 00:00:23,140
We get 4 points from this single point.


00:00:23,700 --> 00:00:29,560
In the next iteration, i.e. the next depth,
we will get the neighbours of these new points.


00:00:30,060 --> 00:00:32,980
So we will iterate through each of their own neighbours.


00:00:42,320 --> 00:00:45,720
From these 4 points, we get 8 more new points.


00:00:46,400 --> 00:00:48,380
We will store the result in an array.


00:00:49,040 --> 00:00:50,740
We will put the first point here.


00:00:51,760 --> 00:00:56,120
We will also keep track of the last points,
so we can use them in the next iteration.


00:00:59,440 --> 00:01:01,020
Let's say for depth 1,


00:01:03,580 --> 00:01:06,660
we will get the neighbours
from the first point in the points array.


00:01:14,180 --> 00:01:18,040
And append these new points to the points
array if they are unique.


00:01:22,380 --> 00:01:27,240
After this we set new points as the last points
to use them in the next iteration.


00:01:31,620 --> 00:01:37,700
So now we found 8 new points, and repeat the
same steps until all depths are met.


00:01:39,080 --> 00:01:43,900
This will be the body of the loop and then
return the points array.


00:01:44,380 --> 00:01:49,920
One important thing to note is we slice the
array before returning to exclude the first


00:01:49,920 --> 00:01:51,800
point which is the point itself.


00:01:52,760 --> 00:01:57,160
The reason for keeping the first point in
the points array is to avoid other neighbour


00:01:57,160 --> 00:01:59,600
points from accumulating this point itself.


00:02:00,720 --> 00:02:06,500
Additionally this also makes the code simpler
as we have the core logic inside a single for loop.


00:02:07,560 --> 00:02:09,780
Let's implement this in Houdini now.






----------------------------------------------------------------------------------------------------

21 - Convolution Kernels - Connectivity & k-Depth Point Neighbours Using Edges - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,700 --> 00:00:06,280
Create a wrangle node.


00:00:06,560 --> 00:00:11,980
We will write a function that returns an integer
array but there is a limitation in VEX, where


00:00:11,980 --> 00:00:16,300
if you try to return an array type directly
from a function it won't compile.


00:00:16,940 --> 00:00:20,520
So you have to prefix the array return type
with the function keyword first.


00:00:21,520 --> 00:00:28,460
So we can call it point neighbours by edges,
and then add arguments for input,


00:00:29,080 --> 00:00:33,120
index of the point and depth.


00:00:35,900 --> 00:00:41,660
Declare the points array and add the point itself to it.


00:00:45,020 --> 00:00:48,840
And create an array to store 
the result of the last iteration.


00:00:50,520 --> 00:00:51,920
Now create the for loop.


00:00:57,240 --> 00:00:59,680
Create an array to store the current neighbours.


00:01:00,540 --> 00:01:03,400
Then iterate through the points in the last points.


00:01:08,000 --> 00:01:09,440
We get the neighbours.


00:01:21,440 --> 00:01:26,920
If we don't find these new points, we append
them to the points and also the new points.


00:01:32,520 --> 00:01:35,120
After we gather all elements and the new points,


00:01:35,140 --> 00:01:37,580
we will assign the new points to the last points.


00:01:39,460 --> 00:01:42,600
Now just return the array excluding the first point.


00:01:44,360 --> 00:01:46,060
So if we call this function.


00:01:56,800 --> 00:01:59,860
We can check the result using 
the geometry spreadsheet.


00:02:04,860 --> 00:02:09,460
As we increase the depth we are getting the
neighbours of the neighbours and so on.


00:02:09,840 --> 00:02:12,200
We will extend this function further later on.


00:02:13,180 --> 00:02:18,720
In the next lesson, we will see how to implement
the same function using primitives instead of edges.


00:02:20,000 --> 00:02:21,800
See you guys in the next lesson.






----------------------------------------------------------------------------------------------------

22 - Convolution Kernels - Connectivity & k-Depth Point Neighbours Using Primitives - Concept

----------------------------------------------------------------------------------------------------



00:00:05,040 --> 00:00:10,260
The next logical step for us is to implement
getting neighbours using primitives rather than edges.


00:00:11,000 --> 00:00:17,400
This is particularly useful for grid based
kernels such as images or 3d volumes which


00:00:17,400 --> 00:00:19,400
are basically an array of images.


00:00:20,520 --> 00:00:24,420
Because we already implemented the edge version
of this function, we only have to replace


00:00:24,420 --> 00:00:29,880
a few lines and swap out the edge based neighbour
functions with primitive based neighbour functions.


00:00:30,840 --> 00:00:33,180
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

23 - Convolution Kernels - Connectivity & k-Depth Point Neighbours Using Primitives - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,515 --> 00:00:06,635
First rename the node for clarity


00:00:08,185 --> 00:00:09,605
and copy paste it.


00:00:19,585 --> 00:00:22,445
So instead of the neighbours function, we first


00:00:22,445 --> 00:00:25,840
need to gather the list of primitives 
that belong to the current point.


00:00:27,400 --> 00:00:30,080
pointprims function gives us this list.


00:00:43,700 --> 00:00:48,280
Now we are gonna get the points of these
primitives using the primpoints function.


00:00:54,380 --> 00:00:57,260
And with that the implementation is complete.


00:01:08,460 --> 00:01:14,300
For this particular model the result is not very different
because the entire model is made up of triangles.


00:01:15,320 --> 00:01:17,520
But the function is indeed working.


00:01:18,220 --> 00:01:20,860
We will extend this function further later on.


00:01:23,300 --> 00:01:25,060
See you guys in the next lesson.






----------------------------------------------------------------------------------------------------

24 - Convolution Kernels - Extending k-Depth Point Neighbours Using Edges - Introduction

----------------------------------------------------------------------------------------------------



00:00:04,920 --> 00:00:10,880
The k-depth neighbour functions are sufficient
for their actual purpose, but for attribute


00:00:10,880 --> 00:00:18,660
convolution, i.e. aggregating, summing, averaging,
etc, we need 2 more crucial pieces of information


00:00:18,660 --> 00:00:20,220
from these functions.


00:00:21,560 --> 00:00:25,920
First off we need to be able to include
the point itself in the resulting array.


00:00:26,720 --> 00:00:32,480
Often times the point itself can also contribute
to the final result computed from its neighbours.


00:00:33,600 --> 00:00:38,140
Returning this in the same array simplifies
the kernel code we have to write later on


00:00:38,140 --> 00:00:43,800
as we can handle all the points uniformly,
instead of making an exception for the current point.


00:00:44,640 --> 00:00:49,040
Secondly we need to compute the weights
of each neighbour including itself,


00:00:49,040 --> 00:00:52,160
which is how much they will contribute to the final result.


00:00:53,680 --> 00:00:55,960
Let's look at the point cloud example first.


00:00:55,960 --> 00:00:57,340
This is the easiest.


00:01:01,000 --> 00:01:09,200
You take the radius, which is the furthest
point found and normalize all the distances,


00:01:09,200 --> 00:01:11,220
using this max distance.


00:01:14,320 --> 00:01:16,680
This will give us a normalized linear curve.


00:01:19,000 --> 00:01:21,620
Now let's look at the point neighbours using edges.






----------------------------------------------------------------------------------------------------

25 - Convolution Kernels - Extending k-Depth Point Neighbours Using Edges - Concept

----------------------------------------------------------------------------------------------------



00:00:05,155 --> 00:00:07,195
This is the second easiest.


00:00:07,700 --> 00:00:11,340
You have to remember, here we can't rely on distances,


00:00:12,340 --> 00:00:15,420
otherwise the connectivity information becomes useless.


00:00:16,280 --> 00:00:19,380
Instead we have to use the depth information


00:00:19,380 --> 00:00:23,980
as our distance metric to compute
how far they are from the original point.


00:00:27,260 --> 00:00:30,380
For this, you take the depth for each point


00:00:30,425 --> 00:00:33,245
and divide the result by max depth.


00:00:34,260 --> 00:00:36,360
So max depth will be 1


00:00:37,260 --> 00:00:39,400
and the point itself will be 0.


00:00:40,260 --> 00:00:45,140
This will also give us a normalized linear
curve that's spread evenly among each depth.


00:00:46,080 --> 00:00:48,800
Now let's look at how to implement this in Houdini.






----------------------------------------------------------------------------------------------------

26 - Convolution Kernels - Extending k-Depth Point Neighbours Using Edges - Implementation

----------------------------------------------------------------------------------------------------



00:00:05,420 --> 00:00:10,200
First we extend the function to allow specifying
whether to include the point itself,


00:00:12,260 --> 00:00:18,040
whether to compute the weights and if so, to take
another array as a parameter and modify this,


00:00:18,380 --> 00:00:21,380
since VEX doesn't allow multiple returns like Python,


00:00:21,960 --> 00:00:23,840
we can only return the neighbouring


00:00:23,840 --> 00:00:29,340
points from the function while we can modify
another array by reference using another parameter.


00:00:31,700 --> 00:00:38,680
If compute weights is True and include self
is True, then we add 0 as the weight of the current point.


00:00:39,360 --> 00:00:44,840
Otherwise just ignore it, as the point itself
will be removed, before returning the result.


00:00:47,220 --> 00:00:49,620
Now we define the depth multiplier.


00:00:50,040 --> 00:00:55,200
We do it this way to avoid performing divisions
for each point which has a performance cost.


00:00:55,960 --> 00:01:01,420
Always avoid divisions and roots like square
root, cube root, etc if you can.


00:01:02,940 --> 00:01:06,880
Now we populate the weights array if compute
weights is True.


00:01:09,360 --> 00:01:16,320
We have to add 1 to i because i is always
1 less than the current depth since it starts from 0.


00:01:17,500 --> 00:01:23,820
And finally we perform an early return if
include self is True, so that we don't slice the array.


00:01:24,820 --> 00:01:29,380
The weights array is already populated at
this point if compute weights was True.


00:01:30,600 --> 00:01:34,020
Now just modify the function call to include
the new parameters.


00:01:42,640 --> 00:01:47,260
Here I am using an array attribute export
syntax so that Houdini will write directly


00:01:47,260 --> 00:01:51,420
to the exported array attribute, instead of
having to assign the weights array later on.


00:01:53,780 --> 00:01:58,020
Now create the extra parameters, and convert
them into toggles.


00:02:01,400 --> 00:02:04,820
Let's bring up the geometry spreadsheet, and hide P.


00:02:10,100 --> 00:02:11,800
You can see the point itself in the


00:02:11,800 --> 00:02:14,480
returned array when include self is True.


00:02:16,520 --> 00:02:19,920
Now look at the weights array, you can see
all the values are 1,


00:02:21,200 --> 00:02:24,660
and if you include the current point, you can see it's 0.


00:02:27,620 --> 00:02:32,720
As you increase the number of depths, you
start seeing the intermediate values as expected.


00:02:33,240 --> 00:02:34,660
So this is working fine.


00:02:36,500 --> 00:02:39,300
Now let's look at the neighbour points using primitives.






----------------------------------------------------------------------------------------------------

27 - Convolution Kernels - Extending k-Depth Point Neighbours Using Primitives - Concept

----------------------------------------------------------------------------------------------------



00:00:04,400 --> 00:00:07,240
This is a bit more involved than the other 2 cases.


00:00:07,540 --> 00:00:14,220
As you can see we have edge neighbours but
also primitive neighbours, so we have to distinguish


00:00:14,220 --> 00:00:15,800
between them in the code.


00:00:17,300 --> 00:00:20,220
We first mark the edge neighbours as sides.


00:00:20,560 --> 00:00:23,600
Then identify the primitive neighbours as corners.


00:00:25,040 --> 00:00:31,820
Corners are further than sides, and as such
we assume the furthest corner to have a radius of 1.


00:00:34,540 --> 00:00:36,900
From there it's simple trigonometry.


00:00:37,460 --> 00:00:43,440
If the radius is 1, i.e. the hypotenuse, 
then call the sides x.


00:00:44,580 --> 00:00:48,920
Hypotenuse squared is x squared plus x squared.


00:00:51,920 --> 00:00:54,000
So 2 x squared is 1,


00:00:57,340 --> 00:00:59,180
so x squared is 0.5,


00:01:02,080 --> 00:01:04,920
and x becomes the square root of 0.5.


00:01:08,900 --> 00:01:13,300
So the furthest side will have a radius of
square root of 0.5.


00:01:14,380 --> 00:01:19,180
Let's call corner radius cr and side radius
sr.


00:01:20,280 --> 00:01:21,540
d is depth.


00:01:28,420 --> 00:01:32,320
Corner weight is depth divided by max depth.


00:01:34,740 --> 00:01:35,980
Side weight is the same.


00:01:38,660 --> 00:01:41,900
Except we multiply the result by side radius.


00:01:43,020 --> 00:01:48,940
We also do this for corner weight but because
the corner radius is 1, we can neglect that.


00:01:50,860 --> 00:01:52,060
Now the fun part.






----------------------------------------------------------------------------------------------------

28 - Convolution Kernels - Extending k-Depth Point Neighbours Using Primitives - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,640 --> 00:00:07,060
We will copy a lot of code from the edge neighbours.


00:00:16,020 --> 00:00:20,600
Ok first copy the first part of the function
as the weight computation is the same.


00:00:22,540 --> 00:00:24,020
Then rename the function.


00:00:27,460 --> 00:00:30,400
Use a calculator to compute the square root
of 0.5.


00:00:31,620 --> 00:00:34,880
We will just store the result instead of having
to calculate it each time.


00:00:36,020 --> 00:00:37,800
This is our max side radius.


00:00:39,660 --> 00:00:41,500
Now an important distinction.


00:00:42,020 --> 00:00:46,460
The current function we implement correctly
returns the point neighbours using primitives


00:00:46,780 --> 00:00:48,820
but it doesn't distinguish between them.


00:00:49,900 --> 00:00:54,660
That's why inside the first loop, we have
to find the edge neighbours first, so we can


00:00:54,660 --> 00:00:59,060
assign the correct weights to them, then do
the second pass for the prim neighbours.


00:01:00,880 --> 00:01:02,940
Copy the weight assignment code here too.


00:01:14,320 --> 00:01:17,440
We perform an early return the same as the
other function.


00:01:21,400 --> 00:01:22,920
And modify the function call.


00:01:30,160 --> 00:01:36,020
If you remember, the weighting of the prim
neighbours is fine as is, so can leave it like that.


00:01:38,320 --> 00:01:42,480
But for the edge neighbours, we multiply the
result by the side radius.


00:01:44,640 --> 00:01:47,120
There is another important difference in this function.


00:01:47,940 --> 00:01:50,180
We have to do a post normalization.


00:01:50,720 --> 00:01:56,100
You might wonder why, as the corner radius
is already 1, therefore all the values should


00:01:56,100 --> 00:01:57,320
already be normalized.


00:01:57,960 --> 00:02:02,840
But the thing is, if a point doesn't have
more neighbours using primitives than using


00:02:02,840 --> 00:02:07,180
only edges, our maximum weight would be stuck
at the square root of 0.5.


00:02:08,180 --> 00:02:10,760
Imagine a point that's only shared by triangles.


00:02:11,440 --> 00:02:16,000
This point wouldn't have corner points, that's
why we have to post normalize our values


00:02:16,040 --> 00:02:18,040
so the maximum value is always 1.


00:02:22,440 --> 00:02:24,220
Just loop over the weights array


00:02:37,240 --> 00:02:40,940
and multiply each weight by 1 divided by max weights.


00:02:43,960 --> 00:02:47,140
Let's comment out this part for now so that
we can see the difference.


00:02:49,780 --> 00:02:53,060
Just create the new parameters and convert
them into toggles.


00:02:54,980 --> 00:02:56,980
Now if you look at the geometry spreadsheet


00:03:00,420 --> 00:03:02,600
you can see the point itself like before


00:03:02,860 --> 00:03:04,320
when include self is on.


00:03:05,320 --> 00:03:07,080
Now take a look at the weights,


00:03:08,840 --> 00:03:12,620
you can see they are not 1, but the square root of 0.5.


00:03:13,500 --> 00:03:15,860
If you uncomment the code we left out,


00:03:20,080 --> 00:03:22,100
suddenly all the weights become 1.


00:03:28,900 --> 00:03:32,940
And as you increase the depth you will see
more intermediate values.


00:03:33,520 --> 00:03:35,580
So the code is working as we intended.


00:03:37,140 --> 00:03:39,360
We are now ready to take our linear weights


00:03:39,440 --> 00:03:42,640
and shape them in the next 
chapter using the shape functions.


00:03:45,460 --> 00:03:48,100
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

29 - Convolution Kernels - smoothstep() [Cubic Hermite Interpolation] - Concept

----------------------------------------------------------------------------------------------------



00:00:04,940 --> 00:00:12,100
Smoothstep is a family of sigmoid-like interpolation
and clamping functions commonly used in computer


00:00:12,100 --> 00:00:14,260
graphics and video game engines.


00:00:15,140 --> 00:00:21,480
smoothstep() takes an input value and performs
cubic Hermite interpolation between a min


00:00:21,480 --> 00:00:25,440
and max value, returning a value in the range of 
0.0 to 1.0.


00:00:27,780 --> 00:00:30,000
So let's say our value is x.


00:00:31,840 --> 00:00:40,520
x becomes x square times in parenthesis 3-2x.


00:00:42,140 --> 00:00:45,980
In Houdini it's provided as the smooth function in VEX.


00:00:47,000 --> 00:00:53,720
For this example, I am gonna assume 0-1 range,
as the shaping functions we will implement later on,


00:00:53,720 --> 00:00:59,280
will assume the same, unlike the Houdini function
which allows you to specify a min and max


00:00:59,300 --> 00:01:03,160
range to normalize the input value into a 0-1 range.


00:01:04,040 --> 00:01:06,720
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

30 - Convolution Kernels - smoothstep() [Cubic Hermite Interpolation] - Implementation

----------------------------------------------------------------------------------------------------



00:00:05,520 --> 00:00:08,820
I am gonna create a grid to show up 
as bounds for the curve.


00:00:09,920 --> 00:00:13,780
And offset it by half so the minimum bounds
sit at the origin.


00:00:14,620 --> 00:00:18,000
Set it 1 by 1 with divisions 2 by 2.


00:00:19,140 --> 00:00:22,840
Create a line with 100 points in X axis direction.


00:00:30,240 --> 00:00:31,620
Create a wrangle node.


00:00:33,160 --> 00:00:37,920
I am just gonna create a basic linear interpolation
by setting P.y to P.x.


00:00:44,960 --> 00:00:48,040
Create a wrangle node for the default smooth
to compare.


00:00:52,040 --> 00:00:55,040
Then create another wrangle node for smootstep.


00:01:09,680 --> 00:01:13,600
Now we can see the cubic Hermite interpolation
of the linear curve.


00:01:17,640 --> 00:01:22,240
Using the formula we just looked at, the implementation
is pretty straight forward.


00:01:45,760 --> 00:01:49,000
As you can see the results are identical.


00:01:51,280 --> 00:01:55,440
In the next lesson, we will implement the
other shaping functions from scratch.


00:01:56,000 --> 00:01:59,960
We will assume the same normalized range within
the body of the function.


00:02:00,240 --> 00:02:06,080
But outside the function, whatever input ranges
our values might have, they will be normalized.


00:02:07,280 --> 00:02:10,280
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

31 - Convolution Kernels - Shaping Functions - Introduction

----------------------------------------------------------------------------------------------------



00:00:05,180 --> 00:00:11,740
Shaping functions allow us to shape signals,
and to calculate a transition between discrete points.


00:00:12,820 --> 00:00:18,000
We can use them to take a linear curve and
turn it into an ease in ease out curve for example.


00:00:19,100 --> 00:00:24,600
We need to use these functions to modify the
influence or contribution of each point in our kernels.


00:00:25,780 --> 00:00:29,360
Note that for our purposes we are working
in a normalized space.


00:00:29,900 --> 00:00:33,520
So whatever the incoming values might be,
we are normalizing them.


00:00:40,520 --> 00:00:43,340
Here I gathered 6 common shaping functions.


00:00:44,060 --> 00:00:49,400
Linear is 1 to 1 mapping without any sort
of modification to the incoming value.


00:00:50,060 --> 00:00:55,100
Smooth is actually hermite interpolation that's
provided by default in Houdini.


00:00:55,740 --> 00:01:00,440
Elendt (Alan), not sure if I am pronouncing
it correctly from SideFX's Mark Elendt (Alan),


00:01:00,440 --> 00:01:04,840
a method that allows fast computation even
if not the highest quality.


00:01:05,580 --> 00:01:10,760
There are also others like Links, RenderMan,
but these are not as commonly popular but


00:01:10,760 --> 00:01:12,540
they are still very useful in CG.


00:01:13,340 --> 00:01:18,680
Gaussian is also extremely useful, and it
requires an additional parameter called Sigma


00:01:19,160 --> 00:01:21,480
which alters the width of the bell shaped curve.


00:01:22,760 --> 00:01:24,640
Let's demonstrate it in Houdini now.






----------------------------------------------------------------------------------------------------

32 - Convolution Kernels - Shaping Functions - Implementation

----------------------------------------------------------------------------------------------------



00:00:05,000 --> 00:00:08,040
I am gonna create a grid to show up 
as bounds for the curve.


00:00:10,500 --> 00:00:13,800
Set it 1 by 1 with divisions 2 by 2,


00:00:13,940 --> 00:00:18,160
and offset it by half so the minimum
 bounds sit at the origin.


00:00:18,800 --> 00:00:20,920
Set its orientation to XY.


00:00:24,760 --> 00:00:27,880
Create a line with 100 points in X axis direction.


00:00:32,600 --> 00:00:37,120
We will use the pointNeighboursUsingEdges
function we implemented previously.


00:00:37,460 --> 00:00:41,520
So copy paste the wrangle node that contains
this function definition.


00:00:42,220 --> 00:00:45,460
We are just using this line to create 
an array of points and weights


00:00:45,460 --> 00:00:48,320
where we can test out shaping functions.


00:00:48,720 --> 00:00:52,940
We need them to be in this format so 
we can use the same shaping functions


00:00:53,000 --> 00:00:54,760
without altering them later on.


00:00:56,060 --> 00:00:58,880
Toggle Include Self and Compute Weights.


00:01:07,620 --> 00:01:09,160
Set depth to 100,


00:01:12,880 --> 00:01:16,440
and set group to 0 and group type to points.


00:01:16,820 --> 00:01:19,840
So that we only compute neighbours for the first point.


00:01:20,540 --> 00:01:23,220
The other points will use this array also.


00:01:24,900 --> 00:01:26,440
Create a wrangle node.


00:01:27,080 --> 00:01:29,940
We will use this to implement the shaping functions.


00:01:32,200 --> 00:01:34,000
Create another wrangle node.


00:01:34,280 --> 00:01:36,620
We will use this to draw the final curve.


00:01:38,520 --> 00:01:43,680
We will do this by reading the weights array
and then indexing into it using the current


00:01:43,700 --> 00:01:45,800
point number to set P.y.


00:01:54,160 --> 00:01:59,080
Now we will create 2 parameters, one for the
Kernel, and one for Sigma.


00:02:13,520 --> 00:02:17,060
I am just gonna populate the menu parameter
with the shaping function names.


00:02:35,700 --> 00:02:38,180
Now we have to implement the shaping functions.


00:02:43,960 --> 00:02:48,400
We have to compute the min and max of the
weights array so we can normalize it.


00:03:07,280 --> 00:03:12,480
Normally shaping functions take a 0-1 curve
and map it to a 0-1 curve.


00:03:12,840 --> 00:03:16,760
But we will actually invert it and map it
to a 1-0 curve.


00:03:17,440 --> 00:03:23,380
The reason for this is simple: For us when
an input value is between 0 and 1,


00:03:23,880 --> 00:03:26,440
0 represents the highest contributor,


00:03:26,740 --> 00:03:28,480
it's the closest to the source,


00:03:29,100 --> 00:03:31,300
think of it in the case of the nearest point


00:03:31,720 --> 00:03:35,040
or the lowest cost output
by the Find Shortest Path SOP,


00:03:35,300 --> 00:03:41,880
so when we get an input value of 0, 
we want to invert it and make it 1, before normalization.


00:03:42,520 --> 00:03:46,780
We want the element closest to the source
to have the highest influence.


00:03:56,920 --> 00:04:01,040
That's why as you can see we are inverting
the weights values here.


00:04:02,540 --> 00:04:06,180
Let's assign the modified weights array back
to the weights attribute.


00:04:09,780 --> 00:04:15,000
As you can see, our 0-1 weights show up as
1-0 in linear form.


00:04:15,740 --> 00:04:18,080
Now we will implement the hermite kernel.


00:04:18,280 --> 00:04:20,620
We will just use the standard smooth function.


00:04:39,280 --> 00:04:41,440
You can see that's working as well.


00:04:42,900 --> 00:04:47,900
The formulas I showed you elendt, 
links, renderman and gaussian,


00:04:47,900 --> 00:04:49,480
in the beginning of this video


00:04:49,480 --> 00:04:54,000
are already inverted, so we have to reserve
our invert weights array logic


00:04:54,000 --> 00:04:56,020
only for linear and hermite.


00:04:56,580 --> 00:05:00,060
Otherwise just normalize the weights array
without inverting it.


00:05:20,180 --> 00:05:22,640
Now we are gonna implement the rest of the kernels.


00:05:23,020 --> 00:05:27,460
It's just about translating the formulas shown
in the beginning of the video.


00:05:27,700 --> 00:05:31,060
So there is no need to narrate each of these
while we implement.


00:05:31,060 --> 00:05:32,880
It should be pretty straight forward.


00:08:09,120 --> 00:08:13,580
You can see how changing the sigma
parameter affects the resulting gaussian curve.


00:08:15,320 --> 00:08:17,820
Now let's talk about the minimum contribution.


00:08:18,240 --> 00:08:23,040
Inverting the input values is ok but now it
creates another problem.


00:08:23,960 --> 00:08:25,880
To see this, create a grid.


00:08:26,420 --> 00:08:31,140
Copy paste both the point_neighbours_by_edges
and shaping_functions wrangle nodes.


00:08:40,440 --> 00:08:41,840
Clear the group parameter.


00:08:50,620 --> 00:08:55,320
As you can see in these point weights, 
the point itself has a weight of 1,


00:08:55,800 --> 00:08:58,660
but now the neighbours have a weight of almost 0,


00:08:58,900 --> 00:09:00,260
in the case of gaussian.


00:09:00,640 --> 00:09:02,600
But if you look at the other kernels,


00:09:04,080 --> 00:09:05,980
you can see that they are actually 0.


00:09:06,940 --> 00:09:09,140
This will effectively render them obsolete.


00:09:09,800 --> 00:09:13,100
You can increase the depth or distance of
the point cloud look up


00:09:13,420 --> 00:09:16,420
but then the last elements will still be discarded.


00:09:17,320 --> 00:09:20,340
It's computationally not wise to traverse
the geometry,


00:09:20,620 --> 00:09:23,160
store these and iterate over these


00:09:23,220 --> 00:09:28,040
just to discard them mathematically
by multiplying by 0 or near 0.


00:09:29,160 --> 00:09:32,420
Let's take a look at the pcfilter function
that we implemented.


00:09:35,780 --> 00:09:38,560
You see the 1.1 times maxdistance.


00:09:39,100 --> 00:09:44,080
What this does is, it will make your 
max value, not 1, but less than 1,


00:09:44,080 --> 00:09:46,600
but because there is 1 - before the smooth,


00:09:47,200 --> 00:09:52,360
your less than 1 value,
say 0.9 as the max, becomes 0.1,


00:09:52,540 --> 00:09:54,780
and this becomes the minimum contribution.


00:09:55,420 --> 00:09:59,780
In our hermite shape function, 
we are not doing 1 minus,


00:10:00,260 --> 00:10:02,800
but rather inverting the values beforehand,


00:10:02,980 --> 00:10:04,720
but the end result is the same.


00:10:05,600 --> 00:10:08,180
So let's implement a similar thing for our code.


00:10:12,480 --> 00:10:15,575
But instead of a multiplier like 1.1,


00:10:15,700 --> 00:10:19,360
we will make it 0.1, so it's easier to conceptualize


00:10:19,360 --> 00:10:22,640
in terms of adding 10 percent 
to the minimum weights value,


00:10:22,640 --> 00:10:24,860
which will be our minimum contribution.


00:10:26,600 --> 00:10:30,320
That's why we have to add 1
to our min contribution value.


00:10:36,380 --> 00:10:39,380
Now you can see how this alters the resulting curve.


00:10:39,580 --> 00:10:43,480
We are basically elevating the minimum
weights values by a percentage.


00:10:59,560 --> 00:11:03,400
We can use 0.1 to mimic the 
default pcfilter implementation.


00:11:06,180 --> 00:11:08,340
Let's look at it on the grid geometry.


00:11:19,940 --> 00:11:24,040
As you can see the furthest
neighbours are not 0 anymore.


00:11:24,820 --> 00:11:28,940
This is also one of the keys to making sure
each kernel gives us a different result.


00:11:34,440 --> 00:11:37,560
There is one last operation we have to do
to our weights.


00:11:37,940 --> 00:11:40,440
Let's first compute the sum of our weights.


00:11:46,060 --> 00:11:49,560
As you can see, it's 42.8.


00:11:50,080 --> 00:11:53,580
We don't just want each individual weight
to be normalized by itself


00:11:54,100 --> 00:11:57,140
but the total sum of all the weights should also be 1.


00:11:57,720 --> 00:12:02,460
We can achieve this by post normalizing the
weights by the sum of the weights.


00:12:02,980 --> 00:12:06,240
This is to ensure that the summed total of
our attribute values


00:12:06,500 --> 00:12:08,700
do not go over the original range.


00:12:09,260 --> 00:12:12,460
Otherwise in the case of colors for example,


00:12:12,580 --> 00:12:14,800
the new colors would be overexposed.


00:12:15,360 --> 00:12:18,100
The post normalization by the sum of the weights


00:12:18,300 --> 00:12:22,500
is the same as the one I showed you for pointNeighboursUsingPrims function


00:12:37,300 --> 00:12:39,060
If you look at the weights values,


00:12:39,060 --> 00:12:42,300
you can see they are now normalized 
by the sum of the weights.


00:12:43,420 --> 00:12:45,900
I am just gonna remove this code to see the difference.


00:12:52,380 --> 00:12:54,260
And now I will just undo.


00:12:58,500 --> 00:13:00,480
Now we can look at the sum of the weights.


00:13:01,520 --> 00:13:04,980
Now as you can see the sum of the weights is 1,


00:13:05,280 --> 00:13:07,200
which is exactly what we want.


00:13:09,600 --> 00:13:12,620
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

33 - Convolution Kernels - Blurring Attributes

----------------------------------------------------------------------------------------------------



00:00:04,720 --> 00:00:08,060
Up until here, we worked on the infrastructure to be


00:00:08,060 --> 00:00:11,860
able to perform convolution
now we are ready to do that.


00:00:12,500 --> 00:00:17,340
We'll start with an image based example first
as it is the easiest to visualize.


00:00:22,240 --> 00:00:23,520
Create a grid first.


00:00:24,280 --> 00:00:27,200
We will copy code we wrote in earlier chapters.


00:00:28,160 --> 00:00:33,440
One is point neighbors by prims function,
and other is the wrangle that contains


00:00:33,440 --> 00:00:34,680
the shaping functions.


00:00:35,740 --> 00:00:37,900
Make sure you set depth to one.


00:00:38,700 --> 00:00:41,380
And include self and compute weights is on.


00:00:48,480 --> 00:00:52,680
For shaping functions, make sure to use
Gaussian kernel initially,


00:00:53,080 --> 00:00:58,160
and set Sigma to 0.5 and Minimum
Contribution to 0.1.


00:00:58,960 --> 00:01:01,359
We will use Attribute from Map SOP


00:01:01,360 --> 00:01:03,660
to set point colors using an image.


00:01:14,880 --> 00:01:19,759
I will manually set the right aspect ratio.
 We can do this procedurally using


00:01:19,759 --> 00:01:23,680
the res expression but it requires a COP network, and we


00:01:23,680 --> 00:01:24,860
don't need to do that.


00:01:25,320 --> 00:01:27,600
We can also use the PIL python image


00:01:27,600 --> 00:01:31,060
library to do this
without having to create a COP network,


00:01:31,520 --> 00:01:35,940
but because we are not trying to create a
generic HDA, it's not necessary.


00:01:36,780 --> 00:01:40,220
Create a new parameter for controlling
resolution of the grid.


00:01:44,000 --> 00:01:47,160
Because my grid is 100 times smaller
than the image size


00:01:47,440 --> 00:01:53,300
here I am using 50 to have it half res 
by multiplying the size with my resolution.


00:02:07,120 --> 00:02:09,580
we almost have half a million points.


00:02:14,480 --> 00:02:17,200
Now create a feedback loop for convolution.


00:02:17,920 --> 00:02:20,560
The convolution code is pretty
straightforward


00:02:21,440 --> 00:02:25,599
All we have to do is loop over the
points array that also contains the


00:02:25,599 --> 00:02:29,200
point itself, get the attribute value we want, which is


00:02:29,200 --> 00:02:32,720
the color in this case,
multiply it with the corresponding


00:02:32,720 --> 00:02:36,840
weight value from the weights array
and then sum them.


00:03:09,280 --> 00:03:12,040
Finally assign the result back onto color.


00:03:12,720 --> 00:03:16,840
You don't have to worry about normalization, 
the weights are already normalized.


00:03:17,780 --> 00:03:21,640
Create the parameters.
set attribute to Cd.


00:03:23,360 --> 00:03:26,360
You can immediately see the blurring effect.


00:03:32,600 --> 00:03:35,360
When you change the kernel, you can see


00:03:35,360 --> 00:03:37,080
how it changed the final result.


00:03:38,800 --> 00:03:40,480
Some of them are pretty close to each


00:03:40,480 --> 00:03:44,560
other but it is similar to how the
kernels used in Attribute Transfer


00:03:44,560 --> 00:03:47,020
do not drastically alter the final
result.


00:03:55,600 --> 00:03:59,700
The higher the iterations, the kernels start
to diverge even more.


00:04:12,800 --> 00:04:16,880
If you set the grid resolution higher, 
we will have a lot more points


00:04:16,880 --> 00:04:21,260
with a lot more detail and as such the
blurring effect will look weaker


00:04:21,260 --> 00:04:23,120
with the same number of iterations.


00:05:05,520 --> 00:05:10,040
You can see the gaussian is really
strong, because the sigma is quite high.


00:05:11,520 --> 00:05:15,440
Reducing the sigma value, reduces the
strength of the gaussian blur.


00:05:16,080 --> 00:05:20,560
Gaussian blur is a very high quality
blur filter that can be applied to any


00:05:20,560 --> 00:05:25,200
attribute or volume data and produce 
very smooth results.


00:05:59,680 --> 00:06:03,940
I find values like 0.344 to give quite good results.


00:06:04,840 --> 00:06:08,840
Let's look at applying the same blur 
convolution to P instead.


00:06:15,360 --> 00:06:16,880
Delete these nodes.


00:06:17,660 --> 00:06:19,480
Bring in some geometry.


00:06:20,320 --> 00:06:24,540
I will use the remeshed statue model that
has uniform geometry detail.


00:06:25,340 --> 00:06:27,960
Create some points with random colors.


00:06:36,240 --> 00:06:39,780
We will transfer these colors using Attribute Transfer.


00:07:01,920 --> 00:07:04,700
You can see the colors are blurred out nicely.


00:07:11,900 --> 00:07:13,840
Experiment with different kernels.


00:07:25,600 --> 00:07:28,000
You can see the effect of the sigma here too.


00:07:33,200 --> 00:07:36,400
Copy paste the for loop to use P now instead.


00:07:44,060 --> 00:07:46,460
You can see P works just as well.


00:08:08,420 --> 00:08:12,060
Let's delete some polygons to show 
what happens to borders.


00:08:17,600 --> 00:08:22,540
As you can see the borders shrink.
Depending on what you are smoothing,


00:08:22,800 --> 00:08:25,420
you might want to exclude them in some cases.


00:08:26,140 --> 00:08:28,440
It's easy to do with the Group SOPs.


00:09:07,280 --> 00:09:11,120
You can see when we exclude the borders, they get stiff,


00:09:11,120 --> 00:09:15,140
in example, they are constrained so that
they are not smoothed.


00:09:17,520 --> 00:09:20,959
Another important thing to mention is,
when you modify


00:09:20,960 --> 00:09:25,060
P most of the time you also want to
update the point normals.


00:09:25,840 --> 00:09:28,880
You can do this using the attribute triangle.


00:09:28,880 --> 00:09:33,279
But because we are doing it inside a for
loop, and the normals are not taken into


00:09:33,279 --> 00:09:36,560
account inside the loop, it is better to do this


00:09:36,560 --> 00:09:39,200
outside the loop, for better performance.


00:09:39,840 --> 00:09:42,740
Also using the Normal SOP gives us more options.


00:09:46,640 --> 00:09:49,600
As you can see the geometry looks better now.


00:09:55,760 --> 00:10:01,080
You can experiment with the kernels like
before and understand their differences visually.


00:10:20,820 --> 00:10:24,140
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

34 - Convolution Kernels - Sharpening Attributes Using Unsharp Mask - Concept

----------------------------------------------------------------------------------------------------



00:00:10,380 --> 00:00:17,300
Unsharp mask is a technique that sharpens
data by enhancing edges via a procedure that


00:00:17,300 --> 00:00:21,240
subtracts a blurred version of the same data
from the original data.


00:00:22,300 --> 00:00:28,080
It's a very useful technique to give the illusion
of having more detail than there actually is.


00:00:29,320 --> 00:00:31,540
Let's demonstrate it in Houdini now.






----------------------------------------------------------------------------------------------------

35 - Convolution Kernels - Sharpening Attributes Using Unsharp Mask - Implementation

----------------------------------------------------------------------------------------------------



00:00:03,520 --> 00:00:05,880
Start from the previous topic's scene file.


00:00:06,440 --> 00:00:09,880
We will apply the unsharp operation
after the blurring,


00:00:10,000 --> 00:00:11,220
not inside the loop.


00:00:12,600 --> 00:00:15,280
It's translating what
we just saw in the concept of the


00:00:15,280 --> 00:00:16,520
algorithm into code.


00:00:31,340 --> 00:00:34,520
The amount controls how much sharpen to apply.


00:00:35,860 --> 00:00:37,840
The amount of blurring you apply


00:00:38,000 --> 00:00:40,800
controls what sort of details are sharpened.


00:00:41,160 --> 00:00:44,239
Generally, you want to be subtle to bring out finer


00:00:44,239 --> 00:00:47,440
details and not overdo it, both in terms of


00:00:47,440 --> 00:00:51,280
blurring iterations but also the amount of sharpening.


00:00:53,280 --> 00:00:55,700
you can experiment with  different kernels.


00:02:02,400 --> 00:02:05,820
We can copy the same wrangle into the 3d example.


00:02:19,680 --> 00:02:24,000
As you can see, the colors on the 3d
model are clearly getting sharper.


00:03:11,120 --> 00:03:16,600
When applied to P, you can see the effect
exaggerates the sharper features of the model.


00:03:17,660 --> 00:03:20,400
In human-like models, it almost looks


00:03:20,400 --> 00:03:25,599
like the geometry becomes more masculine.
As we generally associate squared of


00:03:25,600 --> 00:03:27,580
features as more masculine.


00:03:28,240 --> 00:03:32,480
I have used the same technique
on volumes in pretty much most of the


00:03:32,480 --> 00:03:36,420
movies I worked on
including Aquaman and Shazam.


00:03:37,840 --> 00:03:42,319
When done right on volume simulations,
you can convince your supervisor,


00:03:42,320 --> 00:03:44,860
it is indeed a high resolution simulation,


00:03:45,440 --> 00:03:50,000
because we are not applying sharpen to a
2d representation of the volume data,


00:03:50,000 --> 00:03:54,880
but rather apply it in 3d which creates
a lot better results.


00:04:15,460 --> 00:04:18,740
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

36 - Convolution Kernels - Generalizing the Kernel Code to Handle All Attribute Types - Concept

----------------------------------------------------------------------------------------------------



00:00:04,850 --> 00:00:11,120
In this lesson, I will show you how to write
generic VEX code to work on multiple attributes


00:00:11,120 --> 00:00:12,920
using the least amount of code.


00:00:13,460 --> 00:00:19,200
The most performant approach depends on the
use case, i.e. whether the wrangle node will


00:00:19,200 --> 00:00:24,260
be inside a for loop, whether the code will
change at each iteration, etc.


00:00:24,960 --> 00:00:30,980
It requires a great attention to details to
ensure your code is not recompiled at each iteration.


00:00:31,540 --> 00:00:36,040
But the approach I will show you takes the
least amount of time to implement it.


00:00:37,420 --> 00:00:42,600
There are a lot of nuances and methodologies,
but there is a common scenario for needing


00:00:42,600 --> 00:00:47,020
to modify multiple float attributes of any
type and class.


00:00:47,540 --> 00:00:50,560
Let's explain all these concepts one by one.


00:00:51,920 --> 00:00:57,220
First off we have to decide whether to use
dynamic code vs static code.


00:00:57,580 --> 00:01:01,380
Dynamic code means the VEX code
is not known ahead of the time.


00:01:01,900 --> 00:01:08,360
In Houdini, ideally you want to avoid having
dynamic code for performance reasons, especially


00:01:08,360 --> 00:01:11,100
if the code is changing inside a for loop
network.


00:01:12,080 --> 00:01:18,060
For example, if you create an HDA that allows
artists to write custom VEX code that runs


00:01:18,060 --> 00:01:24,229
inside another VEX code, then you have to
inject this code into your code using the


00:01:24,229 --> 00:01:26,040
chs expression and backticks.


00:01:26,880 --> 00:01:30,860
So anytime the user code is changed, the VEX
code will be recompiled.


00:01:31,780 --> 00:01:35,660
Another example of dynamic code is via dynamic
code generation.


00:01:36,860 --> 00:01:38,560
Consider a simple code like this.


00:01:39,500 --> 00:01:45,600
As you can see, VEX is statically-typed language
so the type of a variable has to be known


00:01:45,600 --> 00:01:46,600
at compile-time.


00:01:48,460 --> 00:01:52,440
We can infer the type and type prefix from
the attribute name.


00:01:53,520 --> 00:01:58,820
When you want to modify a number of attributes
that are unknown ahead of time, you would


00:01:58,820 --> 00:02:02,820
have to generate the proper types in VEX so
that the code compiles.


00:02:03,600 --> 00:02:07,360
That means writing Python code to generate
VEX code.


00:02:08,480 --> 00:02:12,720
A perfect example is demonstrated in 
Attribute Expression SOP.


00:02:13,820 --> 00:02:18,740
As you can see, each type and type prefix
is meticulously handled.


00:02:19,880 --> 00:02:23,820
So be prepared to write this kind of code,
if you want to go this route.


00:02:25,400 --> 00:02:31,000
The 3rd method, which is the method we will
use, is a static code approach where we write


00:02:31,000 --> 00:02:35,320
VEX code in a way that will work the same
regardless of the type of the attributes.


00:02:36,400 --> 00:02:42,580
The key functions that make this approach
possible is getattrib and setattrib functions.


00:02:44,620 --> 00:02:48,640
The second point is, we will only focus on
float attribute types.


00:02:49,260 --> 00:02:56,200
When you want to modify multiple attributes,
almost always you only want to modify float


00:02:56,200 --> 00:03:01,760
attributes, rather than have the exact same
code to also work on integers and strings,


00:03:01,760 --> 00:03:08,240
as these would require special handling, i.e.
rounding, casting, in the case of integers.


00:03:09,820 --> 00:03:12,280
We can think of attribute type qualifier,


00:03:13,900 --> 00:03:18,260
attribute class and attribute type as orthogonal


00:03:18,260 --> 00:03:20,580
axes, independent from one another.


00:03:23,900 --> 00:03:30,500
Attribute type qualifier can specify whether
a vector is a color or a position or a normal,


00:03:30,500 --> 00:03:33,760
etc, so that you may want to handle them differently.


00:03:34,300 --> 00:03:39,080
In the case of blending attributes for example,
if the attribute is a color, then you might


00:03:39,080 --> 00:03:45,880
want to blend them using HSV, HSL or LCh,
which produces the best natural results that


00:03:45,880 --> 00:03:50,930
humans expect as LCh color space is based
on human perception.


00:03:50,930 --> 00:03:54,580
It's my favourite color space for blending
between colors.


00:03:55,140 --> 00:03:59,580
It does not exist in Houdini, but can be implemented
relatively easy.


00:04:00,180 --> 00:04:04,280
On the other hand, if it's a position,
you would use linear interpolation


00:04:04,280 --> 00:04:05,380
to blend between them.


00:04:06,060 --> 00:04:10,520
If it's a normal, then you would use spherical
linear interpolation.


00:04:12,280 --> 00:04:17,620
In the case of transformations using matrices,
for the colors and positions, you can just


00:04:17,620 --> 00:04:24,100
multiply them by a matrix, but for normals,
you need to multiply the normals by the inverse


00:04:24,100 --> 00:04:26,030
transpose of the same matrix.


00:04:26,030 --> 00:04:28,440
That's how normals are handled properly.


00:04:29,960 --> 00:04:37,300
Attribute classes specify whether the attribute
is on points, primitives, vertices or on detail,


00:04:37,500 --> 00:04:39,540
i.e. the entire geometry.


00:04:40,520 --> 00:04:43,920
In this video we will only focus on point
attribute types.


00:04:44,360 --> 00:04:50,320
Because we are dealing with point neighbours,
if we include primitives and vertices, we


00:04:50,320 --> 00:04:55,940
would have to write the same k-depth neighbour
functions using primitives and vertices as well.


00:04:56,460 --> 00:04:59,280
Feel free to experiment with these if you are willing.


00:05:00,120 --> 00:05:05,380
Finally you want to structure your network
where you run everything inside a for loop network.


00:05:05,840 --> 00:05:09,360
Changing the Attribute Wrangle Run Over Mode 
at each iteration.


00:05:09,960 --> 00:05:14,880
In our example because we will only deal with
point neighbours, we won't have this for loop.


00:05:15,760 --> 00:05:21,120
Inside this for loop, you will have another
for loop that runs for X number of iterations.


00:05:22,000 --> 00:05:26,900
Inside this second for loop, you will have
the attribute wrangle that has the VEX code


00:05:26,900 --> 00:05:28,580
to modify multiple attributes.


00:05:30,320 --> 00:05:35,580
We will be able to modify multiple attributes
of the same class, but of different types,


00:05:35,580 --> 00:05:36,920
inside the same Attribute Wrangle node.


00:05:39,640 --> 00:05:42,000
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

37 - Convolution Kernels - Generalizing the Kernel Code to Handle All Attribute Types - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,880 --> 00:00:07,560
I am going to continue from the convolution scene.


00:00:08,480 --> 00:00:10,620
I will create a few extra parameters.


00:00:11,700 --> 00:00:15,280
One of them will be the hue shifted version
of the color attribute.


00:00:38,320 --> 00:00:42,140
I will also create a float attribute to
store the luminous values.


00:00:52,560 --> 00:00:55,420
as you can see, we have all three attributes.


00:00:56,400 --> 00:01:00,559
A naive approach will be to just
duplicate the same convolution node


00:01:00,559 --> 00:01:03,840
for each attribute we want to modify, and then


00:01:03,840 --> 00:01:07,560
alter the code inside for each attribute specifically.


00:01:48,400 --> 00:01:52,660
As  you can see, all three attributes have
been blurred successfully.


00:01:53,360 --> 00:01:58,620
Now we will write the generic VEX code to
modify all 3 attributes in the same node,


00:01:58,620 --> 00:02:00,420
using the exact same code


00:02:01,000 --> 00:02:02,840
So duplicate the for loop network.


00:02:21,040 --> 00:02:24,100
First modify the spare parameter name for clarity.


00:02:24,640 --> 00:02:27,920
We will list all the attributes we want
to modify here


00:02:27,920 --> 00:02:29,820
separated using spaces.


00:02:33,040 --> 00:02:38,440
So first split this string parameter using
space to turn each item into a string.


00:02:40,640 --> 00:02:42,840
We will loop over each attribute name.


00:02:46,480 --> 00:02:51,320
getattrib and setattrib requires a variable
to store the success state.


00:02:51,920 --> 00:02:56,400
The first piece of code we have to write
is to properly initialize the sum array


00:02:56,560 --> 00:02:58,980
so that all the elements are set to 0.


00:02:59,620 --> 00:03:02,240
If you just start accumulating values in


00:03:02,240 --> 00:03:05,440
a new array by incrementing existing element values,


00:03:05,440 --> 00:03:08,960
they are not guaranteed to be
initialized to 0 at start.


00:03:13,920 --> 00:03:17,280
So it's best to get the attribute values
of the first point


00:03:17,280 --> 00:03:20,879
and then using the length of this array,
set the element values


00:03:20,880 --> 00:03:22,480
of the sum array to 0.


00:03:25,360 --> 00:03:29,519
Now inside the for loop get the
attribute values for each point


00:03:29,520 --> 00:03:31,440
and loop over the attribute values.


00:03:42,800 --> 00:03:46,720
Each element of the attribute values
array have  to be multiplied


00:03:46,720 --> 00:03:48,800
by the weight value of the current point.


00:03:49,440 --> 00:03:53,740
That is why, we have to use the 
index variable i, instead of f,


00:03:54,340 --> 00:03:56,560
because the weights belong to the points,


00:03:56,560 --> 00:04:01,060
and as such they are aligned to the pts
array, rather than the values array.


00:04:01,840 --> 00:04:05,320
Finally we write the results back to the
current attribute.


00:04:05,620 --> 00:04:11,820
Using the setattrib function allows us to
write to attributes generically using the array form,


00:04:12,080 --> 00:04:15,120
rather than having to specify the actual
attribute type name.


00:04:15,680 --> 00:04:18,600
Of course this comes at a cost of performance,


00:04:18,880 --> 00:04:22,240
particularly the set attribute functions allow


00:04:22,240 --> 00:04:25,280
setting attribute values even for elements


00:04:25,280 --> 00:04:26,860
other than the current element,


00:04:27,380 --> 00:04:32,860
i.e. setting attribute values of other points
that is not the current point,


00:04:33,360 --> 00:04:37,820
and as such it is not as fast as 
direct binding of attributes.


00:04:38,620 --> 00:04:42,860
As you can see, we got the same result 
as the non-generic version.


00:04:43,480 --> 00:04:46,800
The same is true for the other attributes
we have modified.


00:04:48,320 --> 00:04:51,440
Even though VEX doesn't have
programming language concepts


00:04:51,440 --> 00:04:55,040
like generics, we still have some ways
around handling


00:04:55,040 --> 00:04:58,320
multiple attributes of any type and class,


00:04:58,320 --> 00:05:00,960
even if it forces us to write more code.


00:05:01,840 --> 00:05:04,600
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

38 - Attribute Gradient - Introduction

----------------------------------------------------------------------------------------------------



00:00:10,400 --> 00:00:13,680
Gradient of a scalar value is an extremely
useful concept,


00:00:14,540 --> 00:00:15,580
and even though we have this


00:00:15,580 --> 00:00:19,180
as volumegradient in Houdini in the form of
a volume operation,


00:00:19,340 --> 00:00:20,460
it's still very beneficial


00:00:20,460 --> 00:00:24,920
to know how to compute this manually,
particularly without using volumes.


00:00:27,940 --> 00:00:32,540
The gradient is a vector that points in the
direction of greatest rate of increase.


00:00:33,880 --> 00:00:38,420
Your first intuition might be to convert a
polygonal mesh or a point cloud to a volume


00:00:38,420 --> 00:00:39,800
to compute this vector


00:00:40,700 --> 00:00:44,500
but if you think of
in terms of discretization, it's quite difficult


00:00:44,500 --> 00:00:48,489
to capture all the details of a mesh in a
volume with the same precision a mesh has


00:00:48,489 --> 00:00:50,160
with infinitely sharp edges,


00:00:50,500 --> 00:00:54,399
unless you go very high res in your volume,
but even then


00:00:54,400 --> 00:00:56,920
there will be some blurring, averaging out
of values.


00:00:57,900 --> 00:01:02,480
So my rule of thumb is, if you can have precision,
don't rely on approximation.


00:01:03,800 --> 00:01:08,420
When you use the connectivity information
you can compute very precise gradient vectors


00:01:08,420 --> 00:01:09,600
for each of your points,


00:01:10,220 --> 00:01:12,760
and you don't have
to worry about what resolution to use for


00:01:12,760 --> 00:01:14,000
your volumes in your setup.


00:01:17,400 --> 00:01:21,980
I used this exact technique extensively in
the cracks setup I created for the movie


00:01:21,980 --> 00:01:26,560
X-Men: Dark Phoenix, where this technique formed
one of the key parts of the system.


00:01:27,500 --> 00:01:29,820
You can read more in depth details about it here.


00:01:30,460 --> 00:01:33,800
They explain a lot about the prep and the
animation of the cracks.


00:01:34,720 --> 00:01:36,660
Just to mention for completeness,


00:01:36,940 --> 00:01:39,680
the new measure SOP can also 
compute the gradient now.


00:01:40,460 --> 00:01:42,800
PolyFrame also has a method to do this


00:01:43,680 --> 00:01:46,160
but ironically it expects a scalar attribute to


00:01:46,160 --> 00:01:48,540
be provided in the form of a vector attribute,


00:01:49,020 --> 00:01:51,620
and on vertices, not on points, which was


00:01:51,620 --> 00:01:53,480
the reason I made my own in the first place.


00:01:54,720 --> 00:01:57,040
Let's see the concept of the algorithm first.






----------------------------------------------------------------------------------------------------

39 - Attribute Gradient - Concept

----------------------------------------------------------------------------------------------------



00:00:03,400 --> 00:00:06,040
Imagine a point surrounded by other points.


00:00:06,720 --> 00:00:15,160
Let's call the current point P, and the surrounding
points, Q0, Q1, Q2, and Qn.


00:00:16,200 --> 00:00:21,280
We compute direction vectors that go from
the current point to each of the nearby points.


00:00:27,680 --> 00:00:30,000
We declare gradient as a vector.


00:00:32,660 --> 00:00:34,420
And for each nearby point,


00:00:35,400 --> 00:00:39,320
we compute the normalized vector that goes from P to Q,


00:00:42,440 --> 00:00:44,680
and that means Q minus P.


00:00:45,780 --> 00:00:49,760
Then we declare a scalar value, that's the
difference between the attribute value of


00:00:49,760 --> 00:00:53,880
the nearby point and the current point, for
the attribute we are interested in.


00:00:54,540 --> 00:00:57,860
In the same direction as the N vector we just computed.


00:00:58,940 --> 00:01:04,120
You can think of it as attribute value of
Q, minus attribute value of P.


00:01:04,400 --> 00:01:08,260
This gives us the scalar difference in the
same direction as N.


00:01:13,780 --> 00:01:18,120
We multiply N by s and add this on the gradient vector.


00:01:20,020 --> 00:01:24,240
After the loop is finished, we normalize the
summed gradient vector.


00:01:25,760 --> 00:01:28,800
This gives us the gradient vector of the attribute
we are interested in.


00:01:30,580 --> 00:01:36,160
You may choose to not normalize the gradient
vector as the length of this vector gives


00:01:36,160 --> 00:01:37,540
us the rate of change.


00:01:38,320 --> 00:01:41,480
So it is up to you if you want to preserve it's value or not.


00:01:42,840 --> 00:01:45,320
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

40 - Attribute Gradient - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,240 --> 00:00:05,400
Bring a model in.


00:00:06,120 --> 00:00:08,640
This time I am gonna use the famous Stanford bunny.


00:00:09,420 --> 00:00:10,640
Create an attribute VOP.


00:00:11,440 --> 00:00:14,920
We are gonna create a scalar attribute using
voronoi noise.


00:00:17,480 --> 00:00:19,820
Subtract distance 2 from distance 1.


00:00:25,260 --> 00:00:27,100
I am gonna call this attribute dist.


00:00:29,680 --> 00:00:31,360
Assign the result as color.


00:00:32,780 --> 00:00:36,740
Increase the frequency of the noise so we
can see the voronoi pattern better.


00:00:39,760 --> 00:00:41,420
Find a pattern that you like.


00:00:46,280 --> 00:00:49,300
We should also subdivide the model as there
is not enough detail.


00:00:50,700 --> 00:00:54,100
I am gonna use the OpenSubdiv Loop 
to preserve the triangles.


00:00:58,420 --> 00:01:01,160
As you can see the triangles are preserved.


00:01:03,560 --> 00:01:05,300
Now let's create a wrangle node.


00:01:06,320 --> 00:01:09,820
For this example, I am gonna use the standard
edge neighbours function.


00:01:10,300 --> 00:01:13,340
But feel free to use any of the neighbouring
functions that we implemented


00:01:13,340 --> 00:01:14,800
or even a point cloud look up.


00:01:15,880 --> 00:01:17,700
The algorithm remains the same.


00:01:18,860 --> 00:01:21,120
Declare the gradient vector as 0.


00:01:22,420 --> 00:01:24,180
Loop over all neighbour points.


00:01:28,500 --> 00:01:31,100
Get the point position of the current neighbour
point.


00:01:35,220 --> 00:01:38,700
Now create a normalized vector that points
from the current point


00:01:38,980 --> 00:01:40,600
towards the current neighbour point.


00:01:46,620 --> 00:01:49,360
Store the dist attribute for the current neighbour point.


00:01:53,180 --> 00:01:56,980
And now we will take the difference between
them, in the same direction as the normal


00:01:56,980 --> 00:01:57,960
we just computed.


00:01:57,960 --> 00:02:01,460
Attribute value minus current point's attribute value.


00:02:03,280 --> 00:02:08,000
So we multiply this by N and add this onto
the gradient vector.


00:02:10,540 --> 00:02:14,740
Now the gradient vector has the sum of all
these vectors, we just have to normalize it


00:02:14,740 --> 00:02:16,100
to bring it back to unity.


00:02:18,200 --> 00:02:21,460
You can bring up the node information dialog
and just click on grad.


00:02:22,900 --> 00:02:26,620
Houdini visualizes non-default vector attributes
as colors by default.


00:02:27,040 --> 00:02:31,760
But if you use setattribtypeinfo, you can
hint Houdini that this is a direction vector


00:02:31,760 --> 00:02:32,900
rather than a color.


00:02:33,900 --> 00:02:36,240
For now I am just gonna change the visualization.


00:02:36,660 --> 00:02:39,900
Let's set it to vector and reduce the length
to a smaller value.


00:02:43,180 --> 00:02:44,760
I am also gonna change the color.


00:02:50,840 --> 00:02:52,200
Maybe reduce it even more.


00:02:56,100 --> 00:03:01,540
Now if you look at the pattern closely, all
the gradient vectors are pointing at the center


00:03:01,540 --> 00:03:04,600
of each cell, where the dist attribute values
are increasing.


00:03:06,160 --> 00:03:09,240
And at the borders, the dist attribute values are 0.


00:03:09,660 --> 00:03:12,820
So the gradient vectors are pointing away
from these locations.


00:03:14,620 --> 00:03:17,740
We can also toggle the arrow tips to see the
directions a bit better.


00:03:22,340 --> 00:03:25,680
You can now see how this kind of thing could
be really useful.


00:03:28,340 --> 00:03:30,860
We uniformly defined where to point the vectors,


00:03:30,860 --> 00:03:33,480
towards wherever we want using a scalar attribute.


00:03:34,360 --> 00:03:38,440
But you can see each vector is precisely
pointing at the center of each cell,


00:03:39,080 --> 00:03:41,360
and every point has a distinct direction.


00:03:41,800 --> 00:03:43,440
This is the tip of the iceberg.


00:03:44,240 --> 00:03:48,080
For example you can do post operations to
make sure the corners are smoother.


00:03:49,020 --> 00:03:54,140
With these gradient vectors there are no overlaps
which is what really helped in the cracks


00:03:54,140 --> 00:03:56,260
setup I created for X-Men: Dark Phoenix.


00:03:57,960 --> 00:04:02,600
Just for completeness, I also want to show
you how to set attribute type info in VEX.


00:04:03,980 --> 00:04:07,980
As you can see the gradient vector shows up
as 3 floats.


00:04:10,680 --> 00:04:12,880
You can call this function anywhere in the
code,


00:04:16,860 --> 00:04:20,000
provide the attribute name and the type of the attribute.


00:04:23,480 --> 00:04:26,300
Now you can see it's actually marked as vector.


00:04:27,600 --> 00:04:31,560
If you actually did this and clicked the attribute
name in the node information dialog,


00:04:31,760 --> 00:04:34,320
they would have shown up as vectors instead of colors.


00:04:35,560 --> 00:04:38,260
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

41 - Gradient Ascent & Descent - Planar Geometry - Introduction

----------------------------------------------------------------------------------------------------



00:00:04,540 --> 00:00:10,410
Gradient descent is an optimization algorithm
used to minimize some function by iteratively


00:00:10,410 --> 00:00:16,240
moving in the direction of steepest descent
as defined by the negative of the gradient.


00:00:17,800 --> 00:00:20,560
Imagine youre blindfolded in a rough terrain,


00:00:21,120 --> 00:00:23,740
and your goal is to reach the lowest altitude.


00:00:24,460 --> 00:00:29,740
One of the simplest strategies you can use,
is to feel the ground in every direction,


00:00:30,220 --> 00:00:34,060
and take a step in the direction where the
ground is descending the fastest.


00:00:34,980 --> 00:00:37,980
The rough terrain is analogous to the cost
function,


00:00:38,400 --> 00:00:40,140
and minimizing the cost function


00:00:40,140 --> 00:00:43,900
is analogous to trying to reach lower altitudes.


00:00:44,540 --> 00:00:50,000
Feeling the slope of the terrain around you
is analogous to calculating the gradient.


00:00:51,400 --> 00:00:55,420
The gradient tells us the incline or slope
of the cost function.


00:00:55,420 --> 00:01:00,780
Hence, to minimize the cost function, we move
in the direction opposite to the gradient.


00:01:01,980 --> 00:01:07,619
The learning rate is a positive scalar value
that determines the size of each step in the


00:01:07,620 --> 00:01:09,100
gradient descent process.


00:01:10,140 --> 00:01:14,810
If the step size is too small, the gradient
descent process can be slow.


00:01:14,810 --> 00:01:20,440
Whereas if it's too large, gradient descent
can overshoot the minimum and may fail to


00:01:20,440 --> 00:01:22,380
converge, or even diverge.


00:01:24,600 --> 00:01:26,580
Let's look at the concept of the algorithm.






----------------------------------------------------------------------------------------------------

42 - Gradient Ascent & Descent - Planar Geometry - Concept

----------------------------------------------------------------------------------------------------



00:00:04,640 --> 00:00:07,040
Imagine a heightfield from the side view.


00:00:08,820 --> 00:00:12,820
We will use P.y to represent relative height.


00:00:15,800 --> 00:00:18,220
We then compute the gradient of height.


00:00:23,680 --> 00:00:28,160
Multiply this with a step size, and add this
onto P.


00:00:39,300 --> 00:00:42,280
This will move points along the gradient of the height.


00:00:49,860 --> 00:00:52,380
This is known as gradient ascent.


00:00:57,340 --> 00:01:03,960
If you negate the gradient or the cost attribute,
i.e. the height attribute, then the points


00:01:03,960 --> 00:01:07,260
would be moving along the negative gradient
of the height.


00:01:09,040 --> 00:01:11,720
This is known as gradient descent.


00:01:17,140 --> 00:01:22,540
Gradient descent will converge to a local
minima, rather than the global minima.


00:01:31,600 --> 00:01:37,580
Where as gradient ascent will converge to a local maxima rather than global maxima.


00:01:39,400 --> 00:01:46,900
In mathematical analysis, the maxima and minima
of a function, known collectively as extrema,


00:01:46,900 --> 00:01:51,800
are the largest and smallest value of the
function, either within a given range


00:01:51,800 --> 00:01:56,680
(the local or relative extrema) 
or on the entire domain of a function


00:01:56,680 --> 00:01:59,300
(the global or absolute extrema).


00:02:00,560 --> 00:02:04,420
It's pretty easy to understand them intuitively
from this drawing.


00:02:05,920 --> 00:02:09,220
Lastly we need a way to determine when to stop.


00:02:10,340 --> 00:02:16,220
A popular choice for the termination criteria
is when the cost stops reducing.


00:02:18,160 --> 00:02:21,060
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

43 - Gradient Ascent & Descent - Planar Geometry - Implementation

----------------------------------------------------------------------------------------------------



00:00:09,420 --> 00:00:12,700
Create a grid with 200 by 200 divisions.


00:00:18,720 --> 00:00:20,940
Deform it using some noise.


00:00:21,600 --> 00:00:25,080
I found this particular shape before, which
is what I will use.


00:00:25,360 --> 00:00:28,980
But feel free to use any noise parameters
that you find appealing.


00:00:45,200 --> 00:00:48,700
Now we create the cost attribute using the
height of each point.


00:00:50,160 --> 00:00:53,800
We already know how to compute 
gradient from a scalar attribute.


00:00:54,240 --> 00:00:58,800
But this time I will show how to do it using
the Measure SOP, for variety.


00:01:06,720 --> 00:01:11,420
As you can see the gradients look a bit noisy
due to the surface detail.


00:01:11,820 --> 00:01:13,740
We will deal with this later.


00:01:19,200 --> 00:01:22,200
Also make sure to change the Element Type to Points,


00:01:22,200 --> 00:01:24,880
as we want the gradient to be a point attribute.


00:01:28,080 --> 00:01:32,640
Because we are doing a gradient descent first,
we need to negate the gradient,


00:01:32,640 --> 00:01:34,160
or the cost attribute.


00:01:35,200 --> 00:01:37,420
Scatter some points on the grid.


00:01:38,000 --> 00:01:40,480
Now we will write the gradient descent code.


00:01:48,560 --> 00:01:50,240
First define some parameters.


00:01:56,240 --> 00:01:59,280
Min dist is the minimum distance that must be traversed


00:01:59,280 --> 00:02:01,880
before the path is turned into a polygonal curve.


00:02:13,520 --> 00:02:17,680
Instead of creating the points on the spot,
we want to gather point positions


00:02:17,680 --> 00:02:21,280
in an array first and only create a line using


00:02:21,280 --> 00:02:25,200
them, if they meet the min dist requirement.


00:02:25,200 --> 00:02:28,800
We also want to store the distance
traverse at each point in an array


00:02:28,800 --> 00:02:31,060
to normalize them into 0-1 range.


00:02:31,620 --> 00:02:34,960
We first set P to the current point position.


00:02:35,420 --> 00:02:40,740
Then we have to project this point position
back onto the surface in case the point is


00:02:40,740 --> 00:02:42,020
not actually on the surface,


00:02:42,400 --> 00:02:44,300
using the xyzdist function.


00:02:45,240 --> 00:02:50,300
Now we look up the new P using the primuv
function based on the hit primitive


00:02:50,480 --> 00:02:52,340
and the primitive UV values.


00:02:53,600 --> 00:02:55,900
We do the same for the gradient so we


00:02:55,900 --> 00:02:59,580
know what the gradient is
at this exact position on the surface.


00:03:06,160 --> 00:03:09,700
Append both the position and the
distance which is zero so far.


00:03:16,560 --> 00:03:20,640
Now we want to keep track of the total
distance traveled and use this to check


00:03:20,640 --> 00:03:23,000
against the max dist using a while loop.


00:03:26,640 --> 00:03:31,320
First move P using the normalized
gradient multiply by the step size.


00:03:32,720 --> 00:03:36,480
We have to normalize the gradient in
this case so that we can make sure it is


00:03:36,480 --> 00:03:38,420
exactly the size of the step size.


00:03:39,980 --> 00:03:41,760
You can copy paste the nearest surface


00:03:41,760 --> 00:03:46,160
projection, new P and gradient lookup
using primuv code here.


00:03:49,680 --> 00:03:53,720
Now compute the distance between the
last point and the current point position.


00:03:54,420 --> 00:03:56,640
Here we need the actual distance so we


00:03:56,640 --> 00:03:58,180
can't use squared distance.


00:04:00,320 --> 00:04:02,760
We also need to define some tolerance value.


00:04:07,840 --> 00:04:11,200
if the distance between the last point
and the current point is less than the


00:04:11,200 --> 00:04:13,100
threshold, we break the loop.


00:04:14,700 --> 00:04:17,220
We add the distance to the total distance.


00:04:18,340 --> 00:04:22,560
Append P to pos and sumdist to dists array.


00:04:23,920 --> 00:04:27,420
We want to know how much distance we
traveled for each point.


00:04:28,400 --> 00:04:33,199
In the last step, we check if we travel
more or equal to minimum distance.


00:04:33,200 --> 00:04:36,160
If so, we proceed to create the polygon line.


00:04:37,520 --> 00:04:41,700
We will just loop over the pause array
and create points using these.


00:04:42,400 --> 00:04:46,639
We have to gather point indices in an array
as the addprim function


00:04:46,640 --> 00:04:49,860
requires an index array, rather than a
position array.


00:05:11,120 --> 00:05:15,440
We need to remove the point itself, as we
are recreating the nearest projection of


00:05:15,440 --> 00:05:16,680
it on the surface.


00:05:17,860 --> 00:05:20,660
Just create the parameters and set some values.


00:05:28,220 --> 00:05:31,980
As you can see, we have successfully
implemented gradient descent


00:05:38,580 --> 00:05:42,360
I'm just gonna color the points using
ramp by the normalized distance.


00:06:14,160 --> 00:06:18,060
The infra-red colors Houdini uses are not
the same as the ones I use.


00:06:22,960 --> 00:06:26,080
As you can see the lines are colored
using the ramp


00:06:26,320 --> 00:06:29,320
but it doesn't look like they use the
full range of the color ramp.


00:06:29,920 --> 00:06:35,420
If you set Max dist to 1, you see that the
result looks the same but the colors change.


00:06:35,920 --> 00:06:37,600
The reason for this is simple.


00:06:38,000 --> 00:06:42,560
We are not breaking the loop as soon as
the cost attribute doesn't get any lower,


00:06:42,560 --> 00:06:45,919
but rather break the loop when we reach
max distance,


00:06:45,920 --> 00:06:48,660
and as such the points punch up at the
valleys.


00:06:49,300 --> 00:06:53,360
Before we do this, we will also refine
the result by blurring the grid


00:06:53,360 --> 00:06:55,180
before creating the cost attribute.


00:07:04,320 --> 00:07:07,440
As you can see the lines are much
smoother now.


00:07:11,840 --> 00:07:14,220
We can do the same for the gradient vectors.


00:07:34,960 --> 00:07:38,000
Smoothing the gradient vectors gets rid
of the noise


00:07:38,000 --> 00:07:40,080
and the rapid swirling of the vectors.


00:07:52,000 --> 00:07:53,760
As you increase max dist,


00:07:53,760 --> 00:07:57,440
you can see the lines are getting more and more bunched up in the valleys.


00:08:00,960 --> 00:08:04,160
We can now implement a new logic to
break out of the loop


00:08:04,160 --> 00:08:07,380
if the cost attribute value is not
getting any smaller.


00:08:08,460 --> 00:08:12,700
First create a step size multiplier and
the cost attribute.


00:08:26,800 --> 00:08:30,980
Where we sample gradient from the
surface, we also need to sample the cost,


00:08:39,280 --> 00:08:41,260
but also keep track of the last cost.


00:09:26,800 --> 00:09:30,000
If the cost is greater than the last
cost within tolerance,


00:09:30,000 --> 00:09:33,460
we will reduce the step size using the
step size multiplier.


00:09:33,760 --> 00:09:36,360
Otherwise do what we were doing before.


00:09:48,960 --> 00:09:53,280
If the step size gets smaller than the
tolerance, we need to break out of the loop.


00:10:01,440 --> 00:10:03,580
Also make sure to update the loss cost.


00:10:08,900 --> 00:10:12,160
Now you can see,
we are actually stopping in the valleys


00:10:20,320 --> 00:10:24,760
The closer the step size to one, the
higher the chance that the points will


00:10:24,760 --> 00:10:29,440
find local minima, 
i.e. the nearest valley, at the cost of performance.


00:10:39,440 --> 00:10:44,800
As you can see it took 0.3 seconds when
the step size multiplier was 0.9


00:10:45,200 --> 00:10:50,640
and 1.5 seconds when it was 0.99, 
5 times the difference.


00:10:50,760 --> 00:10:52,440
which is quite significant.


00:11:13,040 --> 00:11:16,720
The effect of the step size multiplier
is quite apparent.


00:11:27,120 --> 00:11:31,040
As you can see, the prematurely
terminated lines find their way to the


00:11:31,040 --> 00:11:34,440
local minima, as we increase the step size multiplier.


00:11:37,680 --> 00:11:41,440
You can also bias the distribution of
the points so that more points are


00:11:41,440 --> 00:11:44,959
spread on the peaks
rather than valleys as we are going from


00:11:44,960 --> 00:11:46,800
the peaks into the valleys.


00:13:03,440 --> 00:13:06,959
In effect this makes the lines a lot
more dense where it matters


00:13:06,960 --> 00:13:08,800
given the same number of points.


00:13:24,560 --> 00:13:27,680
Now we will implement the gradient ascent.


00:13:27,680 --> 00:13:29,600
So just duplicate the entire network.


00:13:35,440 --> 00:13:36,980
Invert the cost attribute.


00:13:39,300 --> 00:13:43,100
And that is all you need to do
everything else will work just the same.


00:14:03,780 --> 00:14:07,740
As you can see, the result is just the
opposite of the gradient descent.


00:14:28,480 --> 00:14:31,940
You can also merge them both, if you want
to cover the entire geometry.


00:15:06,080 --> 00:15:09,240
We can now move on to using an arbitrary geometry.


00:15:11,360 --> 00:15:14,140
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

44 - Gradient Ascent & Descent - 3D Geometry

----------------------------------------------------------------------------------------------------



00:00:04,640 --> 00:00:10,719
We have implemented gradient ascent and descent on planar geometry much like a terrain


00:00:11,040 --> 00:00:13,040
where we use the gradient of the height.


00:00:14,240 --> 00:00:18,020
For an arbitrary geometry, 
what we can use as the cost attribute?


00:00:19,279 --> 00:00:21,279
We cannot use the height.


00:00:21,279 --> 00:00:24,159
Because we don't want a top-down
gradient descent in this case.


00:00:25,120 --> 00:00:28,120
We want to have the lines come from 
the localized peaks


00:00:28,400 --> 00:00:32,320
and find their way into the localized valleys,
same as before.


00:00:33,600 --> 00:00:36,000
One thing we can use is depth.


00:00:38,000 --> 00:00:41,540
There are many ways to compute depth using an arbitrary geometry.


00:00:42,320 --> 00:00:44,879
One of the easiest ways is to use volumes.


00:00:45,520 --> 00:00:47,520
So that's what we are gonna do.


00:01:04,320 --> 00:01:08,339
Set the cost attribute to the result of the volume sample of the SDF volume.


00:01:23,660 --> 00:01:25,980
Disable attribute blu r notes for now.


00:01:30,720 --> 00:01:33,300
As you can see, we have the gradient vectors.


00:01:47,120 --> 00:01:52,100
But the result looks a bit coarse.
Reducing the step size will improve the result,


00:01:54,020 --> 00:01:57,700
but you also have to reduce the 
mean list to keep more of the shorter lines.


00:02:04,320 --> 00:02:08,660
We can also amplify the cost attribute as the values are too close to each other,


00:02:08,660 --> 00:02:11,380
which causes a lot of them to go below the threshold.


00:02:32,239 --> 00:02:36,360
You can see how blurring the cost and the gradient drastically affects the result,


00:02:36,360 --> 00:02:38,480
just like in the height field example


00:02:42,000 --> 00:02:45,440
I'm just going to merge the original 
geometry to see the lines more clearly.


00:02:46,960 --> 00:02:51,040
Gradient descent and ascent is 
very sensitive to initial conditions.


00:04:02,560 --> 00:04:06,920
You can see how increasing the step size
multiplier helps us here, too.


00:04:09,280 --> 00:04:12,560
It helps fix these kinds of junctions quite well.


00:04:16,880 --> 00:04:19,800
I'm gonna convert to polygons into nurbs curves.


00:04:28,960 --> 00:04:34,880
Now take a look at the curvature of the model, it's quite non-uniform due to the topology of the model.


00:04:38,080 --> 00:04:40,440
Let's try the uniform version of this model.


00:04:51,600 --> 00:04:55,860
The result is wildly different, more uniform, and well behaved,


00:04:55,860 --> 00:04:58,980
but not as interesting as the previous result in my opinion.


00:05:12,800 --> 00:05:15,360
We can also scatter on the peaks like before.


00:05:40,480 --> 00:05:43,780
I think this is not as interesting as 
uniform scatter in this case.


00:06:09,440 --> 00:06:11,880
We will now implement the gradient ascend.


00:06:16,080 --> 00:06:19,460
Just duplicate the network and negate the cost like before.


00:06:38,800 --> 00:06:41,480
They are complementing each other quite nicely.


00:06:45,840 --> 00:06:47,840
You can merge them both also.


00:07:26,320 --> 00:07:29,120
Now duplicate the entire network once again.


00:07:29,520 --> 00:07:33,000
This time we will not use depth but curvature,


00:07:33,000 --> 00:07:35,540
which can also give us quite interesting results.


00:07:48,400 --> 00:07:50,540
Make sure the curvature is on points.


00:08:20,640 --> 00:08:23,380
We will experiment with different types of curvatures.


00:08:24,320 --> 00:08:28,100
We can only use the types of curvatures 
that gives us a scalar attribute.


00:08:28,400 --> 00:08:34,280
So that means, gaussian, mean, curvedness, smaller and larger principal curvatures.


00:08:58,000 --> 00:09:04,200
I find that mean curvature to be the closest to depth that approximates peaks and valleys quite nicely.


00:09:30,320 --> 00:09:33,380
Now, you know how to convert this into gradient ascent.


00:10:35,600 --> 00:10:40,180
You can actually see depth and mean curvature are so similar to each other in this case.


00:10:41,760 --> 00:10:45,000
Feel free to play around to come up 
with interesting variations.


00:11:26,640 --> 00:11:29,700
I will try a few other models like this angel model.


00:11:55,520 --> 00:11:57,880
You can see it works quite well.


00:12:38,240 --> 00:12:41,260
This girl model also yields some very interesting patterns,


00:12:41,260 --> 00:12:46,500
almost like x-ray and muscle tissues, 
like you could see her internal organs.


00:13:12,320 --> 00:13:15,420
And with that said, 
see you guys in the next ascent.






----------------------------------------------------------------------------------------------------

45 - Gradient Ascent & Descent - Contour Lines - Introduction

----------------------------------------------------------------------------------------------------



00:00:04,540 --> 00:00:09,299
Now that we know about gradients, there is
another concept that's related to them that


00:00:09,300 --> 00:00:14,440
can be acquired quite easily, 
and that concept is contour lines.


00:00:15,460 --> 00:00:22,200
A contour line (also isoline) of a function
of two variables is a curve along which the


00:00:22,200 --> 00:00:28,140
function has a constant value, so that the
curve joins  points of equal value.


00:00:28,880 --> 00:00:33,920
There are very interesting relationships between
the gradients and the contour lines.


00:00:36,560 --> 00:00:41,700
When you go hiking, it's often useful to look
at a contour map to give you an idea of the


00:00:41,700 --> 00:00:42,720
level of the land.


00:00:43,730 --> 00:00:49,660
These maps display lines of constant elevation;
and the spacing between the lines gives you


00:00:49,660 --> 00:00:52,060
an idea of how steep the landscape is.


00:00:52,820 --> 00:00:57,360
The more contour lines over the same distance,
indicates a steeper slope.


00:00:57,860 --> 00:01:01,940
Areas with widely spaced contour 
lines are gentle slopes.


00:01:05,140 --> 00:01:10,620
Picture yourself on a terraced farm, facing
the peak, to your left and right, will be


00:01:10,630 --> 00:01:15,860
the directions of constant elevation, and
walking along this contour, would not get


00:01:15,860 --> 00:01:18,660
you farther up nor farther down the hill.


00:01:19,320 --> 00:01:21,560
Let's look at the concept of the algorithm.






----------------------------------------------------------------------------------------------------

46 - Gradient Ascent & Descent - Contour Lines - Concept

----------------------------------------------------------------------------------------------------



00:00:04,700 --> 00:00:09,000
We already have the gradient vectors, and
we have the surface normal.


00:00:09,160 --> 00:00:11,840
The gradient vectors are tangent to the surface,


00:00:12,840 --> 00:00:15,060
so performing a cross product between the


00:00:15,060 --> 00:00:18,800
gradient and the normal will give us the contour lines.


00:00:19,760 --> 00:00:24,400
By definition, the velocity vector must be
tangent to the contour line.


00:00:25,120 --> 00:00:27,940
When moving along a contour line of the function,


00:00:27,940 --> 00:00:32,620
the value of the function 
neither decreases nor increases,


00:00:33,600 --> 00:00:36,120
and so the dot product of the velocity vector


00:00:36,120 --> 00:00:38,620
(i.e. the vector along the contour lines)


00:00:38,620 --> 00:00:41,620
and the gradient vector must be zero.


00:00:42,740 --> 00:00:46,880
As such, the gradient vector must be 
perpendicular to the contour line


00:00:47,140 --> 00:00:50,200
in order for the dot product to equal zero.


00:00:51,520 --> 00:00:57,400
While the gradient vector field is curl free,
the velocity field computed from the contour


00:00:57,400 --> 00:00:59,420
lines is divergence free.


00:01:00,280 --> 00:01:03,640
You can easily see this when you look at
the gradient vector fields,


00:01:03,640 --> 00:01:05,500
that there is no rotational part.


00:01:06,220 --> 00:01:09,240
And we will soon see when we look at the contour lines


00:01:09,240 --> 00:01:13,220
that there are no sources or sinks in the vector field.


00:01:14,180 --> 00:01:18,220
Just knowing these simple facts will
unlock so many possibilities,


00:01:18,660 --> 00:01:22,680
because you gain an intuitive inderstanding 
of these fundamental concepts.


00:01:23,400 --> 00:01:27,360
You will be free to combine these and create
some really interesting fx.


00:01:28,440 --> 00:01:30,660
Let's implement this in Houdini now.






----------------------------------------------------------------------------------------------------

47 - Gradient Ascent & Descent - Contour Lines - Implementation

----------------------------------------------------------------------------------------------------



00:00:05,900 --> 00:00:09,400
We will use the previous scene, as we only need to change a few things.


00:00:10,675 --> 00:00:12,685
Duplicate one of the branches.


00:00:14,100 --> 00:00:19,300
Create a wrangle node to perform a cross product between the gradient vector and the normal,


00:00:19,465 --> 00:00:21,815
after the gradient vector is computed.


00:00:33,200 --> 00:00:38,000
Instead of using a hard-coded attribute for the gradient, we will create a string parameter.


00:00:47,265 --> 00:00:50,585
We will also allow turning off the cost attribute.


00:00:51,055 --> 00:00:54,025
This is especially important with contour lines,


00:00:54,025 --> 00:00:58,700
as the cost attribute along the same contour line would always be constant,


00:00:58,900 --> 00:01:02,000
and as such will lead to premature termination.


00:01:05,335 --> 00:01:07,915
Just organize the parameter interface a bit.


00:01:29,900 --> 00:01:33,500
As you can see, we have successfully
traced the contour lines.


00:01:34,080 --> 00:01:37,260
In terms of look, you have so many ways to control it.


00:01:37,840 --> 00:01:39,640
As far as point density,


00:01:39,640 --> 00:01:46,680
you can either have a lot of points with relatively shorter length or a lot less points that travel a lot longer.


00:01:46,980 --> 00:01:50,720
Or you can do both at the same time.
Both looks are very interesting.


00:02:12,860 --> 00:02:16,700
I will also merge the original grid to
occlude the back facing curves.


00:03:32,860 --> 00:03:36,060
I will also merge the gradient ascend
and descent separately,


00:03:36,160 --> 00:03:39,480
and also together, to see how 
everything looks when combined.


00:04:26,620 --> 00:04:29,860
It is a network of curves when you combine all three.


00:04:32,240 --> 00:04:36,680
I think the density of gradient ascend and descent is overpowering the contour lines.


00:05:24,620 --> 00:05:26,660
When you use very large max dist,


00:05:26,660 --> 00:05:30,680
it looks like capturing car lights 
using extended shutter time.


00:05:32,080 --> 00:05:35,660
You can also see how this field is 
actually divergence free.


00:05:36,080 --> 00:05:39,620
There will be no sources and sinks 
present in contour lines.


00:05:42,020 --> 00:05:45,380
You can also experiment with the 
uniform distribution of points


00:05:45,380 --> 00:05:48,020
versus having a non-uniform distribution.


00:05:55,880 --> 00:06:01,180
Another way to control the look of contour lines is by adjusting the attribute blur nodes.


00:06:11,760 --> 00:06:16,580
As the deformed grid becomes smoother and smoother, so does the contour lines.


00:06:38,340 --> 00:06:40,700
Blurring the gradient has the same effect,


00:06:40,720 --> 00:06:43,980
except more on the directionality of the contour lines.


00:06:52,320 --> 00:06:57,520
When viewing from the top, you see a contour map, similar to landscape contour maps.


00:06:58,680 --> 00:07:01,960
We will now apply the same thing on a 3d geometry.


00:07:05,140 --> 00:07:08,160
This time, I will use the victory angel model.


00:07:09,340 --> 00:07:13,700
So just copy paste the wrangles that has the cross product and the contour lines code.


00:07:33,380 --> 00:07:36,700
We have to adjust the parameters 
to conform to this model,


00:07:36,700 --> 00:07:39,140
and merge the peak version of the original model.


00:07:57,560 --> 00:08:00,320
You can see it works perfectly here too.


00:08:00,720 --> 00:08:03,780
It looks like a glowing hot, super reflective model.


00:08:04,340 --> 00:08:07,680
I'm just gonna play with the same 
settings to show you some variations,


00:08:08,340 --> 00:08:11,620
so you can get a better intuition 
how everything plays together.


00:09:07,440 --> 00:09:09,380
When we blur the cost attribute,


00:09:09,500 --> 00:09:12,860
we are losing more and more of the
general shape of the geometry.


00:09:18,740 --> 00:09:20,320
When the blur is so high,


00:09:20,320 --> 00:09:23,500
the entire face area looks like she is wearing a veil


00:09:23,500 --> 00:09:25,960
like Lady Justice, which is pretty cool.


00:09:27,040 --> 00:09:29,835
And of course, you can have different amounts of blur


00:09:29,860 --> 00:09:31,660
at different parts of the geometry.


00:10:08,460 --> 00:10:13,200
In the next video, I will show you how to
apply the same concepts to a heightfield.


00:10:16,140 --> 00:10:18,960
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

48 - Gradient Ascent, Descent & Contour Lines - Heightfields

----------------------------------------------------------------------------------------------------



00:00:04,319 --> 00:00:08,800
We have already seen,
how to apply gradient ascent, descent


00:00:08,800 --> 00:00:12,180
and contour lines to heightfields-like
planar geometry,


00:00:12,800 --> 00:00:16,400
and the same concept can be applied to
height field primitives.


00:00:17,280 --> 00:00:21,199
Because heightfields are volumes,
getting the gradient is very easy


00:00:21,199 --> 00:00:25,680
by using the volume gradient function.
The normal of a heightfield


00:00:25,680 --> 00:00:31,060
is 0, 1, 0 if the heightfield is an XZ volume, meaning facing up.


00:00:31,880 --> 00:00:33,360
And getting the cross product


00:00:33,360 --> 00:00:37,940
of both of these vectors will give you
the contour lines, same as before.


00:00:39,160 --> 00:00:41,940
Let's see how to implement this in Houdini now.


00:00:45,760 --> 00:00:48,100
You can continue from the previous scene.


00:00:48,940 --> 00:00:52,300
Just create a standard heightfield and add some noise.


00:01:06,080 --> 00:01:09,080
Then convert this into a polygonal geometry.


00:01:15,560 --> 00:01:19,020
We compute the gradient using 
the volumegradient function.


00:01:26,560 --> 00:01:30,460
Scatter some points and copy paste the
contour lines with angle node.


00:01:37,120 --> 00:01:40,799
Because height fields are very large, 
we have to increase step size


00:01:40,800 --> 00:01:43,240
and max dist parameters greatly.


00:01:50,480 --> 00:01:52,940
As you can see, it works perfectly.


00:02:21,340 --> 00:02:25,340
I will also reduce the max octaves to
have a smoother heightfield.


00:02:40,240 --> 00:02:44,920
Now perform a cross product between the gradient and the Y up normal.


00:03:00,800 --> 00:03:03,180
Contour lines show up just like before.


00:03:21,300 --> 00:03:25,460
I will merge everything together and
play with the usual parameters.


00:05:03,760 --> 00:05:07,280
Now you might be wondering, this is still
using a polygonal mesh


00:05:07,280 --> 00:05:11,759
under the hood, is a fully volume based
approach even possible


00:05:11,759 --> 00:05:14,800
without resorting to polygonal geometry at all?


00:05:14,800 --> 00:05:18,400
The answer will be yes. 
Just for completeness,


00:05:18,400 --> 00:05:23,520
I will also show this. Keep in mind that
if you want to support heightfields in


00:05:23,520 --> 00:05:26,560
any situation,
you have to take into account a lot more


00:05:26,560 --> 00:05:30,340
things like the orientation,
transformation, etc.


00:05:31,020 --> 00:05:36,000
In our case, we only need to change a few things to
demonstrate a full volume based approach.


00:05:40,000 --> 00:05:44,460
Okay. So first off, instead of scatter, we
will use heightfield scatter.


00:05:44,720 --> 00:05:47,360
Scatter can't scatter on heightfields.


00:05:48,880 --> 00:05:51,340
I set the number of points to 3000.


00:05:51,680 --> 00:05:54,040
Duplicate the contour lines for angle node.


00:05:54,880 --> 00:05:59,880
You can actually use xyzdist and primuv functions on heightfields.


00:06:00,320 --> 00:06:04,319
This makes the code a lot simpler. 
This is also the reason,


00:06:04,320 --> 00:06:07,500
why i wanted to show you 
the full heightfield base approach.


00:06:07,760 --> 00:06:11,520
Because most people are not aware
xyzdist and primuv functions


00:06:11,520 --> 00:06:13,780
are perfectly valid for heightfields.


00:06:14,600 --> 00:06:17,180
xyzdist will return you the volume


00:06:17,180 --> 00:06:20,420
primitive index along with the normalized UV coordinates.


00:06:20,800 --> 00:06:23,920
We take this and pass it to the primuv function to


00:06:23,920 --> 00:06:28,140
look up the world position
on the heat volume, for the heat UV coordinates.


00:06:28,960 --> 00:06:31,060
We can then take this world space


00:06:31,060 --> 00:06:33,580
coordinate and use it with volume sampling.


00:06:35,680 --> 00:06:38,880
So for gradient, instead of primuv, 
we will use


00:06:38,880 --> 00:06:40,480
volume gradient like before.


00:06:41,540 --> 00:06:46,480
In a heightfield, the height is the cost, and volume
sampling the height field


00:06:46,480 --> 00:06:47,880
gives us that value.


00:06:51,360 --> 00:06:54,180
You can negate the grad for gradient descent.


00:06:54,420 --> 00:06:57,140
Update the same functions inside the
while loop.


00:07:23,660 --> 00:07:27,180
As you can see, we have achieved very
similar results.


00:07:31,040 --> 00:07:34,320
Duplicate the wrangle node to implement
the contour lines.


00:07:35,360 --> 00:07:37,740
Turn off the cost, as we don't need it.


00:07:39,100 --> 00:07:40,880
Then do a cross product between the


00:07:40,880 --> 00:07:43,560
gradient vector and the normal
like before.


00:07:54,880 --> 00:07:58,260
Also for heightfield, we might have to
reduce the tolerance a bit.


00:08:01,600 --> 00:08:05,280
As you can see, we have successfully
implemented contour lines


00:08:05,280 --> 00:08:06,980
fully using heightfields.


00:08:24,880 --> 00:08:28,720
If you compare with the previous
approach, you can see the direction is reverse.


00:08:37,920 --> 00:08:42,460
You can just remove the negation from
the gradient vector to match their direction.


00:09:00,460 --> 00:09:03,380
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

49 - Geometric Advection - Orthogonalization & Flowlines - Introduction

----------------------------------------------------------------------------------------------------



00:00:04,920 --> 00:00:09,220
Through the last few chapters, with all the
beautiful structures we have built, using


00:00:09,220 --> 00:00:15,600
gradient ascent, descent, and contour lines,
there is also a functional component to all of this.


00:00:16,640 --> 00:00:21,480
When you want to visualize a vector attribute
or a set of vector attributes, the default


00:00:21,490 --> 00:00:25,869
viewport visualizers fall a bit short, in
terms of being able to read the overall flow


00:00:25,869 --> 00:00:26,900
of the vectors.


00:00:27,710 --> 00:00:32,100
It doesn't take much imagination to see how
we could use the same polygon curve tracing


00:00:32,120 --> 00:00:35,980
code we wrote to visualize vector attributes
on arbitrary geometry.


00:00:37,430 --> 00:00:42,380
First, we will create an orthogonal reference
frame on arbitrary polygonal geometry using


00:00:42,380 --> 00:00:43,620
orthogonalization.


00:00:45,900 --> 00:00:51,479
In linear algebra, orthogonalization is the
process of finding a set of orthogonal vectors


00:00:51,480 --> 00:00:53,800
that span a particular subspace.


00:00:54,320 --> 00:00:58,480
In addition,  if we want the resulting
 vectors to all be unit vectors,


00:00:58,620 --> 00:01:01,420
then the procedure is called orthonormalization.


00:01:02,180 --> 00:01:05,480
There are many methods for performing orthogonalization.


00:01:06,100 --> 00:01:08,140
We will use the double cross product method.


00:01:09,340 --> 00:01:11,620
Let's see the concept of the algorithm first.






----------------------------------------------------------------------------------------------------

50 - Geometric Advection - Orthogonalization & Flowlines - Concept

----------------------------------------------------------------------------------------------------



00:00:03,780 --> 00:00:05,660
Imagine a geometry like this.


00:00:08,080 --> 00:00:09,960
We have the surface normal N.


00:00:10,500 --> 00:00:15,500
For every point, we take the first neighbour
to create a direction from the current point


00:00:15,500 --> 00:00:17,040
to the first neighbour point.


00:00:17,300 --> 00:00:18,300
Let's call it u.


00:00:19,340 --> 00:00:22,960
This vector is not guaranteed to be
 orthogonal to the normal.


00:00:26,040 --> 00:00:28,980
So we take the cross product of the normal and u.


00:00:30,300 --> 00:00:34,160
This vector will be orthogonal to both the
normal and u.


00:00:35,080 --> 00:00:41,140
We then use this new vector v, and perform
another cross product of v and the normal.


00:00:41,720 --> 00:00:45,940
The new u will also be orthogonal to both
v and the normal.


00:00:46,360 --> 00:00:51,220
We have now created an orthogonal reference
frame on an arbitrary polygonal geometry.


00:00:52,020 --> 00:00:54,560
This process is known as orthogonalization.


00:00:57,800 --> 00:01:00,420
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

51 - Geometric Advection - Orthogonalization & Flowlines - Implementation

----------------------------------------------------------------------------------------------------



00:00:05,359 --> 00:00:07,859
We will create a simple sphere with some noise.


00:00:30,160 --> 00:00:32,600
I will remesh this for uniform topology.


00:00:39,840 --> 00:00:42,500
Create a wrangle node for the reference frame code.


00:00:47,360 --> 00:00:50,800
Get the first neighbor of the current point
and get its position.


00:00:56,160 --> 00:00:59,420
Then subtract the current point position
from this position


00:00:59,520 --> 00:01:02,900
to create a vector that goes from
the current point position


00:01:03,040 --> 00:01:05,360
towards the first neighbor point position.


00:01:06,240 --> 00:01:11,200
Just like in the concept drawing, first perform a cross product between the normal and u.


00:01:12,260 --> 00:01:16,020
Then perform another cross product
between v and the normal.


00:01:55,439 --> 00:01:58,600
As you can see, it is hard to read
the general flow of vectors


00:01:58,600 --> 00:02:02,280
using the default viewport visualizers, 
even with the arrow tips,


00:02:02,560 --> 00:02:04,560
which is actually an output trick.


00:02:05,920 --> 00:02:09,180
I will blur u and v a bit to remove some noise from them.


00:02:17,920 --> 00:02:19,280
Scatter some points


00:02:19,280 --> 00:02:23,220
and copy one of the latest contour lines wrangle note from the previous scene.


00:02:40,320 --> 00:02:44,480
As you can see, we immediately get a feel for the flow in the vector field.


00:02:45,400 --> 00:02:47,560
do the same for the v attribute.


00:03:30,560 --> 00:03:35,240
Even when you merge both, you can read the flow of each vector field quite clearly.


00:03:35,840 --> 00:03:41,140
You can play with the max dist and the density of points to read the various features in the vector field.


00:04:04,240 --> 00:04:07,700
You can very clearly see the curl around this area for example.


00:04:08,880 --> 00:04:13,460
It will make sense to make a generic SOP out of this and use it in your Workflow,


00:04:14,040 --> 00:04:17,780
I will leave that fun part to you, as it is 
pretty easy to go from here.


00:04:34,960 --> 00:04:37,700
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

52 - Clustering & Quadtrees - Concept

----------------------------------------------------------------------------------------------------



00:00:11,560 --> 00:00:17,580
What we see right now, your first intuition
might be to create it using adaptive subdivision.


00:00:18,000 --> 00:00:22,240
And it can definitely be done using adaptive
subdivision, where we could take a simple


00:00:22,250 --> 00:00:27,750
grid geometry and recursively subdivide it
based on some rule until all the conditions


00:00:27,750 --> 00:00:30,740
are satisfied or we reach the end of the loop.


00:00:31,400 --> 00:00:35,180
But sometimes to solve a problem, 
you have to start at the end,


00:00:35,180 --> 00:00:37,440
and work your way back to the start.


00:00:38,580 --> 00:00:42,740
So in this case, we will take a simpler approach
using divide and conquer.


00:00:44,460 --> 00:00:46,280
Let's see the concept of the algorithm.


00:00:48,140 --> 00:00:51,020
Imagine an XZ grid that has image data.


00:00:51,940 --> 00:00:55,300
First thing we will do is to quantize the
colors on the grid.


00:00:55,940 --> 00:01:01,780
We will do this by clustering N colors and
then for each color on the grid, assign the


00:01:01,780 --> 00:01:03,900
nearest color from the clustered colors.


00:01:04,240 --> 00:01:08,660
This is a way effective way to reduce data
using k-means clustering.


00:01:09,060 --> 00:01:13,300
We will then create a feedback loop
that runs for X iterations or less


00:01:13,440 --> 00:01:15,600
if no more subdivision can be done.


00:01:21,680 --> 00:01:24,940
For each iteration, we will get the connectivity data.


00:01:26,860 --> 00:01:30,300
For each connected piece that's not marked as final,


00:01:31,820 --> 00:01:33,620
if the number of unique colors are


00:01:33,620 --> 00:01:37,380
greater or equal to M, 
then we will subdivide this piece.


00:01:38,220 --> 00:01:41,100
How we subdivide depends on the shape of the piece.


00:01:41,940 --> 00:01:47,280
If it's a square or almost square, we will
split it in both axes from the center.


00:01:54,780 --> 00:02:00,259
If it's not square, then we will try to converge
to a square by splitting the current piece


00:02:00,260 --> 00:02:05,120
across the longest axis, where at least one
part will form a square.


00:02:05,780 --> 00:02:11,720
So if x is larger than z, then we will split
the current piece, x units along the z axis,


00:02:11,900 --> 00:02:14,180
so we get a square on the left side.


00:02:15,000 --> 00:02:21,220
If z is larger than x, then we will split
the current piece, z units along x,


00:02:21,480 --> 00:02:23,780
so we get a square on the top side.


00:02:24,760 --> 00:02:30,080
If the number of unique colors is less than
M, then we will create a single point where


00:02:30,090 --> 00:02:35,409
we'll copy a single polygon, to replace the current
piece, and set its color to the average color


00:02:35,409 --> 00:02:36,340
of the current piece.


00:02:36,700 --> 00:02:41,120
Then mark this point as final, so that it's
excluded from the for loop.


00:02:42,000 --> 00:02:46,840
Doing this for a few iterations, we will very
quickly converge to squares, and also keep


00:02:46,840 --> 00:02:49,820
detail only where there is high color variance.


00:02:52,260 --> 00:02:55,120
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

53 - Clustering & Quadtrees - Implementation

----------------------------------------------------------------------------------------------------



00:00:03,960 --> 00:00:08,100
We can create the image grid using a grid geometry and Attribute From Map SOP


00:00:08,640 --> 00:00:13,400
but this time I will use the COP2 Network that has options to output a polygonal geometry.


00:00:13,980 --> 00:00:16,260
So create a COP2 network, and a file COP.


00:00:16,900 --> 00:00:20,560
I already have an image set but feel free to use any image that you want.


00:00:21,600 --> 00:00:24,080
Change its orientation to ZX plane.


00:00:24,580 --> 00:00:28,720
We need to rotate the geometry so it will be properly aligned to the top view.


00:00:32,040 --> 00:00:35,060
As you can see, we are getting 1 point per pixel,


00:00:35,060 --> 00:00:41,580
and as such an image of 2560 by 1440 
gives us almost 4 millions pixels.


00:00:42,120 --> 00:00:45,100
To reduce the number of pixels, and polygons


00:00:45,105 --> 00:00:47,955
we need to scale the image inside the COP network.


00:00:48,020 --> 00:00:50,300
I will just scale it down 10 times.


00:00:53,080 --> 00:00:55,995
Because we will split the polygons, we need to convert


00:00:55,995 --> 00:00:58,805
this Mesh geometry into a regular polygonal geometry.


00:00:59,155 --> 00:01:01,975
Now we will quantize the colors which will reduce


00:01:01,975 --> 00:01:05,435
the number of unique colors to a 
limited set of distinct colors.


00:01:05,800 --> 00:01:08,340
We can also do this using COPs,


00:01:08,340 --> 00:01:10,960
but I want to show a use case of the Cluster SOP.


00:01:11,380 --> 00:01:13,120
This is a low-level node


00:01:13,120 --> 00:01:16,220
that uses K-means clustering algorithm to output


00:01:16,225 --> 00:01:19,025
either points with a cluster attribute


00:01:19,025 --> 00:01:20,575
or cluster centers.


00:01:21,280 --> 00:01:23,520
This node also creates a cluster integer


00:01:23,520 --> 00:01:26,680
attribute on the points which indicates the cluster number


00:01:26,680 --> 00:01:28,420
the point belongs to.


00:01:28,420 --> 00:01:31,940
All points in the same cluster will have the same cluster value,


00:01:31,940 --> 00:01:34,280
which will make our job easier in VEX.


00:01:35,040 --> 00:01:37,740
For now I will just use 8 clusters.


00:01:38,700 --> 00:01:42,500
Turn on Output Cluster Centers which will give you 8 points.


00:01:43,280 --> 00:01:46,940
We will use these points to quantize
the colors using VEX.


00:01:47,840 --> 00:01:49,360
Create an attribute wrangle node.


00:01:49,900 --> 00:01:53,400
What we have to do is to find the nearest color for each point.


00:01:53,755 --> 00:01:56,705
If you think of colors as 3d positions,


00:01:56,705 --> 00:01:59,265
you can actually measure the distance between them.


00:02:05,555 --> 00:02:08,455
So loop over the points in the second input,


00:02:08,455 --> 00:02:11,335
and check if the squared distance between them


00:02:11,340 --> 00:02:14,320
is less than the minimum distance 
we are keeping track of.


00:02:23,540 --> 00:02:26,605
If it's less, then update this distance variable,


00:02:26,605 --> 00:02:28,820
and assign the result to this color,


00:02:33,920 --> 00:02:37,560
and also store the current index,
which will be the cluster number.


00:02:47,880 --> 00:02:50,720
As you can see we have the cluster numbers.


00:02:50,960 --> 00:02:54,060
We will first create the splitting methods individually.


00:02:54,300 --> 00:02:58,600
So to see the result easier, we need to reduce this geometry to a single polygon.


00:02:59,100 --> 00:03:02,100
We can do this using the divide and facet nodes.


00:03:11,700 --> 00:03:14,580
Now create a clip node to clip in X.


00:03:24,000 --> 00:03:27,760
We set the clip origin to X Min, so that we start from there


00:03:30,480 --> 00:03:33,560
and we set the clip distance to Z size.


00:03:33,700 --> 00:03:36,020
If you remember just like in the concept,


00:03:36,080 --> 00:03:39,880
this will allow us to cut a square section in X axis.


00:03:40,200 --> 00:03:42,880
Set Keep to Primitives Below the Plane


00:03:42,900 --> 00:03:45,000
so we keep the square piece first.


00:03:45,080 --> 00:03:47,760
Duplicate this node and invert the operation.


00:03:48,020 --> 00:03:51,360
If you keep both sides, then the result is fused together.


00:03:51,500 --> 00:03:54,740
We don't want this. That's why we have to use 2 nodes


00:03:54,760 --> 00:03:56,160
and then merge the result.


00:03:56,540 --> 00:03:59,720
I will also collapse this into a subnet to reduce clutter.


00:04:04,840 --> 00:04:09,440
Add a spare input and paste relative channel reference to the clip nodes inside.


00:04:16,980 --> 00:04:19,920
Now we will create the splitting method in Z instead.


00:04:19,980 --> 00:04:22,420
Just rotate the geometry temporarily


00:04:22,420 --> 00:04:25,140
so that the Z size is bigger than X size.


00:04:25,820 --> 00:04:29,000
We do the exact same operation but in Z axis this time.


00:05:00,600 --> 00:05:03,495
The last splitting method we have to do is to


00:05:03,495 --> 00:05:05,865
subdivide the entire piece once.


00:05:06,215 --> 00:05:08,995
But we can't use the subdivide node as it will


00:05:09,000 --> 00:05:11,220
subdivide each polygon separately.


00:05:11,220 --> 00:05:14,500
We just want to split it by the center in 2 axes.


00:05:54,100 --> 00:05:58,020
As you can see, we have successfully 
split the piece into 4 pieces.


00:06:14,940 --> 00:06:18,480
Create a for loop network that loops over each connected piece.


00:06:57,040 --> 00:07:01,300
First we will set the logic to decide 
between splitting in X or Z.


00:07:12,220 --> 00:07:14,845
If size X of the input piece is


00:07:14,845 --> 00:07:17,355
less than the size Z of the same piece,


00:07:17,355 --> 00:07:18,895
then we split in Z.


00:07:21,960 --> 00:07:24,260
As you can see, currently


00:07:24,260 --> 00:07:26,940
size x is greater than size z.


00:07:36,080 --> 00:07:40,220
Now we will set the logic to split in one axis or 2 axes.


00:07:42,840 --> 00:07:46,540
If the current piece is square within a given tolerance,


00:07:46,540 --> 00:07:48,880
we will split in 2 axes.


00:07:49,095 --> 00:07:52,015
To do this, we just take the absolute difference


00:07:52,015 --> 00:07:55,205
between size X and Z of the current piece.


00:07:55,375 --> 00:07:57,855
If it's less than 0.01,


00:07:57,855 --> 00:07:59,585
then we consider it a square.


00:08:13,785 --> 00:08:16,785
Create a detail wrangle and store the number of


00:08:16,785 --> 00:08:20,800
unique clusters in the current piece 
using the nuniqueval function.


00:08:41,560 --> 00:08:45,660
Now create a feedback loop that will repeat this entire for loop


00:08:45,660 --> 00:08:47,680
for a given number of iterations.


00:09:05,820 --> 00:09:08,640
As you can see, we have some splits.


00:09:09,795 --> 00:09:12,845
Just create a spare parameter that will allow us to


00:09:12,845 --> 00:09:15,465
specify the number of minimum unique colors that the


00:09:15,465 --> 00:09:17,625
inner for loop network will use.


00:09:18,125 --> 00:09:21,155
The only logic left is the one that will decide


00:09:21,160 --> 00:09:24,800
between splitting the geometry or converting the current piece


00:09:24,800 --> 00:09:27,135
into a single point where the color is the


00:09:27,135 --> 00:09:30,395
average color of all the unique colors in that piece.


00:09:47,715 --> 00:09:50,065
For now just check if the number of


00:09:50,065 --> 00:09:52,395
unique colors in the current piece is greater than


00:09:52,400 --> 00:09:56,700
or equal to the minimum colors we have
specified in the outer loop.


00:09:57,180 --> 00:10:00,180
If not, we will convert the current piece into a


00:10:00,180 --> 00:10:02,720
single point at the center of the current piece.


00:10:03,085 --> 00:10:05,035
So create a new detail wrangle.


00:10:11,435 --> 00:10:14,305
We will first get an array of unique cluster values,


00:10:14,305 --> 00:10:17,135
and using these cluster values we will look up


00:10:17,135 --> 00:10:20,545
the point index that has a particular cluster value.


00:10:21,035 --> 00:10:24,115
We only need to look up one point per cluster as


00:10:24,115 --> 00:10:27,565
all points with a given cluster, all share the same color.


00:10:31,005 --> 00:10:33,685
So using the point index that belongs to the


00:10:33,685 --> 00:10:37,640
current cluster value we are looping over, we get the color of this point.


00:10:39,920 --> 00:10:41,820
We sum the color values.


00:10:43,200 --> 00:10:47,460
In the end, we just divide the summed color value by the number of clusters.


00:10:50,140 --> 00:10:52,980
Get the center of the bounding box of the current piece,


00:10:52,980 --> 00:10:57,380
which is the second input. 
We will create the new point at this location.


00:10:57,840 --> 00:11:00,940
Then get the size of the bounding box of the current piece.


00:11:00,965 --> 00:11:04,005
We will store this as scale so that when we


00:11:04,005 --> 00:11:07,395
copy new polygons, they will be perfectly resized to fit.


00:11:09,885 --> 00:11:12,005
Also make sure to store the color.


00:11:19,295 --> 00:11:22,485
Finally add this point to the final point group,


00:11:22,485 --> 00:11:25,175
so that in the next iteration of the feedback loop,


00:11:25,175 --> 00:11:28,015
we will skip these points from the inner loop.


00:11:31,260 --> 00:11:35,040
As you can see, we have the new point
with the proper attributes.


00:11:45,280 --> 00:11:48,520
So now some of the pieces are converted into points.


00:11:49,160 --> 00:11:51,860
But watch what happens when I increase the iterations.


00:11:54,060 --> 00:11:58,900
As you can see, what we had as points before, those pieces are now gone.


00:11:59,280 --> 00:12:03,680
This is because we need to manually merge these points inside the feedback loop.


00:12:13,940 --> 00:12:16,960
You can use Split and Merge nodes to do this.


00:12:35,635 --> 00:12:38,875
As you can see, we now have these points as well.


00:12:39,560 --> 00:12:44,340
We also want to early terminate the feedback loop if no new splitting can be done.


00:12:44,760 --> 00:12:50,400
So create a detail attribute called subdivided
and set it to 0 before the inner loop.


00:13:00,120 --> 00:13:03,420
Then set it to 1, in the splitting branch.


00:13:03,980 --> 00:13:07,520
I will also compile the inner loop for maximum performance.


00:13:07,960 --> 00:13:13,380
This is one of the scenarios where compiling 
can provide significant performance improvements.


00:13:20,980 --> 00:13:24,200
Make sure to turn on multi-threading
in the inner for loop.


00:13:26,340 --> 00:13:30,040
In the stop condition, check if subdivided is 0.


00:13:30,040 --> 00:13:33,640
If so, that means, no new pieces have been split


00:13:33,640 --> 00:13:35,640
and as such we don't need to perform
another feedback iteration.


00:13:38,020 --> 00:13:42,760
Finally we need to convert all the remaining
polygonal pieces into points.


00:13:43,420 --> 00:13:48,120
So first isolate the final group and 
loop over the remaining geometry.


00:14:01,440 --> 00:14:06,080
Copy the same wrangle node that creates a single point from a given geometry.


00:14:07,180 --> 00:14:10,580
As you can see, we have everything as points now.


00:14:11,145 --> 00:14:14,635
Clean up the attributes we have created 
and the final group.


00:14:24,800 --> 00:14:29,240
Now create a 1 by 1 grid geometry with 2 by 2 divisions.


00:14:30,720 --> 00:14:35,820
Copy this geometry onto the points, 
and inherit the colors from the points.


00:14:36,520 --> 00:14:40,120
Now I will just play with some of the
parameters to see what we get.


00:14:44,480 --> 00:14:49,100
As we increase the iterations, we retain more detail from the input image.


00:14:49,680 --> 00:14:54,560
You can see how more geometry is concentrated around high detail areas.


00:14:54,780 --> 00:14:59,480
Areas with less color variation are approximated using larger squares.


00:15:00,100 --> 00:15:06,180
And even though it starts out as a non-square shape, it very quickly converges into perfect squares.


00:15:06,640 --> 00:15:10,140
We also need to add another condition to the splitting logic.


00:15:10,480 --> 00:15:15,160
If the size of the current piece is less than a certain threshold, we don't split it.


00:15:15,200 --> 00:15:18,260
So it will also be converted into a single point.


00:15:50,120 --> 00:15:54,240
At 15 iterations, we still perform all of the iterations.


00:15:54,740 --> 00:15:57,780
So I am gonna gradually increase the
minimum size threshold.


00:16:07,000 --> 00:16:11,540
As you can see, using a minimum size threshold of 0.05,


00:16:11,540 --> 00:16:16,960
allowed the loop to early terminate at iteration 14. So I guess we can use that.


00:16:18,540 --> 00:16:21,640
As you decrease the minimum unique colors, the approximations


00:16:21,640 --> 00:16:24,520
will be denser and so more accurate in retaining


00:16:24,525 --> 00:16:27,675
the original detail, but also will take more time to cook.


00:16:29,085 --> 00:16:31,735
As you increase the minimum unique colors,


00:16:31,740 --> 00:16:35,460
more detail will be represented by
larger pieces of geometry.


00:16:36,040 --> 00:16:39,635
If you set it to a value larger than the available number of unique colors,


00:16:39,640 --> 00:16:44,080
then no splitting will occur and you will get a single polygon as a result.


00:16:59,420 --> 00:17:02,440
Until now we have used RGB colors


00:17:02,440 --> 00:17:05,380
as the base for clustering. But we can also


00:17:05,380 --> 00:17:07,540
use any other attribute that we want.


00:17:07,940 --> 00:17:11,640
So instead of RGB colors, we can use HSV colors.


00:17:11,640 --> 00:17:16,320
As you can imagine that would alter the result of the clustering dramatically.


00:17:16,680 --> 00:17:23,080
But we still want to keep the RGB colors for final coloring, otherwise the final image won't look the same.


00:17:23,420 --> 00:17:26,400
So duplicate the cluster and quantize nodes.


00:17:37,685 --> 00:17:40,255
Store the actual color as Cd2


00:17:40,260 --> 00:17:43,700
so we don't have to modify the existing network as much.


00:17:44,040 --> 00:17:50,760
Rename cluster to cluster2 also, as the cluster values between RGB and HSV colors will be very different.


00:17:51,080 --> 00:17:53,960
Copy Cd2 and cluster2 to the main branch.


00:17:58,100 --> 00:18:00,975
We have to modify the code that converts geometry


00:18:00,980 --> 00:18:02,820
into a single point.


00:18:02,820 --> 00:18:06,680
It has to repeat the same averaging logic for Cd2 as well.


00:18:07,040 --> 00:18:10,840
So I am gonna make it easier by storing attribute names in arrays


00:18:10,840 --> 00:18:13,600
so the code doesn't need to be duplicated.


00:18:21,625 --> 00:18:24,425
I will store the average color in an array.


00:19:04,580 --> 00:19:08,580
Make sure to save both Cd and Cd2 using the colors array.


00:19:24,260 --> 00:19:29,760
As you can see, Cd2 includes very distinct colors that are quite different than Cd.


00:19:35,940 --> 00:19:39,320
Copy the same code into the 
second create_new_point node.


00:19:45,160 --> 00:19:49,440
The result is quite different than what we got when we used RGB Colors.


00:19:49,960 --> 00:19:54,280
And the colors, while interesting in their own right, look nothing like the original colors.


00:19:55,060 --> 00:19:58,980
That's because we're looking at HSV color
values as RGB colors.


00:19:59,480 --> 00:20:02,100
So just move Cd2 into Cd.


00:20:03,885 --> 00:20:07,065
As you can see, we got the original colors back.


00:20:07,140 --> 00:20:13,040
Essentially what we have done is, drive clustering of image data, using an arbitrary attribute


00:20:13,040 --> 00:20:18,200
like HSV colors, while showing them using the average color of the actual pieces,


00:20:18,200 --> 00:20:21,140
that are clustered using the RGB colors.


00:20:21,580 --> 00:20:24,940
We can play with the usual parameters
to see what we get.


00:20:55,360 --> 00:21:00,440
Let's also experiment with only clustering by hue or saturation or value.


00:21:14,100 --> 00:21:18,600
We can only cluster by a blurred version
of the same RGB colors.


00:21:39,400 --> 00:21:44,620
As you decrease minimum colors, you start to see the bands between different regions of the image.


00:22:25,760 --> 00:22:32,000
As you increase the iterations, we start getting more and more detail on the areas that have more variation.


00:22:50,720 --> 00:22:53,795
Since the final colors we see are clustered separately


00:22:53,795 --> 00:22:56,435
than the clustering that drives the splitting,


00:22:56,435 --> 00:22:58,945
we can play with the number of clusters there.


00:23:06,500 --> 00:23:10,980
As we increase the number of colors, we can get a more accurate color representation,


00:23:10,980 --> 00:23:14,160
independent of how the image itself is clustered.


00:23:21,040 --> 00:23:24,760
We can also increase the number of clusters that drive the splitting.


00:23:39,420 --> 00:23:43,420
I will just switch back to using value for
the primary clustering.


00:23:45,960 --> 00:23:50,940
As you can see, we retain a fairly good
amount of detail in high-detail areas.


00:23:52,160 --> 00:23:59,000
With a few relatively simple rules, we have transformed image data into much more intricate patterns


00:23:59,140 --> 00:24:02,040
and approximate it in a visually pleasing way.


00:24:02,780 --> 00:24:08,340
You can add more rules to influence the look of the final result in a variety of ways.


00:24:08,640 --> 00:24:11,600
I will leave that up to you to experiment on your own.


00:24:12,320 --> 00:24:18,740
In the next chapter, we will look at a similar concept called adaptive subdivision, but this time in 3d.


00:24:19,100 --> 00:24:25,000
Instead of using colors, we will use proximity to another geometry to drive the subdivision.


00:24:25,960 --> 00:24:29,220
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

54 - Adaptive Subdivision - Introduction

----------------------------------------------------------------------------------------------------



00:00:03,300 --> 00:00:07,800
- Adaptivity is not a technique, it's a mindset.


00:00:09,760 --> 00:00:15,540
One of my favourite concepts in software design
regarding performance is adaptivity.


00:00:16,600 --> 00:00:21,160
Given the size of the massive data sets that
we have to work with in visual effects,


00:00:21,200 --> 00:00:25,400
optimization can enable you to have
more iterations in your workflow.


00:00:26,020 --> 00:00:32,400
One of these optimization concepts is adaptivity,
which is about focusing your resources where it matters.


00:00:33,480 --> 00:00:39,140
Given the same amount of resources, you can
either implement adaptivity and go way beyond


00:00:39,140 --> 00:00:42,860
what's possible otherwise, while still using
the same amount of resources


00:00:42,860 --> 00:00:44,500
as a non-adaptive approach,


00:00:44,500 --> 00:00:49,720
or you could have the same amount of detail
as a non-adaptive approach while keeping your


00:00:49,720 --> 00:00:52,360
memory and computation footprint very light.


00:00:53,060 --> 00:00:57,680
It's still good to be able to do the former
even if you don't need it, just to see how


00:00:57,680 --> 00:01:01,360
much adaptive approaches can help you push
your hardware to its limits.


00:01:02,140 --> 00:01:05,980
So the higher you go in resolution, 
the bigger the savings become.


00:01:07,160 --> 00:01:09,320
To put things into perspective:


00:01:09,320 --> 00:01:15,460
A 20 thousand polygon model, which is quite
low, could roughly use 2.5MB of memory


00:01:15,560 --> 00:01:19,680
only for the position attribute, and the
connectivity data of the geometry.


00:01:20,280 --> 00:01:26,440
If you were to subdivide this geometry until you reach
subdivision level 10, it would consume about


00:01:26,440 --> 00:01:31,680
1.6tb of memory, producing almost 21 billion polygons


00:01:31,920 --> 00:01:36,880
taking more than 7 hours to cook,
if you had more than a terabyte of memory.


00:01:36,880 --> 00:01:42,899
Whereas if you added detail adaptively only
where you needed, for a fairly good coverage,


00:01:42,900 --> 00:01:48,020
i.e. cracks on a 3d head, like in the movie
X-Men: Dark Phoenix, it would consume about:


00:01:48,020 --> 00:01:53,620
250mb of memory
while having about 1.5 million polygons


00:01:53,620 --> 00:01:55,300
taking about a minutes to cook


00:01:55,780 --> 00:01:59,820
This is the difference between practical and impossible.


00:02:00,540 --> 00:02:05,919
1 minute is nothing in terms of computation
time, especially when this is a pre-processing


00:02:05,919 --> 00:02:11,000
operation, and yet while still keeping an
extremely light geometry that can keep up


00:02:11,000 --> 00:02:14,800
with real-time performance for what might
come later in your setup.


00:02:16,220 --> 00:02:21,220
To reach these levels of performance and interactivity,
while using the best subdivision algorithms,


00:02:21,260 --> 00:02:27,520
i.e. OpenSubdiv Catmull-Clark subdivision, we will
go through an incredible amount of problem solving.


00:02:28,000 --> 00:02:33,160
It's this sort of technical firepower that
you will learn to appreciate in highly complex


00:02:33,160 --> 00:02:37,980
technical setups, in that almost always, there
is always a way.


00:02:39,620 --> 00:02:42,180
Let's see how to implement it in Houdini now.






----------------------------------------------------------------------------------------------------

55 - Adaptive Subdivision - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,640 --> 00:00:10,080
First create a polygonal grid geometry and a NURBS circle that has the same orientation as the grid.


00:00:14,800 --> 00:00:18,940
I set the uniform scale to 4 to get a larger coverage on the grid.


00:00:19,480 --> 00:00:22,100
I create a subnet to have everything in one place.


00:00:31,040 --> 00:00:33,440
Just create the parameters for iterations,


00:00:39,840 --> 00:00:40,680
radius,


00:00:44,320 --> 00:00:45,780
radius multiplier,


00:00:50,480 --> 00:00:53,460
a checkbox for visualizing subdivision levels by color


00:01:01,280 --> 00:01:03,000
and a seed parameter.


00:01:25,120 --> 00:01:29,580
Now the most important question before we start implementing adaptive subdivision is


00:01:29,680 --> 00:01:31,920
how will we do the proximity lookups.


00:01:33,040 --> 00:01:36,160
As always in Houdini, there are multiple ways to do this.


00:01:37,920 --> 00:01:39,900
As you can see with this NURBS curve,


00:01:40,100 --> 00:01:45,000
there aren't that many points to check against. So let's say we increase divisions to 100.


00:01:45,600 --> 00:01:51,240
As we zoom in closer, even this becomes not sufficient to capture all the primitives we might have.


00:01:52,240 --> 00:01:56,800
This is just to say that using the actual points of the proximity geometry is not ideal.


00:01:57,600 --> 00:01:59,740
As the subdivision levels increase,


00:01:59,760 --> 00:02:05,860
the radius will decrease more and more and the gap between the actual points will be too large to be precise


00:02:06,880 --> 00:02:11,300
So we will use the actual distance to the
geometry itself, rather than its points.


00:02:12,000 --> 00:02:15,380
That means xyzdist rather than pcfind.


00:02:34,160 --> 00:02:37,919
So first create the radius as a 64-bit detail attribute.


00:02:38,560 --> 00:02:44,280
We will need the full precision of 64-bit attribute as the radius gets smaller and smaller.


00:02:46,560 --> 00:02:50,080
I'm just gonna set up some initial values.
you can use the same values.


00:02:57,840 --> 00:03:01,380
Now create a depth attribute as integer on primitives.


00:03:01,700 --> 00:03:06,000
Default should be -1 to denote anything
outside the input group.


00:03:08,080 --> 00:03:11,500
Create a feedback loop that will contain
most of the logic.


00:03:16,320 --> 00:03:19,800
Create a primitive wrangle node to 
do the proximity lookups.


00:03:24,160 --> 00:03:28,080
We will use the center of each perimeter as the origin of the proximity lookups.


00:03:28,560 --> 00:03:34,420
We will check if the distance is within the current radius, as we will halve it at 10th of each iteration.


00:03:48,480 --> 00:03:52,140
I will use adaptive subd as the suffix for each group.


00:04:10,480 --> 00:04:13,620
As you can see, we have found some primitives.


00:04:14,240 --> 00:04:17,460
Use the output selection group to highlight
these primitives.


00:04:20,800 --> 00:04:25,360
Now use the group we created in another primitive wrangle to set the current depth


00:04:25,360 --> 00:04:28,260
which is the same as the current iteration plus 1.


00:04:42,240 --> 00:04:44,880
Now create a subdivide node with the same group.


00:04:47,760 --> 00:04:50,920
For now, we will use the OpenSubdiv Bilinear algorithm.


00:04:59,520 --> 00:05:06,160
Finally, a detailed wrangle to reduce the radius attribute at the end of each iteration, using the radius multiplier.


00:05:20,480 --> 00:05:25,540
We will also delete all the groups we created, as we will already have the geometry subdivided


00:05:25,840 --> 00:05:30,360
and the depth attribute already stored at this point, 
so we don't need the groups anymore.


00:05:35,680 --> 00:05:38,520
In the end we will also delete the radius as well.


00:05:47,960 --> 00:05:52,300
We will also write some code to color each step randomly using a seed value.


00:06:20,640 --> 00:06:26,820
I'm using some hard coded values to get the initial colors that I liked. You can use any values that you want.


00:06:32,640 --> 00:06:34,300
Also for the group parameter,


00:06:34,300 --> 00:06:37,600
make sure to only include primitives that are above 0.


00:06:38,000 --> 00:06:40,600
0 means outside the initial radius.


00:06:44,320 --> 00:06:48,880
Make sure to add a spare input for the meta node so we can read the current iteration value.


00:06:58,400 --> 00:07:01,840
As you can see, each subdivision level has a different color.


00:07:06,560 --> 00:07:11,039
We are missing some of the primitives, even though the circle geometry goes over them.


00:07:11,599 --> 00:07:15,379
This just shows that the center of each
perimeter is not enough.


00:07:17,600 --> 00:07:19,660
So what can we do to remedy this?


00:07:20,560 --> 00:07:26,440
We can also include the points of each primitive to increase accuracy, using the primpoints function.


00:07:33,760 --> 00:07:39,240
We first add @P to a vector array so we can loop over these to check for proximity.


00:08:30,080 --> 00:08:34,040
If it is less than or equal to the max distance,
then we break the loop.


00:08:38,240 --> 00:08:41,260
As you can see, the result improved dramatically.


00:08:46,960 --> 00:08:51,900
We can zoom in closer and see it actually correctly finds each nearby primitive


00:08:51,940 --> 00:08:54,360
within the radius of the proximity geometry.


00:09:07,200 --> 00:09:11,600
But as we reduce the radius, you can see that we run into the same issue as before.


00:09:12,400 --> 00:09:17,060
So we definitely don't want to use large enough radius to include legitimate primitives.


00:09:17,200 --> 00:09:19,060
They should be included anyway,


00:09:19,060 --> 00:09:24,180
as they might not have their center or their points within the range of the proximity geometry


00:09:25,040 --> 00:09:30,720
We need something more precise than this to make sure we get every primitive within the current radius.


00:09:32,960 --> 00:09:37,020
As we zoom in closer, you can see the missing primitives in between.


00:09:37,720 --> 00:09:39,600
so what can we do to improve this?


00:09:41,040 --> 00:09:44,040
Instead of just relying on primitive points and center,


00:09:44,040 --> 00:09:47,420
we will create our own point structure over the input geometry


00:09:48,480 --> 00:09:51,700
This structure will particularly be optimized for quads


00:09:51,700 --> 00:09:54,780
for uniform coverage across the surface of the input geometry.


00:09:56,480 --> 00:09:58,480
Let's see the concept of the algorithm first.


00:10:02,560 --> 00:10:05,240
Let's imagine a single quad polygon like this.


00:10:06,560 --> 00:10:10,080
We first split it into multiple polygons using its edges.


00:10:12,480 --> 00:10:18,480
For each of these polygons, we then create lines that go from each of their points towards its center.


00:10:27,839 --> 00:10:31,059
Then we resample everything using half of the current radius.


00:10:37,600 --> 00:10:40,400
This will give us a point formation roughly like this.


00:10:41,760 --> 00:10:45,840
We will use the resulting points as the origin for each lookup for a given perimeter.


00:10:52,640 --> 00:10:55,140
Let's see how to implement this in Houdini now.


00:10:59,360 --> 00:11:02,620
First create a primitive wrangle to 
store the primitive number


00:11:03,440 --> 00:11:07,840
Make sure to limit the group to the primitives that have the same depth as the current iteration,


00:11:08,480 --> 00:11:11,459
meaning the subdivided polygons of the last iteration.


00:11:27,120 --> 00:11:31,400
Then we will ask to keep these polygons with their original primitive numbers preserved.


00:11:39,840 --> 00:11:45,140
After this, we have to split each polygon so that they don't share any points with other polygons.


00:11:45,980 --> 00:11:49,400
Primitive Split is faster than Facet SOP in this operation.


00:11:56,560 --> 00:12:00,140
We then subdivide these polygons just 
like we saw in the concept.


00:12:01,180 --> 00:12:03,180
Subdivide SOP does this nicely.


00:12:03,839 --> 00:12:05,980
Make sure to use open software by linear


00:12:05,980 --> 00:12:10,339
as we don't want to smooth out the points rather only subdivide the polygons.


00:12:11,120 --> 00:12:13,779
Also turn off unnecessary options to increase performance.


00:12:14,800 --> 00:12:18,719
In this case, we don't care about point normals or removing of holes.


00:12:19,680 --> 00:12:22,760
Create another primitive wrangle to create the polylines


00:12:22,760 --> 00:12:25,206
that go from each of the primitive points


00:12:25,206 --> 00:12:26,339
towards its center.


00:12:57,760 --> 00:13:00,300
As you can see we now have the polylines.


00:13:01,520 --> 00:13:07,520
We now have to promote primid from primitives to points so we can access this data from the points.


00:13:08,560 --> 00:13:12,179
Make sure to use the First Match method rather than Average for performance.


00:13:13,120 --> 00:13:18,179
Now create a resample node to resample these polylines where Length is half the current radius.


00:13:32,079 --> 00:13:37,299
Make sure to resample by polygon edge and turn off Allow Primitive Attributes to Override Parameters.


00:13:38,399 --> 00:13:44,559
Lastly create an integer attribute on points called hitprim, and set the default value to -1.


00:13:45,600 --> 00:13:48,420
We will store which primitive is found in this attribute.


00:13:50,959 --> 00:13:57,679
So now instead of using the input geometry for xyzdist, we will use these points. So change Run Over mode to Points.


00:13:58,480 --> 00:14:00,500
The code will be simplified greatly.


00:14:04,079 --> 00:14:06,179
We only need to use @P now.


00:14:17,040 --> 00:14:22,579
Create a new primitive wrangle that will use the result of these points to group the primitives within radius.


00:14:32,639 --> 00:14:36,639
We just check if the hitprim value matches the current primitive, and if so,


00:14:37,040 --> 00:14:40,260
that means the primitive is within the radius of the proximity geometry.


00:14:56,399 --> 00:14:59,059
As you can see, we now have the nearby parameters.


00:15:10,959 --> 00:15:13,219
Let's increase the number of iterations to 12.


00:15:27,440 --> 00:15:32,280
Also change the Arc Type to Open Arc, we will see the differences between these soon.


00:15:36,960 --> 00:15:40,080
If you look at the polyline structure we have built up close,


00:15:40,080 --> 00:15:43,360
you can see we now have a fairly uniform dense coverage.


00:15:44,640 --> 00:15:48,060
If you noticed, the resample takes quite a 
bit of time to cook.


00:15:49,120 --> 00:15:52,960
Turn on Create Only Points to improve the performance significantly.


00:15:53,520 --> 00:15:57,300
After all we only need the points as the origin of the lookups, not the primitives.


00:16:01,680 --> 00:16:07,620
As we zoom in closer, you can see that we no longer have gaps in the nearby primitives within the radius.


00:16:12,400 --> 00:16:18,640
There is also an important distinction here. If you set the Arc Type Closed, you will see that the result is the same.


00:16:20,080 --> 00:16:24,660
Jeff Lait told me this is not a bug though, and it's the behaviour of Polygons that's wrong.


00:16:25,280 --> 00:16:27,960
It's because of the fundamental confusion in Houdini


00:16:27,960 --> 00:16:32,140
between a surface defined by a boundary and a closed polygonal loop.


00:16:32,480 --> 00:16:37,680
But if you switch the primitive type to polygon, you will see a closed polygon which covers the entire area,


00:16:37,759 --> 00:16:39,759
it is occupying over the input geometry.


00:16:40,720 --> 00:16:42,640
In any case this is a nice side effect


00:16:42,640 --> 00:16:45,520
that would enable you to either have curve based adaptivity,


00:16:45,520 --> 00:16:47,300
or surface based adaptivity,


00:16:47,680 --> 00:16:52,080
so it would be up to the artist to decide whichever they need for their particular needs.


00:16:52,880 --> 00:16:54,420
I switch back to NURBS


00:16:54,420 --> 00:16:56,720
even though it acts like an open curve,


00:16:56,720 --> 00:17:00,100
NURBS curves would still provide a smoother grouping of the primitives,


00:17:00,100 --> 00:17:02,620
using the xyzdist function.


00:17:03,200 --> 00:17:06,340
We will now try adaptive subdivision on a character head model.


00:17:07,520 --> 00:17:09,520
So copy the adaptive subdivide subnet.


00:17:11,199 --> 00:17:14,899
I will use this model but feel free to use any model you like.


00:17:28,080 --> 00:17:33,040
I am isolating the head geo because that's where I want to apply the adaptive subdivision.


00:17:33,320 --> 00:17:35,580
Draw some curves over the geometry.


00:17:56,560 --> 00:17:58,560
I will clean up the  curves a bit.


00:18:31,280 --> 00:18:35,939
Now ray the curves onto the geometry using minimum distance so they conform better.


00:18:45,120 --> 00:18:47,219
Finally I resample the curves.


00:19:03,360 --> 00:19:09,140
To visualize the radius, we can use poly wire, and fine tune the radius relative to the head model.


00:19:29,039 --> 00:19:31,219
You can increase the subdivision levels gradually.


00:19:37,440 --> 00:19:41,580
As we zoom in closer, you can see we are getting a very high quality result.


00:19:44,480 --> 00:19:50,420
There is something else that is very important though. Create a group node to group unshared edges.


00:20:01,360 --> 00:20:04,959
As you can see, we have a lot of this around each subdivision level.


00:20:05,600 --> 00:20:07,760
This is a known issue with the subdivide SOP.


00:20:08,480 --> 00:20:14,900
The only way to have closed borders while using sub-geometry is by using the Houdini Catmull Clark algorithm.


00:20:34,800 --> 00:20:38,980
But as you can see, the time complexity of this algorithm is exponential,


00:20:39,520 --> 00:20:42,360
4 to the power of n times t0,


00:20:42,540 --> 00:20:47,680
Where n is the number of iterations and t0 is the time it takes to cook the first iteration.


00:20:48,740 --> 00:20:51,060
That means after the first iteration,


00:20:51,060 --> 00:20:52,720
at each successive iteration,


00:20:52,720 --> 00:20:54,540
the cook time will quadruple.


00:20:54,540 --> 00:20:56,960
This becomes impractical very quickly.


00:20:57,840 --> 00:21:02,740
As you can see, there are no open borders around the subdivision levels using this algorithm.


00:21:03,040 --> 00:21:05,500
But the speed is just not reasonable.


00:21:06,720 --> 00:21:09,100
I switch back to open OpenSubdiv Bilinear.


00:21:49,840 --> 00:21:54,440
As you can see the performance difference is just too great and the gaps become wider


00:21:54,440 --> 00:21:55,920
as we increase iterations.


00:21:56,640 --> 00:22:03,359
So for subdivision level 10, it would be about 30 seconds vs 7 hours. This is too big of a difference to dismiss.


00:22:04,559 --> 00:22:08,900
In the next chapter we will see how we can solve this problem while still using


00:22:08,900 --> 00:22:11,380
the OpenSubdiv subdivision surfaces algorithm,


00:22:11,560 --> 00:22:15,060
among many other enhancements to really make it a production ready tool


00:22:15,400 --> 00:22:18,100
that could be used as part of more complex setups.


00:22:19,040 --> 00:22:22,100
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

56 - Hashing

----------------------------------------------------------------------------------------------------



00:00:04,680 --> 00:00:11,460
A hash function is any function that can be
used to map data of arbitrary size to fixed-size values.


00:00:12,540 --> 00:00:15,360
It basically converts one value to another.


00:00:16,620 --> 00:00:22,980
Hashing data is a common practice in computer
science and is used for several different purposes,


00:00:23,200 --> 00:00:28,020
like cryptography, compression,
checksum generation, and data indexing.


00:00:28,540 --> 00:00:33,280
Since hashed values are generally smaller
than the originals, it is possible for a hash


00:00:33,280 --> 00:00:35,840
function to generate duplicate hashed values.


00:00:36,690 --> 00:00:42,180
These are known as "collisions" and occur
when identical values are produced from different


00:00:42,180 --> 00:00:43,210
source data.


00:00:43,720 --> 00:00:48,920
Collisions can be resolved by using multiple
hash functions or by creating an overflow


00:00:48,920 --> 00:00:52,180
table when duplicate hashed values are encountered.


00:00:52,900 --> 00:00:56,780
Collisions can be avoided by using larger
hash values.


00:00:58,060 --> 00:01:03,460
In VEX, we have 3 functions for hashing integers,
strings and floats.


00:01:04,620 --> 00:01:06,380
Let's see them in Houdini.


00:01:08,980 --> 00:01:10,500
Create some geometry.


00:01:10,500 --> 00:01:12,500
And create a random float attribute.


00:01:12,840 --> 00:01:14,960
I will use Attribute Randomize.


00:01:27,320 --> 00:01:30,480
Create a wrangle to store the result of the
hash function.


00:01:39,940 --> 00:01:45,540
As you can see we are getting integer values
by feeding our float attribute as the seed.


00:01:49,760 --> 00:01:54,000
Note that the slightest change to the seed
values, will result in completely different


00:01:54,000 --> 00:01:55,620
values as you see here.


00:01:56,580 --> 00:02:02,400
For our purpose, we will use random_ihash
to convert point numbers to hash values so


00:02:02,800 --> 00:02:08,300
so that we reduce the chance of ending up with
the same interpolated values from the subdivide SOP.


00:02:08,860 --> 00:02:10,739
More on that in the next chapter.


00:02:11,480 --> 00:02:14,440
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

57 - Adaptive Subdivision - Improving OpenSubdiv Catmull-Clark Subdivision Surfaces Algorithm

----------------------------------------------------------------------------------------------------



00:00:05,440 --> 00:00:10,300
OpenSubdiv used in Subdivide SOP has the clear benefit of high performance,


00:00:10,300 --> 00:00:12,900
but it also comes with its own set of problems.


00:00:13,440 --> 00:00:17,580
The biggest problem is, especially because we are interested in adaptive subdivision,


00:00:17,760 --> 00:00:22,140
that means using a subset of the current geometry at each subdivision level.


00:00:22,640 --> 00:00:26,780
But doing so causes the geometry to be split up into separate pieces


00:00:26,780 --> 00:00:29,740
for each subset of primitives we wish to subdivide.


00:00:30,480 --> 00:00:36,540
Just doing fuse, is not only imprecise, and error prone, it's also extremely slow to do at each iteration.


00:00:37,280 --> 00:00:40,560
We need to figure out a method to perfectly
preserve the borders


00:00:40,560 --> 00:00:43,040
so that each new point at the borders in the split


00:00:43,340 --> 00:00:46,560
geometry will have a unique point match 
to be fused together,


00:00:46,560 --> 00:00:49,100
rather than purely relying on distance metrics.


00:00:50,400 --> 00:00:53,120
As you can see the borders are unshared edges.


00:00:53,680 --> 00:00:59,380
We will first experiment with groups to see what we would get if we created a group before subdivision.


00:01:00,880 --> 00:01:04,380
What happens to each attribute and group is up to each operator,


00:01:04,380 --> 00:01:07,120
and as such there is no standard behaviour.


00:01:07,760 --> 00:01:10,360
The operator in question might preserve them,


00:01:10,360 --> 00:01:13,120
initialize new elements to some arbitrary default value,


00:01:13,520 --> 00:01:16,159
Include or exclude them from existing groups, etc.


00:01:16,800 --> 00:01:20,720
Even with different attribute types, the behaviour might differ dramatically.


00:01:20,740 --> 00:01:22,820
There is no way to know but to test.


00:01:23,520 --> 00:01:27,820
So when we created a primitive group for the entire geometry before Subdivide,


00:01:27,820 --> 00:01:31,200
the group also includes the new primitives 
after subdivision,


00:01:31,600 --> 00:01:34,540
and as such it's not useful to us in any way,


00:01:34,540 --> 00:01:37,560
we might as well create the same group 
after the subdivision.


00:01:38,160 --> 00:01:42,380
Note that there is a cost to having additional groups before an operator in most cases


00:01:42,960 --> 00:01:46,020
so that's why we need to strive to have the
bare minimum if we can.


00:01:46,640 --> 00:01:50,840
As you can see, changing the group type to point group, the outcome is the same.


00:01:51,440 --> 00:01:55,300
But if you set the group type to an edge group, it doesn't include the new primitives.


00:01:55,840 --> 00:02:00,800
This will be useful for us to identify the old vs the new polygons. So we will keep it.


00:02:07,880 --> 00:02:12,000
I will convert this edge group into a primitive group called old prims.


00:02:27,360 --> 00:02:30,180
Now invert the old prims group as the new prims group.


00:02:44,239 --> 00:02:46,440
Create Group from Attribute Boundary SOP,


00:02:46,440 --> 00:02:49,540
to create an edge group for the borders of the new prims group.


00:02:55,440 --> 00:02:57,900
I will call this new edge border hires.


00:03:03,120 --> 00:03:07,840
We will temporarily use a Peak node to see the borders of the new vs the old polygons easier.


00:03:08,560 --> 00:03:11,839
But how can we get the borders of the matching polygons on the other side?


00:03:13,360 --> 00:03:17,839
We can't just get the borders of the old prims group as that might already have open edges.


00:03:24,319 --> 00:03:28,499
So instead we need to group all unshared edges before the subdivision.


00:03:37,440 --> 00:03:40,419
Then copy paste this to do the same after the subdivision.


00:03:46,640 --> 00:03:50,160
As you might have guessed, subtracting open_edges_before group,


00:03:50,160 --> 00:03:53,620
from open_edges_after group, will give us the new open edges


00:03:53,720 --> 00:03:55,380
created after the subdivision.


00:03:55,980 --> 00:03:58,260
I will call this, new open edges group.


00:04:20,880 --> 00:04:25,119
Now we can separate the edge borders of the new prims vs the old prims that match.


00:04:25,760 --> 00:04:30,320
I will call this, new open edges lowres group.
This group is very important,


00:04:30,800 --> 00:04:33,720
as we will use this edge group to split 
the edges in the middle,


00:04:34,560 --> 00:04:39,880
so we can have corresponding edge points on the borders that match the subdivided polygons perfectly.


00:04:40,560 --> 00:04:46,260
Edge Divide SOP does this for us, even though it's a very old SOP that can not be compiled yet.


00:04:59,760 --> 00:05:04,300
As you can see, we have new points on the edges of the new open edges lowres group.


00:05:05,200 --> 00:05:09,220
Now we have to figure out how to match these points perfectly on the other side.


00:05:10,240 --> 00:05:12,400
First we have to experiment with attributes.


00:05:13,040 --> 00:05:18,120
Your first intuition might be to create an integer attribute that holds the current point number.


00:05:18,800 --> 00:05:20,820
I will add a visualizer to see the values.


00:05:33,040 --> 00:05:36,260
It's hard to see these because of back facing geometry.


00:05:36,720 --> 00:05:40,020
I will clip the geometry so we can see 
the values more clearly.


00:05:57,440 --> 00:06:01,860
But after the edge divide, the edge points on both sides do not match at all.


00:06:02,480 --> 00:06:04,200
You can clearly see the difference


00:06:04,200 --> 00:06:07,920
where subdivide seems to assign the value of the next point to the new edge point,


00:06:08,320 --> 00:06:14,440
while edge divide seems to assign the value of the previous point to the new edge point. So this won't work.


00:06:16,560 --> 00:06:18,580
What if we store this value as a string?


00:06:24,080 --> 00:06:26,080
It's exactly the same.


00:06:32,479 --> 00:06:34,479
What about storing it as a float?


00:06:39,280 --> 00:06:42,780
Now everything matches perfectly and some with fractional values.


00:06:43,280 --> 00:06:48,680
As you might have noticed they are interpolating between attribute values by simple averaging.


00:06:49,360 --> 00:06:53,760
That tells us the integer attribute interpolation wasn't about the next or the previous point


00:06:53,760 --> 00:06:56,140
but rather subdivide was using ceil


00:06:56,240 --> 00:07:01,700
and edge divide was using floor after it averaged the integer attributes that resulted in fractions.


00:07:02,640 --> 00:07:08,000
Because of this, you can imagine point numbers could result in a lot of duplicate interpolated values.


00:07:08,520 --> 00:07:12,400
Think of the average of 0 and 20 vs 4 and 16,


00:07:12,640 --> 00:07:15,380
they would both result in the same value of 10.


00:07:15,380 --> 00:07:17,740
So we have to come up with better initial values.


00:07:18,480 --> 00:07:21,920
As we have seen in the last chapter, 
we will use hashing for this.


00:07:22,960 --> 00:07:26,500
We can use random_ihash and pass ptnum as the seed.


00:07:35,840 --> 00:07:40,600
As you can see, it's much harder to get a collision by averaging these values.


00:07:40,600 --> 00:07:43,539
We can use these values to fuse by IDs.


00:07:50,160 --> 00:07:52,500
But look at the new open edges lowres group,


00:07:52,900 --> 00:07:55,440
after edge divide, all the edges are lost.


00:07:56,879 --> 00:07:58,960
So we need to reconstruct this group


00:07:58,960 --> 00:08:02,760
as we want to limit the fuse operation to only the elements we need to fuse.


00:08:03,600 --> 00:08:06,740
We can use the same methods we used before to get these edges.


00:08:15,680 --> 00:08:18,819
But this time we need to convert it into a point group.


00:08:27,759 --> 00:08:31,420
Fusing by IDs requires an integer or a string attribute,


00:08:31,420 --> 00:08:35,260
so we need to convert our float hash into an integer attribute.


00:08:50,260 --> 00:08:53,400
Create a fuse node and use hash as the Match Attribute.


00:08:55,600 --> 00:08:59,519
Turn off the Snap Distance for now, and use the new open_edges group.


00:09:02,720 --> 00:09:05,540
As you can see, the borders are perfectly fused.


00:09:25,360 --> 00:09:29,500
Because I edited the cooking states, you might have thought this was super fast,


00:09:29,680 --> 00:09:34,160
but looking at the cook time of the last fuse, you can see it almost took a minute.


00:09:34,720 --> 00:09:36,720
This is extremely slow.


00:09:36,959 --> 00:09:38,880
How can we fix this?


00:09:38,880 --> 00:09:40,540
We can use a Snap Distance.


00:09:40,800 --> 00:09:45,040
0.001 should be enough as the points 
are actually on top of each other.


00:09:45,839 --> 00:09:50,639
Just turning this on, made this operation 150 times faster for this iteration.


00:09:52,080 --> 00:09:55,519
The performance gain is much higher at higher subdivision levels.


00:09:56,240 --> 00:10:00,080
But what's the point of fusing by an attribute if we are using distance metrics?


00:10:00,959 --> 00:10:03,680
The difference is we are still fusing by attribute,


00:10:03,680 --> 00:10:05,860
but using distance on top,


00:10:05,860 --> 00:10:08,440
not fusing points purely based on distance metrics,


00:10:08,880 --> 00:10:11,700
which as I mentioned before, would be imprecise


00:10:11,700 --> 00:10:15,760
and could fuse more points than necessary at higher subdivision levels,


00:10:16,720 --> 00:10:19,280
unless you reduced the Snap Distance iteratively,


00:10:19,760 --> 00:10:24,660
at which point you would also deal with floating point issues, and it will be extremely slow.


00:10:25,520 --> 00:10:27,460
Although I couldn't reproduce it here,


00:10:27,460 --> 00:10:29,900
but when using OpenSubdiv Bilinear,


00:10:29,900 --> 00:10:32,080
I was getting duplicated hash values.


00:10:33,040 --> 00:10:37,960
This wasn't because of the hash function or the average interpolation but something else.


00:10:37,960 --> 00:10:40,500
I was getting more than 2 of the same hash value.


00:10:41,120 --> 00:10:44,940
Even so, Snap Distance would help reduce this problem significantly,


00:10:45,040 --> 00:10:47,100
but not eliminate completely,


00:10:47,100 --> 00:10:51,420
as there is still a chance these duplicate values might end up next to each other,


00:10:51,420 --> 00:10:53,680
though very hard for this to happen.


00:10:54,080 --> 00:10:58,520
In this case, guarding against this potential problem won't cost us anything,


00:10:58,840 --> 00:11:01,180
and what's better than a strong hash value?


00:11:01,580 --> 00:11:03,400
It's 2 strong hash values :)


00:11:03,600 --> 00:11:06,920
So instead of computing a hash value from the current point number,


00:11:07,040 --> 00:11:11,060
we will compute another hash value using the point number of its first neighbour.


00:11:18,220 --> 00:11:23,640
Accordingly we have to update the hash attribute to be a string attribute that holds both of these hashes.


00:11:23,640 --> 00:11:25,300
I will call it hash pair.


00:11:46,560 --> 00:11:49,260
As you can see, the values are still matching,


00:11:49,260 --> 00:11:53,619
but in this case, we reduced the chance 
of collisions to practically 0.


00:12:04,160 --> 00:12:06,180
Now everything works perfectly.


00:12:06,180 --> 00:12:08,840
We have the speed of OpenSubdiv algorithm


00:12:08,840 --> 00:12:12,280
but we also preserved the new primitive borders without any holes.


00:12:22,560 --> 00:12:24,220
We are still not done though.


00:12:24,220 --> 00:12:26,820
Take a close look at the shading of the area


00:12:26,820 --> 00:12:30,060
we adaptively subdivided. It looks very faceted.


00:12:31,100 --> 00:12:33,760
Is it a normals issue? Not completely.


00:12:34,240 --> 00:12:38,980
This is because we are using OpenSubdiv Bilinear which subdivides the geometry


00:12:39,000 --> 00:12:42,800
but doesn't smooth it out the way you expect from subdivision surfaces.


00:12:45,380 --> 00:12:50,040
To obtain a smooth surface, you have to use OpenSubdiv Catmull Clark algorithm.


00:12:57,200 --> 00:13:00,600
I chose OpenSubdiv Bilinear first for a reason though.


00:13:00,600 --> 00:13:04,099
Let's switch the algorithm to OpenSubdiv Catmull Clark to see the issue.


00:13:08,000 --> 00:13:11,000
As you can see, we now have a smoother surface.


00:13:11,200 --> 00:13:17,680
but the borders are also smooth, and as such they are not overlapping anymore, unlike OpenSubdiv Bilinear,


00:13:18,080 --> 00:13:20,720
which kept point positions where they were.


00:13:21,840 --> 00:13:23,240
Now you might be thinking:


00:13:23,240 --> 00:13:29,120
can't we still just fuse the border points, as we are fusing by IDs, and not by pure distance metrics?


00:13:30,079 --> 00:13:32,079
This is where the new obstacle lies.


00:13:32,320 --> 00:13:34,200
Even if we remove the Distance metric


00:13:34,200 --> 00:13:38,719
so it could fuse all points with the same hash pair value regardless of where they are,


00:13:39,040 --> 00:13:40,520
this still wouldn't work.


00:13:40,960 --> 00:13:42,960
and as you can see, it doesn't.


00:13:43,520 --> 00:13:45,520
Take a look at the unfused points.


00:13:46,300 --> 00:13:49,020
Their hash payer values are completely different.


00:13:50,000 --> 00:13:50,960
Why is that?


00:13:51,600 --> 00:13:53,600
The answer is rather simple.


00:13:54,160 --> 00:13:59,460
By changing the subdivision algorithm from OpenSubdiv Bilinear to OpenSubdiv Catmull Clark,


00:13:59,520 --> 00:14:01,820
you are not just affecting the point positions,


00:14:01,820 --> 00:14:05,500
you are also changing how all attribute values are interpolated,


00:14:06,480 --> 00:14:10,740
and as such we not long have matching hash pair values everywhere like before,


00:14:11,120 --> 00:14:13,300
which relied on bilinear interpolation.


00:14:13,920 --> 00:14:15,200
How can we fix this?


00:14:15,880 --> 00:14:18,220
One approach you might think off right off the bat


00:14:18,220 --> 00:14:20,500
is to use high crease weights on the borders


00:14:20,600 --> 00:14:23,900
to make them rigid like before. So let's try that first.


00:14:24,560 --> 00:14:29,340
So create Group From Attribute Boundary SOP to group boundary edges of near prims.


00:14:29,540 --> 00:14:32,260
There is another important thing you should pay attention to here.


00:14:32,740 --> 00:14:38,020
The output group type from this node, is not just a simple edge to target group type conversion,


00:14:38,020 --> 00:14:41,620
but rather it affects how attribute boundaries are considered.


00:14:49,520 --> 00:14:53,960
So as you can see, edge vs points, the result is dramatically different.


00:14:54,640 --> 00:14:59,740
So we have to use edges, and then convert this edge group to a point group afterwards.


00:15:04,160 --> 00:15:09,700
But even this won't be enough as is. 
Let's color near prims first to demonstrate it.


00:15:16,000 --> 00:15:18,520
We have to fiddle with radius to get this issue.


00:15:23,840 --> 00:15:27,160
As you can see, here the Group From Attribute Boundary node


00:15:27,160 --> 00:15:30,500
didn't include these edges 
and so we don't have them as points.


00:15:34,480 --> 00:15:38,080
To fix this, we also have to use the connectivity attribute in this node.


00:15:39,680 --> 00:15:44,520
So for each primitive in near_prim group, create an integer attribute, called is near prim.


00:15:49,600 --> 00:15:52,020
Select this attribute for the connectivity attribute.


00:15:55,200 --> 00:15:58,880
As you can see, we now have the border edges perfectly.


00:15:58,880 --> 00:16:01,760
These are very subtle but important differences to know about.


00:16:05,280 --> 00:16:07,840
We can now set the radius back to the original value.


00:16:09,280 --> 00:16:13,300
Now that we have this border group, we can use it to set crease weight values.


00:16:25,120 --> 00:16:27,360
Note that crease weight is a vertex attribute.


00:16:30,560 --> 00:16:35,720
Setting crease weight to 1 will make these vertices rigid just like OpenSubdiv Bilinear.


00:16:40,720 --> 00:16:43,640
As you can see this doesn't work as we expected.


00:16:44,160 --> 00:16:47,460
Some points end up being warped due to interpolation.


00:16:48,000 --> 00:16:52,100
When we turn on Override Crease Weight Attribute which is set to 0 by default,


00:16:52,240 --> 00:16:55,660
we are effectively turning off any crease weight values that we have.


00:16:56,320 --> 00:17:00,360
Even though we have control over the crease weight values on the border vertices,


00:17:00,640 --> 00:17:07,260
we don't have a way to set the intermediate vertices being created, that will be interpolated by the Subdivide SOP.


00:17:08,960 --> 00:17:13,040
Those values you see between 0 and 1, we don't have a way to set these,


00:17:13,340 --> 00:17:16,620
as some of these points don't exist before subdivide.


00:17:23,680 --> 00:17:25,680
So we cannot use crease weights.


00:17:26,080 --> 00:17:27,500
What can we use then?


00:17:28,040 --> 00:17:33,220
What we can do is to use another subdivide operation using OpenSubdiv Bilinear algorithm


00:17:33,360 --> 00:17:36,560
and copy the border values from this secondary subdivision.


00:17:37,920 --> 00:17:43,060
We have to duplicate the same group nodes on the new branch up until we get the new borders.


00:17:43,480 --> 00:17:46,360
But we have to convert this edge group into a point group


00:17:46,540 --> 00:17:49,620
as Attribute Copy does not support edge groups.


00:18:03,840 --> 00:18:05,840
Create an attribute copy node


00:18:13,040 --> 00:18:17,200
and set the same groups for both source and destination and copy P.


00:18:23,520 --> 00:18:26,100
As you can see, we now have rigid borders.


00:18:41,520 --> 00:18:46,780
But when we look at the final result, the edges are still not fused. Why is that?


00:18:47,920 --> 00:18:51,520
It's because we are fusing by hash pair values, not by P,


00:18:52,240 --> 00:18:56,260
and so we also need to copy the hash values so they match too, not only P.


00:19:01,120 --> 00:19:04,180
After we copy the hash values from OpenSubdiv Bilinear,


00:19:04,180 --> 00:19:08,760
we have perfectly matching borders and as such the fusing works perfectly.


00:19:09,280 --> 00:19:12,840
So now we have the performance of OpenSubdiv subdivision surfaces,


00:19:12,840 --> 00:19:17,360
completely preserved boundaries on the subdivided regions, and a smooth subdivision.


00:19:18,180 --> 00:19:21,540
It seems like we have created the perfect adaptive subdivision operator.


00:19:22,400 --> 00:19:23,340
But is it?


00:19:24,140 --> 00:19:25,280
In the next chapter,


00:19:25,280 --> 00:19:29,920
we will see how much we can push the performance of this operator to extreme levels


00:19:30,080 --> 00:19:34,140
and use all kinds of tricks in the book to squeeze out every drop of performance


00:19:34,140 --> 00:19:36,560
that has a practical cost/benefit ratio.


00:19:37,040 --> 00:19:38,560
This is only the beginning.


00:19:39,840 --> 00:19:42,660
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

58 - Half-Edges

----------------------------------------------------------------------------------------------------



00:00:10,780 --> 00:00:14,540
In VEX, we don't have edges as an actual construct.


00:00:15,040 --> 00:00:21,080
Even outside VEX, the concept of edges in
Houdini, is derived by point references, and


00:00:21,420 --> 00:00:24,160
as such they are second class types.


00:00:24,840 --> 00:00:29,970
In other 3d apps, just like you have unique
point and primitive numbers, you can also


00:00:29,970 --> 00:00:31,780
have actual edge numbers,


00:00:31,780 --> 00:00:33,680
such as edge number 5.


00:00:34,220 --> 00:00:37,300
But that requires a maintenance of these structures


00:00:37,300 --> 00:00:39,100
any time the topology changes,


00:00:40,100 --> 00:00:43,460
Houdini rather defines edges with the use of points,


00:00:43,460 --> 00:00:46,300
i.e. instead of edge 5,


00:00:46,300 --> 00:00:49,720
it will refer to this edge by referencing its points.


00:00:50,080 --> 00:00:55,620
For example p4-5, meaning, the edge whose
points are 4 and 5.


00:00:56,540 --> 00:00:59,760
And it's this very reason, we don't have edge attributes


00:00:59,760 --> 00:01:03,000
nor have them as first class types like points,


00:01:03,000 --> 00:01:06,720
which could come in very handy
to model things like graph theory.


00:01:07,320 --> 00:01:11,120
Currently, we have to use primitives to model
these kinds of edges.


00:01:12,360 --> 00:01:18,620
So the only way to have programmatic access
to edges in Houdini, outside HDK and C++ Wrangle,


00:01:18,620 --> 00:01:21,660
i.e. inline C++, is Python.


00:01:22,600 --> 00:01:28,420
Unfortunately, due to their performance,
 I would advise against using edges in Python


00:01:28,420 --> 00:01:30,600
except for the most trivial things.


00:01:31,100 --> 00:01:33,900
Performance wise, it just doesn't scale.


00:01:34,860 --> 00:01:39,820
It would be extremely limiting if this was
all we had for such a very modern and powerful


00:01:39,820 --> 00:01:41,380
3d app like Houdini.


00:01:43,280 --> 00:01:45,680
And that's where half edges save the day.


00:01:46,720 --> 00:01:51,280
Actually the help card for half edges is remarkably
clear and concise.


00:01:52,160 --> 00:01:57,980
"VEX has functions that let you treat edges
as unshared per-face "half-edges"."


00:01:59,080 --> 00:02:04,000
This definition alone tells you almost everything
you need to know about what half edges are.


00:02:04,840 --> 00:02:10,360
[3rd pic] From this picture you can clearly
see how half edges are different than edges.


00:02:10,900 --> 00:02:12,080
They have a direction.


00:02:12,860 --> 00:02:17,720
They have previous and next half edges, as
well as whether it's the primary half edge.


00:02:18,880 --> 00:02:21,680
This part about equality is very important.


00:02:22,260 --> 00:02:27,500
"The source (and destination) vertex of a
half-edge uniquely identifies it,


00:02:27,900 --> 00:02:33,820
meaning that there can be at most one half-edge 
with a given vertex as the source.


00:02:34,940 --> 00:02:38,720
This is because a vertex can belong to only
one primitive.


00:02:39,220 --> 00:02:45,040
However, because several vertices can be wired
to the same point, half-edges can have shared


00:02:45,040 --> 00:02:46,940
source and destination points.


00:02:47,580 --> 00:02:53,560
Another way of thinking about equivalence
is that two half-edges are equivalent if their


00:02:53,560 --> 00:02:56,560
vertices are shared by the same two points."


00:02:57,320 --> 00:03:02,540
There are all sorts of functions to efficiently
traverse a geometry using half edges.


00:03:03,520 --> 00:03:08,799
Going through all of these functions, especially
showing actual production examples, would


00:03:08,800 --> 00:03:10,669
be a full length course on its own.


00:03:11,680 --> 00:03:16,559
Needless to say I wrote a dozen VEX based
tools that utilize half edges, that were orders


00:03:16,560 --> 00:03:19,740
of magnitude faster than the equivalents in Houdini.


00:03:20,740 --> 00:03:28,260
For our needs, we will use the pointedge and
hedge_equivcount functions to select unshared edges.


00:03:28,900 --> 00:03:32,660
pointedge will return a half-edge with the
given endpoints.


00:03:33,380 --> 00:03:38,760
hedge_equivcount will return the number of
half-edges equivalent to a given half-edge.


00:03:39,440 --> 00:03:45,220
Using point neighbours and point edge functions,
we will get the first available half edge


00:03:45,220 --> 00:03:50,680
from each pair of points, and then use the
hedge_equivcount function to get the number


00:03:50,680 --> 00:03:53,960
of half-edges equivalent to the given half-edge.


00:03:54,780 --> 00:03:57,959
If it's one, that means it's an unshared edge.


00:03:58,320 --> 00:04:00,720
We will implement this in the next chapter.


00:04:01,640 --> 00:04:04,740
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

59 - Adaptive Subdivision - Aggressive Performance Optimizations - Eliminating Groups

----------------------------------------------------------------------------------------------------



00:00:13,660 --> 00:00:18,420
What we have created so far as a tool, is
perfectly usable in a production environment.


00:00:19,020 --> 00:00:23,560
By no means, you have to squeeze out every
single drop of performance out of everything


00:00:23,560 --> 00:00:28,290
you have to use. Quite the opposite
actually. But if you are building complex


00:00:28,290 --> 00:00:33,690
setups, particularly solvers where every bit
of performance is critical, you can at least


00:00:33,690 --> 00:00:36,340
devote some time to speed up the slowest parts.


00:00:36,960 --> 00:00:42,360
Performance just like many things in life
has a point where you start to see diminishing returns.


00:00:43,220 --> 00:00:47,860
So you have to do a basic cost benefit analysis
for any optimization you might wanna do.


00:00:48,160 --> 00:00:53,240
For example, one big potential gain would be
to compile the entire for loop network,


00:00:53,240 --> 00:00:56,720
which could give us considerable performance improvement in theory


00:00:57,200 --> 00:00:59,660
but because we are using Edge Divide,


00:00:59,660 --> 00:01:03,280
which is a non-compilable SOP, it prevents us from doing so.


00:01:03,900 --> 00:01:06,400
In fact, it's the only operator


00:01:06,400 --> 00:01:07,840
that's not compilable in this setup.


00:01:08,440 --> 00:01:13,440
AFAIK, there is no alternative we can use that's compilable to do the same,


00:01:13,440 --> 00:01:15,700
unless we create our own version.


00:01:16,760 --> 00:01:21,040
It's perfectly doable in VEX, 
exceptit's quite involved.


00:01:22,000 --> 00:01:27,120
You can see here, I actually asked how to
do this, and Jeff Lait replied though he posted


00:01:27,120 --> 00:01:30,060
a solution for splitting a face using its
points,


00:01:30,660 --> 00:01:32,520
but when I clarified that I am looking


00:01:32,520 --> 00:01:37,710
to split faces using its edges, he also said
it's very involved to achieve it, and it would


00:01:37,710 --> 00:01:39,360
possible require 4 wrangles.


00:01:39,780 --> 00:01:42,860
A complicated
part is to include a tracking attribute in


00:01:42,860 --> 00:01:47,280
the face/edge points to make sure you can
find them again in the successive steps.


00:01:48,420 --> 00:01:52,920
That's why I wanted to dedicate a chapter
for advanced optimization techniques that's


00:01:52,929 --> 00:01:58,000
not only applicable to adaptive subdivision,
but many other common problems you might encounter


00:01:58,000 --> 00:02:00,840
in a variety of situations in a production
environment.


00:02:01,820 --> 00:02:05,460
So these skills should help you to use the same techniques for different problems,


00:02:05,460 --> 00:02:08,240
but more importantly to increase your technical


00:02:08,240 --> 00:02:11,120
capacity to solve different problems on your own.


00:02:11,800 --> 00:02:16,620
That's also why I included common choices
you might pick that ends up being suboptimal.


00:02:17,320 --> 00:02:22,720
I think these kinds of attempts and mistakes
are very important to see, as a natural progression


00:02:22,720 --> 00:02:26,280
rather than a straight line from 0 to 
the perfect final result.


00:02:27,180 --> 00:02:29,260
And with that said, let's dive in.


00:02:33,060 --> 00:02:38,160
First duplicate the subnet, so we can compare
the optimized version to the original version.


00:02:39,000 --> 00:02:43,100
One of the most important things in optimization before you start modifying anything


00:02:43,100 --> 00:02:44,920
is performance profiling.


00:02:45,400 --> 00:02:52,840
You want to see the slowest parts,
and what you can fix easily, etc, rather than


00:02:52,840 --> 00:02:54,260
blind optimization.


00:02:55,240 --> 00:03:01,600
As you can see, level 8 subdivision took about
41 seconds, where fuse node is taking more


00:03:01,600 --> 00:03:03,580
than 20 percent of the entire operation.


00:03:03,940 --> 00:03:06,600
We definitely need to find an alternative to this.


00:03:07,420 --> 00:03:10,700
You can also see the subdivision nodes are
taking quite a bit of time.


00:03:10,700 --> 00:03:14,560
We will do a test by deleting all the groups except near_prims group


00:03:14,680 --> 00:03:16,740
and see if it impacts the cook times.


00:03:37,380 --> 00:03:40,960
Just doing this, we doubled up the performance
of subdivide node.


00:03:41,660 --> 00:03:47,580
Creating edge groups are unimaginably
costly, and preserving them is even more costly.


00:03:47,920 --> 00:03:49,460
We need to get rid of these.


00:03:49,940 --> 00:03:50,720
But how?


00:03:51,380 --> 00:03:54,400
We use these to isolate the new subdivided primitives,


00:03:54,720 --> 00:03:59,400
and using that, we find its border edges and
from this edge group we find the matching


00:03:59,400 --> 00:04:02,340
edge border surrounding the subdivided primitives.


00:04:03,540 --> 00:04:06,980
We can try creating an ad hoc group using
the depth attribute.


00:04:19,000 --> 00:04:20,580
The current depth is 4.


00:04:26,320 --> 00:04:28,240
The current iteration is 3.


00:04:28,520 --> 00:04:31,020
So we need to add 1 to the current iteration.


00:04:49,260 --> 00:04:51,380
You see, we got the exact same border edges.


00:04:54,600 --> 00:04:56,800
So we can bypass these group nodes.


00:05:03,500 --> 00:05:06,040
Make sure to bypass them on the other branch also.


00:05:13,520 --> 00:05:16,680
We gained a significant a mount of
performance from this.






----------------------------------------------------------------------------------------------------

60 - Adaptive Subdivision - Aggressive Performance Optimizations - Custom Fusing In VEX

----------------------------------------------------------------------------------------------------



00:00:05,120 --> 00:00:08,480
Fuse is taking more than 7 seconds for the
last iteration.


00:00:08,920 --> 00:00:11,280
It's also one of the hardest nodes to replace.


00:00:11,700 --> 00:00:13,040
Well, not anymore.


00:00:13,620 --> 00:00:16,240
We will implement custom fusing using VEX.


00:00:16,980 --> 00:00:19,960
Let's first check to see if the result is still correct.


00:00:24,040 --> 00:00:28,520
Create a wrangle node to match points using
the hash pair values manually.


00:00:31,940 --> 00:00:34,920
Make sure to limit the group to
the new open_edges group.


00:00:36,800 --> 00:00:41,620
First declare a max distance parameter we
will use to limit the matches by distance,


00:00:41,680 --> 00:00:44,260
similar to what we are doing using the fuse SOP.


00:00:44,980 --> 00:00:47,220
Take the square of it for performance.


00:00:58,940 --> 00:01:04,140
We only match if the point index we find is
different than the index of the current point.


00:01:04,140 --> 00:01:05,800
We don't want to match ourselves.


00:01:11,600 --> 00:01:15,440
Compare the squared distance between
the current point and the matched point


00:01:15,440 --> 00:01:17,520
to the squared max distance parameter.


00:01:19,240 --> 00:01:24,240
If it's equal or less, we add it to our fuse
group and store the index of the match


00:01:24,240 --> 00:01:26,500
in an attribute called targetpt.


00:01:48,200 --> 00:01:54,480
You can see, targetpt is 0 for half the points,
so we will only match one side to another


00:01:54,760 --> 00:01:58,400
rather than each point in the hash value pair
matching each other.


00:02:01,940 --> 00:02:04,340
Now create Rewire Vertices SOP.


00:02:04,540 --> 00:02:09,800
This is a very powerful low-level node that
can help us build all kinds of modeling tools


00:02:10,200 --> 00:02:16,340
like bridge edges SOP, collapse edges SOP,
or in our case, our very own fuse SOP.


00:02:17,420 --> 00:02:20,060
Set group to the fuse group we just created


00:02:22,660 --> 00:02:25,660
and set target points attribute to targetpt.


00:02:26,500 --> 00:02:30,900
Now look at the final point count on the fuse
node and the rewire vertices node.


00:02:34,160 --> 00:02:40,300
As you can see, they have the exact same point
count, meaning we fused the same points perfectly.


00:02:40,960 --> 00:02:42,300
And look at that speed.


00:02:42,880 --> 00:02:46,760
7.2 seconds vs 158 millisecond.


00:02:47,100 --> 00:02:48,800
This is extremely fast.


00:02:49,200 --> 00:02:51,620
We also have to take the wrangle node into account.


00:03:08,660 --> 00:03:12,280
So we optimized the fuse operation by a factor of 24,


00:03:12,860 --> 00:03:14,780
and the higher subdivision levels you went,


00:03:15,060 --> 00:03:16,640
the bigger the gains would be.


00:03:22,580 --> 00:03:25,260
We fused almost 100 thousand points.


00:03:38,500 --> 00:03:42,240
We just doubled up th e performance of the
entire adaptive subdivision node.


00:03:42,240 --> 00:03:46,060
This is an incredible performance gain for
10 minutes of work.






----------------------------------------------------------------------------------------------------

61 - Adaptive Subdivision - Aggressive Performance Optimizations - Recreating Proximity Structures In VEX

----------------------------------------------------------------------------------------------------



00:00:14,160 --> 00:00:16,319
Now you can see that the 
non-primary subdivision


00:00:16,320 --> 00:00:18,740
we are performing for the proximity lookups,


00:00:18,740 --> 00:00:21,920
is taking more time than the actual subdivision


00:00:21,920 --> 00:00:22,900
we are performing.


00:00:23,360 --> 00:00:24,860
We need to improve this.


00:00:25,120 --> 00:00:28,180
We can try the same technique of
deleting unnecessary groups.


00:00:57,040 --> 00:00:59,500
We don't see much of a performance 
gain here.


00:00:59,960 --> 00:01:02,560
We can also try it by deleting 
unnecessary attributes.


00:01:07,600 --> 00:01:10,580
We have to make sure to keep 
the radius and depth attributes.


00:01:29,020 --> 00:01:32,300
33 percent performance improvement
for the subdivide node.


00:01:48,400 --> 00:01:50,660
We doubled up the speed of the 
resample node.


00:01:51,600 --> 00:01:53,340
We can also delete the depth attribute


00:01:53,340 --> 00:01:56,020
after we are done with it, 
to get a bit more speed.


00:02:09,200 --> 00:02:11,420
We can do a lot better than this though.


00:02:11,420 --> 00:02:14,940
Instead of using all these nodes to 
create the point structures,


00:02:14,940 --> 00:02:17,240
we can create everything using VEX.


00:02:19,040 --> 00:02:21,140
Let's see the concept of the algorithm first.


00:02:24,480 --> 00:02:26,739
We will also modify the point 
structure slightly.


00:02:36,239 --> 00:02:38,239
First we create the center point.


00:02:42,560 --> 00:02:45,940
Then we store the primitive points, 
we don't need to recreate them.


00:02:53,760 --> 00:02:57,260
Then we will create edge points and 
store these separately.


00:03:06,240 --> 00:03:09,520
We will use all of these points to 
create non-physical edges,


00:03:09,520 --> 00:03:11,700
to help create the new points easier.


00:03:12,480 --> 00:03:14,760
By storing these edges in another array,


00:03:14,760 --> 00:03:18,000
we can easily create any number 
of points between them,


00:03:18,000 --> 00:03:20,240
the same way the resample SOP does.


00:03:27,600 --> 00:03:29,600
Let's implement it in Houdini now.


00:03:31,900 --> 00:03:36,080
Copy the adaptive subdivide
subnet and paste it inside the grid object.


00:03:36,080 --> 00:03:38,420
It's easier to see the result on a grid.


00:03:56,319 --> 00:03:57,720
Create a primitive SOP


00:03:57,720 --> 00:03:59,740
to resize the primitives temporarily


00:03:59,740 --> 00:04:02,018
so we can see the overlapping points easier.


00:04:16,560 --> 00:04:19,440
Create a primitive wrangle to 
implement the new logic.


00:04:22,400 --> 00:04:25,380
First create an array for the 
new points to be created.


00:04:30,800 --> 00:04:32,880
Add primitive center to this array.


00:04:35,040 --> 00:04:36,720
Loop over the pos array


00:04:36,720 --> 00:04:39,940
and create a new point for 
each element inside this array.


00:04:44,560 --> 00:04:48,320
Set primid on the new points to
primitive primid attribute value.


00:04:55,820 --> 00:04:59,119
Attribute promote primid from 
primitives to points.


00:05:01,440 --> 00:05:04,320
Create an add SOP to 
delete everything but the points.


00:05:05,360 --> 00:05:09,780
As you can see, we have the primitive
 points and primitive center point.


00:05:10,960 --> 00:05:15,220
Now we will store primitive points in 
another array to use them to create edges.


00:05:55,199 --> 00:05:57,060
Note that if the primitive is closed,


00:05:57,060 --> 00:05:59,080
we also want to append the first point


00:05:59,080 --> 00:06:00,499
to the end of the same array.


00:06:18,960 --> 00:06:20,960
We will create the edge points now.


00:06:32,479 --> 00:06:35,480
We will loop over the primpts array in pairs


00:06:35,480 --> 00:06:38,899
so we want to set the loop count 
to array length minus 1.


00:06:52,080 --> 00:06:54,400
We compute the center of each point pair,


00:06:54,400 --> 00:06:58,200
that is the current point and the 
next point in the primpts array.


00:06:58,880 --> 00:07:00,680
This will be the edge points.


00:07:08,640 --> 00:07:12,100
Add this to the pos array so they
 will show up as new points.


00:07:12,540 --> 00:07:15,300
But also add these to the edge pts array.


00:07:17,440 --> 00:07:20,319
As you can see, we now have
 the edge points created.


00:07:21,199 --> 00:07:24,680
Now we want to create edges 
from the current point to the edge point,


00:07:24,680 --> 00:07:29,460
and another edge from the edge 
point to the next point at each iteration.


00:07:35,140 --> 00:07:37,140
Store these in edges array.


00:07:57,360 --> 00:07:59,100
Make sure to append the first point


00:07:59,100 --> 00:08:00,680
to the end of the edges array


00:08:00,680 --> 00:08:03,080
if the primitive is closed, like before.


00:08:07,520 --> 00:08:10,100
Loop over the edges array, but this time


00:08:10,480 --> 00:08:12,980
the loop count will be half of the array length.


00:08:13,440 --> 00:08:15,760
Because we will loop over them 2 by 2.


00:08:24,640 --> 00:08:27,820
Compute the center of each edge
similar to the previous code,


00:08:38,320 --> 00:08:41,380
but this time make sure to double
 up the loop indices.


00:08:54,800 --> 00:08:58,940
Looks like we made a mistake here. 
Change the last append to p instead.


00:09:03,840 --> 00:09:05,100
Now as you can see,


00:09:05,100 --> 00:09:08,580
we have created new points 
between each edge successfully.


00:09:10,880 --> 00:09:12,720
Now we want to create new edges


00:09:12,720 --> 00:09:16,260
that go from the edge points to 
the primitive center point.


00:09:31,680 --> 00:09:34,020
As you can see, once we create edges,


00:09:34,020 --> 00:09:35,839
it's very easy to create new points.


00:09:37,440 --> 00:09:40,180
Now we can create some diagonal 
edges separately


00:09:40,180 --> 00:09:43,280
as we will create 3 points between these, 
rather than 1.


00:09:43,760 --> 00:09:47,460
That means from the current primitive 
point towards the primitive center.


00:10:24,480 --> 00:10:31,140
And so, 3 new points at 25, 50 and 75
 percent along the diagonal edges.


00:10:40,079 --> 00:10:42,579
As you can see, the new points 
show up correctly.


00:10:49,120 --> 00:10:51,560
Finally we will use the edge 
points to create


00:10:51,560 --> 00:10:54,580
2 more points between them. 
Not 3, like diagonals,


00:10:54,760 --> 00:10:55,980
as that would overlap


00:10:55,980 --> 00:10:58,460
with one of the points on 
the diagonal edges.


00:11:11,360 --> 00:11:12,300
So that means


00:11:12,300 --> 00:11:16,820
two new points at 25 and 75 percent 
along the edge points.


00:11:36,880 --> 00:11:40,659
We can visualize the primid 
on these points using a color node.


00:11:49,840 --> 00:11:50,960
It looks correct.


00:12:00,880 --> 00:12:04,020
Now copy the new subnet, 
back onto the character head model.


00:12:15,600 --> 00:12:18,160
We will profile the old method 
and the new method.


00:12:55,360 --> 00:12:57,380
We got about 10% speed up.






----------------------------------------------------------------------------------------------------

62 - Adaptive Subdivision - Aggressive Performance Optimizations - Getting Unshared Edges In VEX

----------------------------------------------------------------------------------------------------



00:00:03,985 --> 00:00:06,855
We are also grouping unshared 
edges quite a lot,


00:00:06,995 --> 00:00:09,535
so we will implement this using VEX.


00:00:10,665 --> 00:00:12,225
Create a wrangle node.


00:00:17,115 --> 00:00:19,545
Declare a parameter for the group name.


00:00:23,465 --> 00:00:26,145
Get the point neighbours of the current point.


00:00:35,045 --> 00:00:37,995
At first thought, you might check 
if the neighbour point number is


00:00:38,000 --> 00:00:41,660
less than the current point number to 
avoid checking edge points both ways.


00:00:42,320 --> 00:00:44,980
We will find the half edge between 
these 2 points.


00:00:49,080 --> 00:00:52,200
Then get the number of half-edges 
equivalent to this half-edge.


00:00:52,680 --> 00:00:55,935
Equivalent half-edges may be 
oppositely oriented,


00:00:55,940 --> 00:01:00,980
i.e. the source of one can be the 
destination of the other and vice versa.


00:01:01,320 --> 00:01:05,180
As you can see from the help card, 
1 means boundary edge.


00:01:05,520 --> 00:01:09,000
And as such if the count is 1, 
we group this edge.


00:01:39,280 --> 00:01:42,740
And as you can see, some of the 
boundary edges are missing.


00:01:43,055 --> 00:01:46,080
This is normal because we are 
only looking up the half edge


00:01:46,080 --> 00:01:49,335
if the neighbour point number is less
 than the current point number.


00:01:49,460 --> 00:01:51,860
Note that half edges are not like edges,


00:01:51,860 --> 00:01:54,440
so we can't do this kind of optimizations.


00:01:54,440 --> 00:01:57,520
They may be oriented one way or 
the other way,


00:01:57,520 --> 00:02:00,695
i.e. opposite of the if statement 
we are using.


00:02:00,695 --> 00:02:03,225
So we have to check both of t
hese regardless.


00:02:04,840 --> 00:02:06,960
Once we remove the if comparison,


00:02:06,960 --> 00:02:09,860
we get the same number of edges 
as the group node.


00:02:22,920 --> 00:02:25,400
But you have to take note of another issue.


00:02:25,400 --> 00:02:28,940
If there are no boundary edges, 
then you will have no group,


00:02:29,080 --> 00:02:32,100
and unfortunately some group nodes 
error out,


00:02:32,180 --> 00:02:34,480
if the input group is non-existent.


00:02:35,020 --> 00:02:38,360
So create a detail wrangle that will 
initialize the empty group.


00:02:58,600 --> 00:03:01,960
As you can see, we now have an 
edge group in any case.


00:03:02,740 --> 00:03:06,720
So we can now replace the group 
node that was grouping unshared edges


00:03:06,760 --> 00:03:09,520
everywhere in the network 
with our wrangle nodes.


00:04:04,880 --> 00:04:07,480
The performance looks similar 
to the previous version,


00:04:07,700 --> 00:04:09,340
even though you can see that our code


00:04:09,340 --> 00:04:12,300
that groups unshared edges is 
heavily multi-threaded.


00:04:17,480 --> 00:04:20,200
We can also optimize the 
second subdivide node.


00:04:20,440 --> 00:04:22,980
We don't need to have all the 
groups on that branch.


00:04:23,140 --> 00:04:27,880
So delete all the groups except near 
prims and open_edges_before groups.


00:04:38,280 --> 00:04:40,860
We got a bit of performance boost there.


00:04:40,860 --> 00:04:44,440
We are not done yet, there are a lot 
more we can do.


00:04:50,375 --> 00:04:53,645
The VEX code we wrote to group 
unshared edges,


00:04:53,645 --> 00:04:56,900
we can also use it to group the 
boundary edges of the new primitives


00:04:56,900 --> 00:04:58,340
after subdivision.


00:05:40,120 --> 00:05:43,200
As you can see we got the same 
number of edges.


00:05:49,875 --> 00:05:52,655
We just made this operation 
5 times faster


00:05:52,655 --> 00:05:56,015
just on this iteration alone. 
This will help us a lot.


00:06:14,915 --> 00:06:17,975
It seems to show about 10% 
improvement overall,


00:06:17,985 --> 00:06:20,175
not bad for a quick switcheroo.


00:06:20,665 --> 00:06:23,575
Also in the secondary subdivide
node, turn on


00:06:23,580 --> 00:06:26,640
Override Crease Weight Attribute 
and set it to 1.


00:06:27,220 --> 00:06:30,940
Because we are only using the 
secondary subdivide node for the borders,


00:06:30,940 --> 00:06:33,515
we want to make sure that it 
is always rigid.


00:06:33,515 --> 00:06:35,775
And even if there is a crease 
weight attribute,


00:06:35,780 --> 00:06:38,920
we want to discard it in the 
secondary subdivide node,


00:06:38,920 --> 00:06:41,980
so that it doesn't interfere with 
the border interpolation.






----------------------------------------------------------------------------------------------------

63 - Adaptive Subdivision - Aggressive Performance Optimizations - Final Optimizations

----------------------------------------------------------------------------------------------------



00:00:04,700 --> 00:00:07,200
There is another major change 
we have to do.


00:00:07,720 --> 00:00:11,780
As you can see, in match pts 
by hash pair wrangle node,


00:00:11,780 --> 00:00:13,600
we are using the entire new


00:00:13,600 --> 00:00:14,400
open edges group,


00:00:14,400 --> 00:00:17,280
whereas we only need to
match one side with another.


00:00:17,980 --> 00:00:19,240
Using only one side,


00:00:19,240 --> 00:00:21,700
will reduce the performed
calculations by half.


00:00:22,380 --> 00:00:24,640
Instead of relying on code to do this,


00:00:24,640 --> 00:00:27,040
we have to be explicit using groups.


00:00:28,160 --> 00:00:30,800
We can use the new open edges low res group.


00:00:30,800 --> 00:00:33,240
Just duplicate the group combine node that


00:00:33,240 --> 00:00:37,220
creates the new open edges low res group after group combine 5.


00:00:46,600 --> 00:00:49,580
Then convert this edge group into a point
group.


00:01:13,220 --> 00:01:14,700
We will replace this group


00:01:14,700 --> 00:01:16,880
with new open edges
low res group.


00:01:17,500 --> 00:01:18,540
But before that


00:01:18,540 --> 00:01:20,800
we will also change the match
by ID logic.


00:01:21,480 --> 00:01:23,140
We want to find matches


00:01:23,140 --> 00:01:26,360
originating from the
new open edges low res group.


00:01:26,360 --> 00:01:28,520
That means, these points should be fused


00:01:28,520 --> 00:01:31,280
to their matches, 
no matter where they are.


00:01:31,900 --> 00:01:33,700
Because the subdivided polygons


00:01:33,700 --> 00:01:34,980
dictate the new shape,


00:01:34,980 --> 00:01:37,580
the unsubdivided polygons should follow.


00:01:38,540 --> 00:01:41,000
That's why we have to remove 
the distance checking.


00:01:41,500 --> 00:01:43,520
So delete the max distance
parameter first.


00:01:49,540 --> 00:01:51,860
And change the findattribval return


00:01:51,860 --> 00:01:55,460
to integer array that will return
all the matches in an array.


00:01:56,080 --> 00:01:57,980
Now loop over these matches.


00:02:02,740 --> 00:02:04,620
Remove the distance comparisons.


00:02:07,240 --> 00:02:09,540
And if you find a match, break the loop.


00:02:10,340 --> 00:02:12,000
There shouldn't be more than 2 matches


00:02:12,000 --> 00:02:14,260
but the first match could be 
the correct match


00:02:14,260 --> 00:02:17,440
so we don't want to waste another iteration
checking the other point.


00:02:18,240 --> 00:02:20,780
Set the group to new open edges low res group.


00:02:32,020 --> 00:02:34,240
As you can see, the result is the same.


00:03:00,060 --> 00:03:02,420
We got about 30 percent 
speed improvement


00:03:02,420 --> 00:03:04,180
on the rewire vertices node.


00:03:07,940 --> 00:03:09,860
Also there is one thing I forgot to set.


00:03:10,380 --> 00:03:13,120
If you noticed the radius attribute is 64 bit.


00:03:13,320 --> 00:03:15,780
But VEX Precision Auto doesn't mean,


00:03:15,780 --> 00:03:19,060
process attributes in their 
actual precision but rather


00:03:19,060 --> 00:03:22,060
use the preferred precision for 
a particular geometry,


00:03:22,060 --> 00:03:24,040
which is 32bit by default.


00:03:25,340 --> 00:03:27,940
If we want to process radius as 64bit,


00:03:27,940 --> 00:03:31,580
we have to explicitly set VEX 
Precision to 64bit.


00:03:32,220 --> 00:03:33,480
Not that when you do this,


00:03:33,480 --> 00:03:35,000
everything in that wrangle


00:03:35,000 --> 00:03:37,260
would be processed in 64bit,


00:03:37,640 --> 00:03:39,140
it's all or nothing.


00:03:39,340 --> 00:03:40,720
You can't mix and match.


00:03:41,260 --> 00:03:43,560
If we could, Side Effects would have to add


00:03:43,560 --> 00:03:45,900
double precision types for every type like


00:03:46,060 --> 00:03:49,480
float vs double, int vs long, etc.


00:03:50,480 --> 00:03:52,660
You can change the preferred precision


00:03:52,660 --> 00:03:54,480
using the Attribute Cast SOP.


00:03:55,860 --> 00:03:58,260
There is another very important thing 
we have to note here.


00:03:58,520 --> 00:04:00,240
When we are matching hash values


00:04:00,240 --> 00:04:02,860
by using copying these from 
another subdivide node


00:04:02,860 --> 00:04:04,760
set to OpenSubdiv Bilinear,


00:04:04,760 --> 00:04:06,420
we are also copying P,


00:04:06,420 --> 00:04:09,440
and thus making the border points rigid.


00:04:10,700 --> 00:04:12,160
Before we had to do this


00:04:12,160 --> 00:04:15,880
to match them perfectly, 
but now we are not relying


00:04:15,880 --> 00:04:16,760
on distance metrics.


00:04:17,700 --> 00:04:20,240
So by default it's more correct 
to exclude P


00:04:20,240 --> 00:04:23,020
from attribute copy so the
point positions


00:04:23,020 --> 00:04:26,340
are entirely driven by our 
primary subdivision node.


00:04:26,840 --> 00:04:28,740
You can definitely expose this 
as an option


00:04:28,740 --> 00:04:31,540
whether other people want to have 
smooth or rigid borders


00:04:32,360 --> 00:04:33,500
but out of the box,


00:04:33,500 --> 00:04:36,120
due to the way OpenSubdiv Catmull Clark works,


00:04:36,120 --> 00:04:38,240
the borders should also be smooth.


00:04:39,140 --> 00:04:41,640
As you can see, the borders 
became smooth now,


00:04:41,640 --> 00:04:43,840
and we still do not have any 
holes or cracks


00:04:43,900 --> 00:04:45,360
around subdivision sites,


00:04:45,360 --> 00:04:48,500
which means our
custom fusing works perfectly.


00:04:49,120 --> 00:04:51,300
We can now do a performance comparison


00:04:51,300 --> 00:04:53,380
between the new version and the old version.


00:05:01,840 --> 00:05:03,880
The old version took about 6 minutes,


00:05:03,880 --> 00:05:05,620
at subdivision level 10.


00:05:09,300 --> 00:05:12,900
Fuse node by itself took half of 
the entire computation time.


00:05:23,280 --> 00:05:25,940
The new version took 1.5 minutes,


00:05:26,520 --> 00:05:29,240
so about 4 times faster than 
the old version.


00:05:29,480 --> 00:05:31,120
A very significant improvement


00:05:31,120 --> 00:05:33,120
to an already impressive performance.


00:05:38,500 --> 00:05:41,100
The unshared edges seem to 
take quite a bit of time.


00:05:41,600 --> 00:05:43,180
We can reduce this further.


00:05:44,000 --> 00:05:46,540
There is more performance juice to squeeze.


00:05:47,340 --> 00:05:50,480
You see all the group operations 
after the second subdivide node?


00:05:51,060 --> 00:05:52,580
We don't really need these.


00:05:52,880 --> 00:05:54,740
The topology is exactly the same,


00:05:54,740 --> 00:05:57,080
so we can just copy the group 
from the main branch,


00:05:57,080 --> 00:05:58,540
which is much cheaper to do.


00:05:59,140 --> 00:06:02,840
Create a group copy node for the 
final point group we are interested in.


00:06:19,420 --> 00:06:20,880
Let's profile this change.


00:06:23,600 --> 00:06:26,140
We got about 9 percent improvement overall.


00:06:26,140 --> 00:06:27,380
That's pretty good actually.


00:06:30,480 --> 00:06:31,580
Group combine nodes


00:06:31,580 --> 00:06:33,240
are also taking quite a bit of time,


00:06:33,240 --> 00:06:34,280
but I don't know if there is


00:06:34,300 --> 00:06:36,020
a way we can make them faster.


00:06:37,080 --> 00:06:40,240
We spent a bit of time and 
used all kinds of tricks


00:06:40,240 --> 00:06:42,400
and made our tool 4 times faster.


00:06:43,100 --> 00:06:44,620
That's a major achievement


00:06:44,620 --> 00:06:47,040
given how fast
OpenSubdiv already is


00:06:47,040 --> 00:06:48,680
and how many more operations


00:06:48,680 --> 00:06:51,760
we had to do before subdivision to achieve
adaptivity,


00:06:51,760 --> 00:06:54,160
and after subdivision to preserve


00:06:54,160 --> 00:06:56,980
the boundary of the subdivision 
sites at each level.


00:06:57,680 --> 00:07:00,000
There were so many challenges 
we went through,


00:07:00,180 --> 00:07:02,860
which is also typical of shot work 
in a production environment.


00:07:03,600 --> 00:07:06,560
But the more you get used to this 
kind of intense problem solving,


00:07:06,560 --> 00:07:07,920
the easier your next


00:07:07,920 --> 00:07:10,940
challenges will be to the point it 
might come easy to you


00:07:10,940 --> 00:07:13,160
when others perceive them as very


00:07:13,160 --> 00:07:14,760
tough to solve to impossible.


00:07:23,880 --> 00:07:26,040
Now one final challenge remains.


00:07:26,640 --> 00:07:28,920
It's something that might not be as apparent.


00:07:28,920 --> 00:07:31,440
If you look carefully from some 
angles in the viewport,


00:07:31,440 --> 00:07:33,120
you might sort of see it,


00:07:33,420 --> 00:07:35,340
but it's more visible in the renders.


00:07:36,020 --> 00:07:37,920
And that's the surface normals.


00:07:42,680 --> 00:07:44,520
Depending on your shot requirements,


00:07:44,520 --> 00:07:47,000
you might get away with using 
a Normal SOP.


00:07:54,600 --> 00:07:57,260
Make sure to not use By 
Vertex Angle option


00:07:57,260 --> 00:08:00,280
as that might produce artifacts 
like you see here.


00:08:00,520 --> 00:08:02,680
Each Vertex Equally is a good one.


00:08:03,100 --> 00:08:05,160
It's hard to see the difference here,


00:08:05,160 --> 00:08:07,540
but in any case, 
there is a technique that we


00:08:07,540 --> 00:08:09,740
can use to obtain the surface normals


00:08:09,740 --> 00:08:10,960
from the limit surface.


00:08:11,240 --> 00:08:14,140
Something that would produce 
the highest quality result


00:08:14,140 --> 00:08:16,740
as when we render polygons as subdivision,


00:08:16,740 --> 00:08:21,100
which you might end up doing, nothing can
be smoother than the limit surface.


00:08:22,740 --> 00:08:27,260
In the next chapter, we will tap into the
power of the limit surface, and how we can


00:08:27,260 --> 00:08:30,840
use it to solve all kinds of problems that
look almost like magic,


00:08:31,320 --> 00:08:33,520
as we don't have to perform any subdivision


00:08:33,520 --> 00:08:34,980
to utilize the limit surface.


00:08:36,680 --> 00:08:39,880
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

64 - Limit Surface Sampling - Introduction

----------------------------------------------------------------------------------------------------



00:00:04,200 --> 00:00:06,320
To generate interpolating surfaces


00:00:06,320 --> 00:00:08,200
for other subdivision schemes


00:00:08,200 --> 00:00:10,090
we need a method of determining


00:00:10,090 --> 00:00:12,820
the position and the normal at 
a set of points


00:00:12,820 --> 00:00:14,220
on the limit surface.


00:00:14,850 --> 00:00:16,920
Because the surface is the result of repeated


00:00:16,920 --> 00:00:19,300
application of a subdivision step, we can


00:00:19,300 --> 00:00:22,080
analyze the behavior of a small
neighborhood of points


00:00:22,080 --> 00:00:24,580
as they converge to the limit surface


00:00:24,580 --> 00:00:26,000
in order to determine


00:00:26,000 --> 00:00:29,180
the surface properties
at the point of convergence.


00:00:29,940 --> 00:00:32,300
Subdivision surfaces are a powerful


00:00:32,300 --> 00:00:33,600
and useful technique


00:00:33,600 --> 00:00:35,760
in modeling free-form surfaces.


00:00:35,760 --> 00:00:38,160
They can be widely used in many fields,


00:00:38,160 --> 00:00:41,340
such as computer-aided design, 
graphical modeling,


00:00:41,340 --> 00:00:45,080
computer animation, 
medical image processing and so on.


00:00:45,900 --> 00:00:47,760
The CatmullClark subdivision surface


00:00:47,760 --> 00:00:50,560
was designed to generalize 
the bi-cubic B-spline


00:00:50,560 --> 00:00:53,200
surface to the meshes of arbitrary topology.


00:00:54,220 --> 00:00:55,720
In CatmullClark scheme,


00:00:55,720 --> 00:00:57,640
an initial control mesh is refined


00:00:57,640 --> 00:01:00,180
by adding new vertices, faces


00:01:00,180 --> 00:01:02,580
and edges at each subdivision step [1].


00:01:03,340 --> 00:01:06,600
In the limit as the number of 
subdivision steps goes to infinity,


00:01:06,600 --> 00:01:08,290
the control mesh converges


00:01:08,290 --> 00:01:10,000
to the CatmullClark limit surface.


00:01:10,800 --> 00:01:13,400
Thus CatmullClark scheme 
is an approximation


00:01:13,400 --> 00:01:14,400
subdivision scheme.


00:01:15,440 --> 00:01:18,180
I would consider the use of 
subdivision limit surface


00:01:18,180 --> 00:01:21,440
a wildcard of sorts, 
meaning it can


00:01:21,440 --> 00:01:22,980
solve some elusive problems


00:01:22,980 --> 00:01:25,800
that might look like black magic 
to unfamiliar people,


00:01:26,040 --> 00:01:30,220
or the solution simply be faster, 
smarter, lighter, etc.


00:01:31,040 --> 00:01:32,600
It's a very simple concept


00:01:32,600 --> 00:01:34,800
and thanks to OpenSubdiv
and the integration


00:01:34,800 --> 00:01:36,380
within Houdini that makes


00:01:36,380 --> 00:01:37,540
it very easy to use.


00:01:38,240 --> 00:01:40,160
We will look at some practical examples


00:01:40,160 --> 00:01:42,180
that you can use in your day to day work.


00:01:42,860 --> 00:01:45,580
With that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

65 - Limit Surface Sampling - OpenSubdiv Patches

----------------------------------------------------------------------------------------------------



00:00:04,860 --> 00:00:06,580
Each quadrilateral face


00:00:06,580 --> 00:00:08,260
in a Catmull-Clark control mesh


00:00:08,260 --> 00:00:10,800
corresponds to a single bicubic patch


00:00:11,340 --> 00:00:15,680
except for quadrilaterals that contain
an extraordinary vertex.


00:00:16,160 --> 00:00:17,940
These extraordinary patches,


00:00:17,940 --> 00:00:19,740
patches containing one or more


00:00:19,740 --> 00:00:21,760
extraordinary vertices, are actually


00:00:21,760 --> 00:00:24,460
composed of an infinite collection of 
bicubic patches.


00:00:25,300 --> 00:00:26,980
We can get a list of patch IDs


00:00:26,980 --> 00:00:28,300
in a subdivision hull


00:00:28,300 --> 00:00:30,220
using the osd_patches function.


00:00:31,180 --> 00:00:32,520
It's a nice candidate


00:00:32,520 --> 00:00:33,980
to be our first function


00:00:33,980 --> 00:00:36,300
into the world of OpenSubdiv in VEX.


00:00:37,280 --> 00:00:38,900
We will use a single polygon


00:00:38,900 --> 00:00:40,520
as the input geometry,


00:00:40,520 --> 00:00:43,000
so create a circle with 3 divisions.


00:00:48,320 --> 00:00:49,680
Subdivide this polygon,


00:00:53,000 --> 00:00:55,080
split and scale down
each polygon


00:01:10,540 --> 00:01:12,140
and color them randomly.


00:01:19,860 --> 00:01:22,900
What we are looking at are basically 
the OpenSubdiv patches.


00:01:23,560 --> 00:01:25,140
Now create an attribute wrangle,


00:01:28,060 --> 00:01:32,200
call osd_patches
and store the resulting array as an attribute.


00:01:33,660 --> 00:01:35,260
I also store the array count.


00:01:41,420 --> 00:01:44,260
As you can see, we get 3 patches for a triangle.


00:01:46,380 --> 00:01:48,280
But watch what happens when we have a quad.


00:01:48,980 --> 00:01:52,720
We don't get 4 patches, instead we only get 1 patch.


00:01:53,220 --> 00:01:54,260
Why is that?


00:01:54,980 --> 00:01:57,980
Catmull-Clark subdivision behaves nicely with quads,


00:01:57,980 --> 00:02:01,140
and as such using them yields a smoother surface.


00:02:01,900 --> 00:02:04,600
By the nature of the Catmull-Clark
subdivision algorithm,


00:02:04,600 --> 00:02:06,660
a quad will yield a single patch,


00:02:07,080 --> 00:02:09,760
unless they contain an extraordinary vertex.


00:02:10,460 --> 00:02:12,560
As you can see in the last 2 images,


00:02:12,560 --> 00:02:16,140
the central quads produce more 
than 1 patch, even though


00:02:16,140 --> 00:02:16,780
they are quads.


00:02:17,600 --> 00:02:20,920
That's because the vertex at 
the very center of the geometry,


00:02:20,920 --> 00:02:22,640
is an extraordinary vertex,


00:02:22,640 --> 00:02:25,560
and as such causes the 
quads that contain


00:02:25,560 --> 00:02:28,060
it to produce multiple patches, rather than


00:02:28,060 --> 00:02:28,560
1 patch.


00:02:32,440 --> 00:02:35,220
A pentagon produces 5 patches,


00:02:40,600 --> 00:02:44,820
and a 10 sided polygon produces,
well 10 patches, and so on.


00:02:50,100 --> 00:02:52,620
With that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

66 - Limit Surface Sampling - Moving Points to the Subdivision Limit Surface

----------------------------------------------------------------------------------------------------



00:00:03,900 --> 00:00:06,620
There is a very practical use of 
the limit surface


00:00:06,620 --> 00:00:10,280
to figure out where a point is 
on the subdivision limit surface.


00:00:10,560 --> 00:00:13,480
Imagine a character head like 
the one we were using,


00:00:13,480 --> 00:00:15,420
and you add a lot of detail to it,


00:00:15,420 --> 00:00:18,580
just like what we did before using 
adaptive subdivision.


00:00:18,940 --> 00:00:20,840
That sounds all good in action,


00:00:20,840 --> 00:00:22,900
but what we are missing is 
the future projection


00:00:22,900 --> 00:00:25,100
of the same geometry, by additional


00:00:25,100 --> 00:00:27,975
subdivision, either as a post operation


00:00:27,980 --> 00:00:31,560
before exporting out the geometry, 
or as a render time operation.


00:00:32,160 --> 00:00:32,920
In any case,


00:00:32,920 --> 00:00:35,320
this will result in the areas where 
there is added detail,


00:00:35,320 --> 00:00:36,740
to be flatter than before.


00:00:36,780 --> 00:00:39,940
Even though, it's not the case 
with our adaptive subdivision.


00:00:40,160 --> 00:00:41,580
Because if you recall,


00:00:41,580 --> 00:00:44,220
we were using Catmull-Clark 
subdivision algorithm,


00:00:44,460 --> 00:00:47,140
which will keep the original 
geometry nice and smooth.


00:00:47,520 --> 00:00:50,200
But in other cases where you 
might have additional


00:00:50,200 --> 00:00:52,580
detail using algorithms such as Bricker,


00:00:52,580 --> 00:00:54,160
it might result in the areas where


00:00:54,160 --> 00:00:56,780
there is added detail to be flatter 
than before.


00:00:57,500 --> 00:00:58,940
But we don't necessarily want this.


00:00:59,640 --> 00:01:01,540
What we want is to create detail


00:01:01,540 --> 00:01:03,000
without disturbing the feature


00:01:03,000 --> 00:01:04,740
subdivide and shape of the geometry.


00:01:05,340 --> 00:01:06,580
How can we do this?


00:01:07,140 --> 00:01:08,500
Evaluating attributes


00:01:08,500 --> 00:01:10,160
at the subdivision limit surface


00:01:10,160 --> 00:01:11,420
gives us a way out.


00:01:11,820 --> 00:01:14,760
We will use a few new 
OpenSubdiv functions,


00:01:14,760 --> 00:01:17,300
one of them is osd_lookuppatch.


00:01:17,880 --> 00:01:21,820
Given a face ID and texture 
coordinates for a point inside the face,


00:01:21,820 --> 00:01:23,600
face u and face v),


00:01:23,600 --> 00:01:26,600
this function will return 
the corresponding patch ID


00:01:26,600 --> 00:01:30,020
and patch interpolants 
(patch u and patch v).


00:01:30,520 --> 00:01:33,500
Essentially we are converting 
a primitive ID and


00:01:33,500 --> 00:01:37,380
primitive UVs into OpenSubdiv 
patch ID and patch UVs.


00:01:38,160 --> 00:01:41,760
That's how we establish a 
correspondence between the subdivision cage


00:01:41,760 --> 00:01:43,880
and the subdivision limit surface.


00:01:44,280 --> 00:01:47,720
You can do the opposite using the
osd_lookupface function.


00:01:48,280 --> 00:01:51,940
The key function that comes after 
is where the real magic happens.


00:01:52,480 --> 00:01:54,820
It's the osd_limitsurface function


00:01:54,820 --> 00:01:56,820
that will evaluate any point attribute


00:01:56,820 --> 00:01:58,720
at the subdivision limit surface.


00:01:59,220 --> 00:02:03,680
For vertex attributes, you can use the osd_limitsurfacevertex function.


00:02:04,060 --> 00:02:06,620
Let's see how to implement this in Houdini.


00:02:08,940 --> 00:02:12,860
First create a box with bounding 
box colors and point normals.


00:02:15,840 --> 00:02:19,340
Create a subdivide node and set depth to
a high value like 7.


00:02:24,500 --> 00:02:28,320
Now create another box, 
but this time with a lot of divisions.


00:02:28,760 --> 00:02:32,020
We are mimicking adding a lot of detail 
to the same model.


00:02:32,440 --> 00:02:35,800
If you subdivide this geometry, 
we would not get anything


00:02:35,800 --> 00:02:38,180
similar to the subdivided version 
of the original box,


00:02:38,180 --> 00:02:39,880
which is what we want to get.


00:02:40,280 --> 00:02:41,580
So create a subnet.


00:02:54,000 --> 00:02:57,020
Then create a string parameter 
for the list of attributes


00:02:57,025 --> 00:03:00,285
we want to sample at the subdivision 
limit surface.


00:03:03,020 --> 00:03:06,440
Add an option for minimum projection.
We will get to this soon.


00:03:13,100 --> 00:03:16,460
We have color, normal and position attributes.


00:03:17,520 --> 00:03:20,260
So add these to the list of attributes to sample.


00:03:33,155 --> 00:03:35,875
Link minimum projection to the switch node.


00:03:38,985 --> 00:03:40,385
Create an attribute wrangle.


00:03:40,905 --> 00:03:43,695
What we mean by minimum projection is


00:03:43,695 --> 00:03:46,265
what the Ray SOP does with Minimum Distance option.


00:03:46,720 --> 00:03:48,940
We need to find out the primitive indices


00:03:48,940 --> 00:03:51,400
and UVs that belong to each point.


00:03:51,840 --> 00:03:54,860
You can imagine the corner points of the box,


00:03:54,865 --> 00:03:57,895
each point can reference any of 
the faces it's shared by


00:03:57,895 --> 00:04:00,745
and the appropriate UV values for each face


00:04:00,745 --> 00:04:03,235
and they will all yield the same end result.


00:04:03,575 --> 00:04:06,855
So here we will just perform a 
standard nearest surface lookup


00:04:06,855 --> 00:04:10,065
using the xyzdist function and 
store the returned primitive


00:04:10,065 --> 00:04:11,800
index and UV values.


00:04:12,440 --> 00:04:14,460
As you can see, we got these attributes.


00:04:17,820 --> 00:04:21,160
Now create another attribute 
wrangle node where we will evaluate


00:04:21,160 --> 00:04:24,220
each point attribute at the 
subdivision limit surface.


00:04:45,320 --> 00:04:48,445
We first call osd_lookuppatch on 
the second input


00:04:48,445 --> 00:04:51,400
and pass our primitive index 
and UV values


00:04:51,400 --> 00:04:54,840
and it will return the corresponding 
patch ID  and UV values.


00:05:03,040 --> 00:05:06,160
Once we get these, we will split the 
attribute list


00:05:06,160 --> 00:05:09,300
using spaces so we can loop over 
each attribute.


00:05:16,355 --> 00:05:19,375
Now we can call the osd_limitsurface
function on the


00:05:19,380 --> 00:05:22,580
second input, which is where 
the original geometry is connected,


00:05:22,580 --> 00:05:25,485
and pass to it the name of the 
attribute to evaluate,


00:05:25,485 --> 00:05:28,875
the patch ID and the UVs we 
want to evaluate at.


00:05:29,220 --> 00:05:33,140
This function returns 1 if successful, 
otherwise 0.


00:05:33,820 --> 00:05:35,760
So we use it in an if statement,


00:05:36,140 --> 00:05:40,840
and if successful, store the values 
we get from this function on the current point.


00:05:44,200 --> 00:05:47,320
As you can see, we got all 3 attributes.


00:05:47,320 --> 00:05:51,080
Except if you look at their data type, 
they are float arrays,


00:05:51,080 --> 00:05:53,760
not colors or normals except P.


00:05:54,380 --> 00:05:57,360
Only P is configured correctly as 
a position attribute.


00:05:57,820 --> 00:06:00,565
But P is always special so it's
not surprising


00:06:00,565 --> 00:06:03,055
that it's automatically recognized 
as a position.


00:06:03,440 --> 00:06:06,760
Not to mention we already had P 
in the input geometry.


00:06:07,240 --> 00:06:10,340
We can do this in VEX, 
but it would complicate the code.


00:06:10,940 --> 00:06:13,920
And there are some workarounds 
like copying attributes


00:06:13,920 --> 00:06:17,180
from the source geometry, 
but even that is not fail proof.


00:06:17,860 --> 00:06:21,180
There is a more generic and 
robust way to do this in Python.


00:06:21,340 --> 00:06:25,160
So create a python SOP, and add 
the same attributes parameter to this.


00:06:51,740 --> 00:06:55,160
First, we store a reference to 
the second input geometry.


00:06:59,585 --> 00:07:02,235
Split the attribute list just like in VEX.


00:07:04,380 --> 00:07:06,040
Loop over these attributes


00:07:06,040 --> 00:07:07,820
and try to find the current attribute


00:07:07,820 --> 00:07:10,140
as a point attribute on the second input.


00:07:14,620 --> 00:07:18,240
If it's a valid attribute and it does 
not exist on the first input,


00:07:18,240 --> 00:07:20,380
then we need to create this attribute.


00:07:29,175 --> 00:07:31,795
The key is to set the same attribute type.


00:07:32,195 --> 00:07:34,805
There is no specialized function for this.


00:07:35,060 --> 00:07:38,540
But you can do this using the 
setOption function to set the type.


00:07:44,800 --> 00:07:49,520
As you can see, we have Cd as 
color, and N as normal.


00:07:50,840 --> 00:07:53,480
VEX code will respect the incoming 
data type,


00:07:53,480 --> 00:07:56,300
and as such everything will 
work as expected.


00:07:56,840 --> 00:07:59,800
We now have the colors 
and normals perfectly.


00:08:13,140 --> 00:08:14,340
As you can see,


00:08:14,340 --> 00:08:15,780
the distribution of the points


00:08:15,780 --> 00:08:17,700
are different between both geometry


00:08:17,700 --> 00:08:19,040
but this is expected.


00:08:19,400 --> 00:08:22,020
One of them is moving the 
points of an already


00:08:22,025 --> 00:08:25,305
uniformly subdivided cube 
to the subdivision limit surface


00:08:25,305 --> 00:08:28,780
while the other one is subdividing
 a simple cube recursively.


00:08:29,440 --> 00:08:32,900
If you look at the surface itself, 
you can see that they are identical.


00:08:33,540 --> 00:08:37,180
You might notice a difference in the 
shading. This is due to normals.


00:08:37,940 --> 00:08:41,420
The default subdivide is using 
the final polygonal surface


00:08:41,420 --> 00:08:43,020
to compute the normals,


00:08:43,020 --> 00:08:46,600
while we are computing the normals
from the subdivision limit surface,


00:08:46,600 --> 00:08:49,740
which should be smoother than 
any normals we can get


00:08:49,745 --> 00:08:53,105
from any subdivided polygonal geometry at any level.


00:08:53,600 --> 00:08:55,440
It's this kind of power


00:08:55,440 --> 00:08:57,340
that makes subdivision limit surface


00:08:57,340 --> 00:09:00,520
a very valuable tool in any TDs arsenal.


00:09:01,480 --> 00:09:04,160
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

67 - Limit Surface Sampling - Scattering Points on the Subdivision Limit Surface

----------------------------------------------------------------------------------------------------



00:00:04,259 --> 00:00:06,940
If you have access to the 
subdivision limit surface


00:00:06,940 --> 00:00:09,809
by patch IDs and the patch UVs, then


00:00:09,809 --> 00:00:13,280
surely you can scatter points 
on the subdivision limit surface


00:00:13,280 --> 00:00:14,680
just like you scatter


00:00:14,680 --> 00:00:16,820
points on a polygonal surface.


00:00:16,820 --> 00:00:17,740
And this is exactly


00:00:17,740 --> 00:00:19,860
what we are gonna do in
this chapter.


00:00:20,220 --> 00:00:22,100
Continuing from where we left off,


00:00:22,100 --> 00:00:23,660
clean up some of the nodes.


00:00:24,180 --> 00:00:25,440
Create a scatter node


00:00:25,440 --> 00:00:26,960
and connect this to the first


00:00:26,960 --> 00:00:29,280
input of the limit surface sample subnet.


00:00:32,560 --> 00:00:33,620
As you can see,


00:00:33,620 --> 00:00:36,340
all the scattered points are projected onto


00:00:36,340 --> 00:00:37,940
the subdivision limit surface,


00:00:37,940 --> 00:00:39,640
as well as getting the colors


00:00:39,640 --> 00:00:41,140
and the normals from the


00:00:41,140 --> 00:00:42,500
subdivision limit surface,


00:00:42,500 --> 00:00:44,140
without doing any changes.


00:00:45,600 --> 00:00:48,040
Instead of using the minimum 
projection option,


00:00:48,040 --> 00:00:50,580
you can use the equivalent options 
in the Scatter SOP.


00:00:52,240 --> 00:00:53,840
Just rename them to limitprim


00:00:53,840 --> 00:00:55,060
and limitprimuv


00:00:55,060 --> 00:00:57,980
to match the limit surface sample 
internal attributes.


00:00:58,780 --> 00:01:01,600
As you can see, 
we get the exact same result.


00:01:02,340 --> 00:01:05,220
With that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

68 - Limit Surface Sampling - Generating a Point Cloud on the Subdivision Limit Surface

----------------------------------------------------------------------------------------------------



00:00:04,540 --> 00:00:06,960
This time instead of using the scatter SOP,


00:00:06,960 --> 00:00:09,280
we will generate a point cloud directly on


00:00:09,280 --> 00:00:10,520
the subdivision limit surface.


00:00:11,080 --> 00:00:13,380
We will use the osd_lookupface function


00:00:13,380 --> 00:00:16,300
to get the face ID 
and UV coordinates from the


00:00:16,300 --> 00:00:17,730
given OpenSubdiv patches.


00:00:18,700 --> 00:00:20,000
The code we will write


00:00:20,000 --> 00:00:21,880
is based on the example
code in the


00:00:21,880 --> 00:00:23,520
osd_lookupface function help card.


00:00:24,740 --> 00:00:26,960
So create a box and point normals first.


00:00:28,380 --> 00:00:29,920
Now create a detail wrangle


00:00:29,920 --> 00:00:32,580
and connect the box geometry 
into the second input.


00:00:33,380 --> 00:00:34,760
Get the target point count


00:00:34,760 --> 00:00:36,100
from a spare parameter.


00:00:39,120 --> 00:00:40,960
We need to figure out the patch count


00:00:40,960 --> 00:00:43,160
using the osd_patchcount function.


00:00:43,720 --> 00:00:46,180
Declare variables for face ID,


00:00:46,180 --> 00:00:48,460
face U, V and point position.


00:00:49,040 --> 00:00:50,540
We won't be using these though.


00:00:50,620 --> 00:00:52,020
We just need to pass these


00:00:52,020 --> 00:00:53,600
to the osd_lookupface function.


00:00:54,040 --> 00:00:57,400
Create a for loop 
from 0 to point_count, being exclusive.


00:00:58,200 --> 00:01:00,560
The code sample was using nrandom


00:01:00,560 --> 00:01:03,280
which is a non-deterministic 
random function, so we


00:01:03,280 --> 00:01:04,540
will just use that.


00:01:04,800 --> 00:01:05,880
In production,


00:01:05,880 --> 00:01:07,300
you don't really want to use that


00:01:07,300 --> 00:01:09,740
as the random values will always be different


00:01:09,740 --> 00:01:12,080
than the last time they were evaluated


00:01:12,080 --> 00:01:14,320
so it won't match the previous results.


00:01:14,940 --> 00:01:17,080
This is more useful for interactive applications


00:01:17,080 --> 00:01:18,100
like games.


00:01:18,680 --> 00:01:19,940
So pick a random patch


00:01:19,940 --> 00:01:22,120
using npatches, and nrandom,


00:01:22,120 --> 00:01:23,880
and cast the result to int.


00:01:24,100 --> 00:01:25,940
We create 2 more random values


00:01:25,940 --> 00:01:27,579
for U and V respectively.


00:01:28,320 --> 00:01:29,480
Pass all of these


00:01:29,480 --> 00:01:32,860
to osd_lookupface function,
and use the second input.


00:01:34,600 --> 00:01:36,580
The code sample is using the "uv"


00:01:36,580 --> 00:01:38,020
as the texture attribute,


00:01:38,020 --> 00:01:39,720
which is the same as the intrinsic


00:01:39,720 --> 00:01:41,080
polygon interpolants.


00:01:41,720 --> 00:01:44,720
So if you left this out, the result would
be exactly the same.


00:01:45,300 --> 00:01:48,720
If the function succeeds we get 1, otherwise 0.


00:01:49,160 --> 00:01:52,160
So use this function inside an 
if statement like before.


00:01:52,700 --> 00:01:54,640
If we successfully look up a face,


00:01:54,640 --> 00:01:57,920
then we evaluate P on the 
subdivision limit surface


00:01:58,080 --> 00:02:00,590
using the osd_limitsurface function.


00:02:00,590 --> 00:02:02,520
Again use the second input.


00:02:03,060 --> 00:02:04,780
This gives us a point position.


00:02:05,260 --> 00:02:07,060
Create a point at this position,


00:02:08,880 --> 00:02:10,400
and store the face ID


00:02:10,400 --> 00:02:12,480
and the patch ID on the new point.


00:02:23,360 --> 00:02:25,600
I will set point count to 1 million.


00:02:27,880 --> 00:02:30,640
Color the points using 
the face ID attribute.


00:02:31,500 --> 00:02:33,100
I am just going to merge the points


00:02:33,100 --> 00:02:34,760
with a slightly smaller subdivided


00:02:34,760 --> 00:02:36,220
geometry to remove


00:02:36,220 --> 00:02:38,140
back facing points from the view.


00:02:54,760 --> 00:02:56,720
Because all the polygons are quads


00:02:56,720 --> 00:02:57,760
and none of them have


00:02:57,760 --> 00:02:59,080
extraordinary vertices,


00:02:59,080 --> 00:03:00,240
the face IDs


00:03:00,240 --> 00:03:02,080
and the patch IDs are the same.


00:03:04,360 --> 00:03:06,340
Let's try it on a tetrahedron.


00:03:21,200 --> 00:03:23,360
As you can see the face IDs


00:03:23,360 --> 00:03:25,420
and the patch
IDs are quite different.


00:03:29,380 --> 00:03:31,260
Once you visualize the patch IDs,


00:03:31,260 --> 00:03:32,480
you can see that they are a lot


00:03:32,480 --> 00:03:33,840
more than the face IDs.


00:03:34,440 --> 00:03:37,240
It's really useful to see the differences
between them


00:03:37,240 --> 00:03:39,120
for different types of geometry


00:03:39,160 --> 00:03:41,600
to better understand open subdiv patches.


00:03:42,740 --> 00:03:45,460
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

69 - Limit Surface Sampling - Pre-Generating a Point Cloud on the Subdivision Limit Surface

----------------------------------------------------------------------------------------------------



00:00:04,480 --> 00:00:06,880
When we are talking about creating a lot of points,


00:00:06,880 --> 00:00:11,360
we can not avoid pre-generating them
for maximum performance.


00:00:12,000 --> 00:00:13,780
It might not always be applicable


00:00:13,780 --> 00:00:14,740
but when it is,


00:00:14,740 --> 00:00:15,960
the performance gains


00:00:15,960 --> 00:00:17,440
can be quite significant.


00:00:18,140 --> 00:00:19,940
Continuing from where we left off,


00:00:19,940 --> 00:00:21,700
create a point generate node,


00:00:21,700 --> 00:00:23,820
Set point count to 1 million.


00:00:26,400 --> 00:00:29,540
And connect it to the first input of 
the attribute wrangle node.


00:00:31,120 --> 00:00:33,840
Remove the point count parameter 
on the wrangle node.


00:00:34,240 --> 00:00:36,460
Instead of storing the position attribute,


00:00:36,460 --> 00:00:38,320
we can directly overwrite @P


00:00:38,320 --> 00:00:41,000
by passing it to the osd_limitsurface function.


00:00:41,840 --> 00:00:44,140
Because this parameter is passed by reference,


00:00:44,140 --> 00:00:46,240
the function is able to write to it directly.


00:00:47,280 --> 00:00:49,280
Remove the for loop from the code.


00:00:55,360 --> 00:00:58,739
We can also directly bind the face id 
and patch id attributes.


00:01:15,600 --> 00:01:17,600
The result looks quite different.


00:01:18,000 --> 00:01:20,100
The point count looks a lot more scarce


00:01:20,100 --> 00:01:21,600
than the previous method.


00:01:21,600 --> 00:01:22,440
Why is that?


00:01:23,040 --> 00:01:25,040
The reason is the VEX threading.


00:01:25,520 --> 00:01:28,300
By default VEX will only call nrandom


00:01:28,300 --> 00:01:29,839
once for every 1000 points,


00:01:30,560 --> 00:01:33,180
therefore, we will get the exact same values


00:01:33,180 --> 00:01:34,960
for each cluster of 1000 points.


00:01:35,760 --> 00:01:38,800
We have to force VEX to compute 
random values for


00:01:38,800 --> 00:01:42,140
each point, and as such we will use 
the rand function


00:01:42,320 --> 00:01:45,060
and pass ptnum with some arbitrary offsets.


00:02:01,600 --> 00:02:04,879
As you can see, the result now looks 
similar to the previous method.


00:02:05,979 --> 00:02:07,979
Finally, let's look at the performance.


00:02:08,720 --> 00:02:11,660
The new parallel approach takes about 80 ms


00:02:14,320 --> 00:02:17,400
vs 1.4 seconds of the old method.


00:02:18,140 --> 00:02:20,160
So, if we increase the point count 10X,


00:02:21,440 --> 00:02:24,340
the new approach now takes 472 ms.


00:02:27,280 --> 00:02:29,140
Even with 30 million points,


00:02:29,140 --> 00:02:31,199
it takes less time than the old method.


00:02:31,840 --> 00:02:33,840
So it's more than 30 times faster,


00:02:33,840 --> 00:02:35,539
and I only have 8 cores.


00:02:36,319 --> 00:02:38,700
So you can see how pre-generating points


00:02:38,700 --> 00:02:40,340
is an easy performance trick


00:02:40,340 --> 00:02:42,180
we should always try to use if we can,


00:02:42,180 --> 00:02:44,220
over single threaded approaches.


00:02:45,280 --> 00:02:48,100
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

70 - Limit Surface Sampling - Creating Isolines on the Subdivision Limit Surface

----------------------------------------------------------------------------------------------------



00:00:04,140 --> 00:00:05,900
There is a useful modeling concept


00:00:05,900 --> 00:00:08,020
that's available in many 3d apps


00:00:08,020 --> 00:00:09,320
including Houdini


00:00:09,320 --> 00:00:12,040
where you see the original geometry projected


00:00:12,040 --> 00:00:14,940
onto the subdivision limit surface that allows


00:00:14,940 --> 00:00:17,520
you to see the flow of the geometry
without cluttering the view


00:00:17,520 --> 00:00:19,900
with the newly added geometric detail.


00:00:20,360 --> 00:00:22,880
And that's exactly what we will create in this lesson.


00:00:23,280 --> 00:00:25,560
Bring any geometry that you like to use.


00:00:25,880 --> 00:00:27,620
And make sure it has point normals.


00:00:28,060 --> 00:00:30,780
Loop over each connected piece of geometry.


00:00:30,900 --> 00:00:33,060
We are not doing this for speed


00:00:33,060 --> 00:00:35,240
but the accuracy of the minimum projection


00:00:35,240 --> 00:00:36,460
can greatly improve


00:00:36,460 --> 00:00:39,380
when we deal with each
connected piece individually.


00:00:39,920 --> 00:00:42,500
Inside the loop, create Convert Line node,


00:00:46,880 --> 00:00:49,460
and resample it to ensure the polylines have


00:00:49,460 --> 00:00:52,740
enough detail to capture the curvature
of the subdivision limit surface.


00:00:53,460 --> 00:00:54,360
Remember


00:00:54,360 --> 00:00:56,220
if you only project the original geometry


00:00:56,220 --> 00:00:58,040
onto the subdivision limit surface,


00:00:58,240 --> 00:00:59,960
you will only get a slightly shrinked version


00:00:59,960 --> 00:01:01,120
of the input geometry.


00:01:01,680 --> 00:01:04,820
So essentially we have to subdivide
the polylines before.


00:01:05,540 --> 00:01:07,740
Copy and paste the latest
limit_surface_sample


00:01:07,740 --> 00:01:10,360
subnet from the previous lesson 
and make sure


00:01:10,360 --> 00:01:12,640
to connect the polylines into the first input


00:01:12,640 --> 00:01:15,000
and the current loop piece into 
the second input.


00:01:15,700 --> 00:01:17,880
Now we have to merge the 
subdivided version


00:01:17,880 --> 00:01:20,800
of the input geometry with 
the projected polylines.


00:01:31,540 --> 00:01:32,460
As you can see,


00:01:32,460 --> 00:01:34,640
some parts are more faded than others.


00:01:35,180 --> 00:01:38,840
It's because the polylines lie on 
the subdivision limit surface,


00:01:38,840 --> 00:01:40,430
whereas the original geometry


00:01:40,430 --> 00:01:42,000
is only subdivided twice.


00:01:42,800 --> 00:01:46,400
So some parts of the polylines are 
bound to be inside more than others.


00:01:46,840 --> 00:01:49,680
We can easily fix this by ray tracing 
the polylines


00:01:49,680 --> 00:01:51,760
against the subdivided geometry.


00:01:52,340 --> 00:01:55,080
So first isolate the subdivided
piece of geometry


00:01:55,080 --> 00:01:57,240
that corresponds to the current loop piece.


00:02:13,640 --> 00:02:15,300
Now create an attribute wrangle.


00:02:26,740 --> 00:02:29,360
What we will do is to fire a ray from each point


00:02:29,360 --> 00:02:31,840
along the point normal, against the


00:02:31,840 --> 00:02:33,500
subdivided piece of geometry.


00:02:41,680 --> 00:02:42,600
If we have a hit,


00:02:42,600 --> 00:02:45,080
then we sample the normal
at the hit location.


00:02:45,660 --> 00:02:47,840
Compare this to the current point normal.


00:02:48,600 --> 00:02:50,240
If the dot product between them


00:02:50,240 --> 00:02:53,760
is greater than 0, 
that means they are both facing the


00:02:53,760 --> 00:02:57,000
same direction, short of being
 perpendicular to each other.


00:02:57,620 --> 00:03:01,020
If that's the case, we move this point to
the hit location.


00:03:01,560 --> 00:03:03,780
Essentially this will push the points


00:03:03,780 --> 00:03:06,600
that are inside the subdivided geometry, outside.


00:03:07,860 --> 00:03:09,960
And as you can see, the fading is gone.


00:03:10,620 --> 00:03:12,340
But we can still improve this.


00:03:12,720 --> 00:03:15,120
We can displace the polylines outward


00:03:15,120 --> 00:03:18,060
just a little bit more, 
to make them more pronounced.


00:03:19,980 --> 00:03:22,820
So create another attribute wrangle 
after the for loop.


00:03:23,200 --> 00:03:25,920
We will just implement a basic peak operation.


00:03:33,720 --> 00:03:36,360
You can see, 
even with a very low peak value,


00:03:36,360 --> 00:03:38,780
the polylines became much 
more pronounced.


00:03:40,260 --> 00:03:43,320
This depends on the scale and the
topology of the incoming geometry,


00:03:43,320 --> 00:03:44,780
so you might have


00:03:44,780 --> 00:03:46,320
to use different values than mine.


00:03:46,660 --> 00:03:48,460
Such a simple but neat effect.


00:03:49,000 --> 00:03:51,440
Essentially you can reference an object


00:03:51,440 --> 00:03:53,060
that you are interactively modeling,


00:03:53,060 --> 00:03:53,940
inside another


00:03:53,940 --> 00:03:57,360
object and create polylines from 
the actively edited object


00:03:57,360 --> 00:03:59,560
to have your own isoline display.


00:04:00,320 --> 00:04:02,800
It would be fast enough to 
use it interactively.


00:04:09,580 --> 00:04:12,140
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

71 - Adaptive Subdivision - Computing Surface Normals from the Subdivision Limit Surface

----------------------------------------------------------------------------------------------------



00:00:04,300 --> 00:00:06,720
We have already seen how to evaluate attributes


00:00:06,720 --> 00:00:08,460
at the subdivision limit surface,


00:00:08,460 --> 00:00:10,320
including the surface normals.


00:00:10,900 --> 00:00:13,480
This is not only a useful trick in many situations


00:00:13,480 --> 00:00:15,900
where you need accurate surface normals,


00:00:15,900 --> 00:00:17,640
but it also becomes a necessity


00:00:17,640 --> 00:00:20,960
when you have to add localized
detail on your geometry


00:00:21,160 --> 00:00:23,420
where any operation to compute the normals


00:00:23,420 --> 00:00:24,800
from the final geometry


00:00:24,800 --> 00:00:26,220
might create render artifacts


00:00:26,220 --> 00:00:28,280
due to the surface normals.


00:00:28,900 --> 00:00:31,180
It could either be resulting from a manual


00:00:31,180 --> 00:00:33,440
computation of the normals inside Houdini


00:00:33,440 --> 00:00:36,620
or at render time i.e. by RenderMan,


00:00:36,620 --> 00:00:40,460
especially when using render-time
subdivision or displacements.


00:00:41,080 --> 00:00:44,100
I have seen this issue when I was 
working on the crack setup


00:00:44,100 --> 00:00:45,720
for the movie Dark Phoenix.


00:00:46,260 --> 00:00:48,940
Fortunately, computing the surface normals


00:00:48,940 --> 00:00:50,700
from the subdivision limit surface


00:00:50,700 --> 00:00:51,980
easily fixes that.


00:00:52,640 --> 00:00:56,220
Because these normals not only 
originate from the original geometry,


00:00:56,220 --> 00:00:57,920
but they are also much more accurate


00:00:57,920 --> 00:01:01,820
than any representation of the
polygonal geometry can give us,


00:01:01,820 --> 00:01:04,360
except the subdivision limit surface itself.


00:01:05,140 --> 00:01:08,220
You might only want to do this for
the newly added geometry


00:01:08,220 --> 00:01:10,220
as opposed to the entire geometry


00:01:10,220 --> 00:01:12,920
to ensure you don't modify the existing normals


00:01:12,920 --> 00:01:14,780
of the unmodified geometry.


00:01:15,460 --> 00:01:17,600
So first create a primitive wrangle,


00:01:20,100 --> 00:01:24,180
and create a group for all primitives 
whose depth is greater than 0.


00:01:33,500 --> 00:01:35,720
Promote this group into a point group.


00:01:41,440 --> 00:01:44,420
Copy and paste the latest
 limit_surface_sample subnet


00:01:44,420 --> 00:01:46,480
and set Attributes to N.


00:02:04,940 --> 00:02:07,240
As you can see, there is a difference,


00:02:07,240 --> 00:02:10,980
but more visible in the unmodified parts
of the original geometry.


00:02:11,560 --> 00:02:13,760
You can also normalize the point normals.


00:02:22,740 --> 00:02:26,680
Add group and group type parameters 
to the limit_surface_sample subnet


00:02:26,680 --> 00:02:28,980
from one of the attribute wrangle nodes inside.


00:02:33,160 --> 00:02:36,900
Set group to an ad-hoc group where 
the depth is greater than 0,


00:02:36,900 --> 00:02:39,240
and set the group type to primitives.


00:02:39,880 --> 00:02:42,460
Paste relative references 
of these parameters


00:02:42,460 --> 00:02:45,000
to the attribute wrangle nodes 
inside the subnet.


00:03:06,040 --> 00:03:08,160
Visually it's hard to see the difference


00:03:08,620 --> 00:03:12,020
but you can see the normals have 
different values indeed.


00:03:12,760 --> 00:03:15,740
After all they are not going
to be significantly different.


00:03:16,440 --> 00:03:18,740
If you are dealing with adaptive subdivision,


00:03:18,740 --> 00:03:21,200
Catmull-Clark based subdivision, or any kind


00:03:21,200 --> 00:03:25,740
of localized detail, you can use this 
trick to fix surface normal issues,


00:03:25,740 --> 00:03:28,680
or at the very least improve your
render accuracy.


00:03:30,020 --> 00:03:33,020
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

72 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - Concept

----------------------------------------------------------------------------------------------------



00:00:11,580 --> 00:00:16,320
Splitting edges of a polygon is
precursor to subdividing a polygon.


00:00:17,240 --> 00:00:18,200
So doing this


00:00:18,200 --> 00:00:21,500
will move you closer to 
implementing subdivision using VEX,


00:00:21,500 --> 00:00:22,760
if you choose to do it.


00:00:23,480 --> 00:00:25,480
There are many ways to implement this.


00:00:25,900 --> 00:00:29,280
You can perform everything using 
a single detail wrangle,


00:00:29,320 --> 00:00:31,000
but that would be very slow


00:00:31,140 --> 00:00:32,680
and would not scale at all.


00:00:33,660 --> 00:00:35,120
You want to break down each step


00:00:35,120 --> 00:00:38,980
so that you can parallelize anything
that can be parallelized.


00:00:39,520 --> 00:00:43,500
That's the only way you can have a 
shot at beating C++ tools,


00:00:43,500 --> 00:00:45,980
apart from using faster algorithms.


00:00:47,140 --> 00:00:49,440
Let's see the concept of the algorithm first.


00:00:53,300 --> 00:00:55,160
Imagine a grid geometry like this


00:00:55,160 --> 00:00:58,300
where we have 3 selected edges like so.


00:00:58,740 --> 00:01:00,740
We first get the edge primitives,


00:01:00,740 --> 00:01:03,640
so that we can rebuild these
primitives later on.


00:01:04,160 --> 00:01:07,580
Any primitive that touches an 
edge in the input edge group,


00:01:07,580 --> 00:01:09,400
will have to be rebuilt,


00:01:09,400 --> 00:01:10,700
and as such


00:01:10,700 --> 00:01:14,040
we need the primitives that are
shared by each edge


00:01:14,040 --> 00:01:15,440
in the input edge group.


00:01:15,780 --> 00:01:18,260
As we will see inside Houdini soon,


00:01:18,260 --> 00:01:21,680
unfortunately
Group Promote doesn't give us this.


00:01:22,580 --> 00:01:24,240
What Group Promote does is,


00:01:24,240 --> 00:01:27,520
when you convert an edge group 
into a primitive group,


00:01:27,660 --> 00:01:30,080
it treats the edge group as a point group


00:01:30,080 --> 00:01:33,940
and so we lose the additional 
information provided by edges.


00:01:34,400 --> 00:01:36,940
Therefore you end up with these primitives.


00:01:38,360 --> 00:01:40,840
But we want to end up with
these primitives.


00:01:41,620 --> 00:01:44,140
We can actually implement this using VEX


00:01:44,140 --> 00:01:45,760
by using half edges.


00:01:46,220 --> 00:01:48,220
But the thing is, we don't have to.


00:01:49,100 --> 00:01:51,620
Using an edge group on a primitive wrangle,


00:01:51,620 --> 00:01:54,060
we automatically get these edge primitives


00:01:54,060 --> 00:01:56,280
and as such we can easily group it


00:01:56,280 --> 00:01:58,380
with the one-liner group syntax.


00:01:58,820 --> 00:02:02,180
Then we create the new points,
using a point wrangle.


00:02:02,180 --> 00:02:04,180
Because there is no edge wrangle,


00:02:04,180 --> 00:02:07,600
not out of the box anyway, 
we will have to use a point wrangle.


00:02:08,160 --> 00:02:11,060
Because of this we will use 
the neighbours function.


00:02:11,720 --> 00:02:16,660
The issue is, again, we are losing additional
information provided by edges.


00:02:17,520 --> 00:02:18,880
And as you can see,


00:02:18,880 --> 00:02:20,940
we are getting all these extra edges


00:02:20,940 --> 00:02:22,520
that we will have to exclude.


00:02:23,380 --> 00:02:25,160
If we just use the points directly,


00:02:25,160 --> 00:02:26,600
there is no way to know


00:02:26,600 --> 00:02:28,020
if 2 points actually form


00:02:28,020 --> 00:02:30,740
a valid edge inside the input edge group.


00:02:31,660 --> 00:02:33,160
So if we take a single point


00:02:33,160 --> 00:02:35,360
inside the input edge group like this,


00:02:35,360 --> 00:02:36,960
call it ptnum,


00:02:38,280 --> 00:02:43,620
and call the neighbour points 
pt0, pt1, pt2, pt3.


00:02:44,400 --> 00:02:46,180
And because we are using points,


00:02:46,180 --> 00:02:48,580
there is also the issue of doubling up,


00:02:48,840 --> 00:02:49,640
where you could


00:02:49,659 --> 00:02:51,520
end up with twice the amount of points


00:02:51,520 --> 00:02:52,740
on each input edge.


00:02:53,290 --> 00:02:55,760
We will avoid it by creating new points


00:02:55,760 --> 00:02:58,400
only if the current point number 
is less than the


00:02:58,400 --> 00:02:59,700
neighbour point number.


00:03:00,120 --> 00:03:02,400
You can also do it,
the opposite way if you want.


00:03:03,080 --> 00:03:05,300
The most important function that will help


00:03:05,300 --> 00:03:07,640
us check if a particular edge is inside the


00:03:07,650 --> 00:03:09,240
input edge group is


00:03:09,240 --> 00:03:10,660
inedgegroup function.


00:03:11,100 --> 00:03:13,280
And only if both conditions are met,


00:03:13,280 --> 00:03:14,980
then we will create a new point.


00:03:23,920 --> 00:03:27,660
So as an example: ptnum-pt2 is valid.


00:03:27,660 --> 00:03:30,620
But ptnum-pt3 is not valid,


00:03:30,620 --> 00:03:32,780
because even though it's a valid edge,


00:03:32,780 --> 00:03:34,580
it's not in the input edge group.


00:03:36,520 --> 00:03:38,640
After the new points are created,


00:03:38,640 --> 00:03:40,520
we will create new primitives


00:03:40,520 --> 00:03:42,460
using a primitive wrangle.


00:03:45,220 --> 00:03:47,780
We will use the edge prims 
group we created


00:03:47,780 --> 00:03:50,220
using a primitive wrangle before hand.


00:03:51,460 --> 00:03:53,800
Because the number of primitives 
do not change,


00:03:53,800 --> 00:03:56,440
we can just copy all primitive attributes,


00:03:56,440 --> 00:03:58,360
rather than interpolate them.


00:03:58,940 --> 00:04:01,360
Always think of the least costly and most


00:04:01,360 --> 00:04:03,240
exact method that you need.


00:04:04,080 --> 00:04:06,180
So copying is more exact, accurate


00:04:06,180 --> 00:04:07,680
and computationally cheaper


00:04:07,680 --> 00:04:09,840
in this case than interpolating them.


00:04:10,680 --> 00:04:12,760
For point and vertex attributes,


00:04:12,760 --> 00:04:15,900
we have to interpolate them, including P.


00:04:16,520 --> 00:04:19,760
We include P because P is not special for us


00:04:19,760 --> 00:04:21,040
to do it in VEX.


00:04:21,660 --> 00:04:24,320
Interpolating an arbitrary number of attributes


00:04:24,320 --> 00:04:27,120
of multiple types and classes in VEX


00:04:27,120 --> 00:04:28,780
is not only really involved


00:04:28,780 --> 00:04:31,020
in terms of how much
code you have to write,


00:04:31,460 --> 00:04:33,380
it's also really slow to perform.


00:04:34,280 --> 00:04:36,520
Fortunately, we have Attribute Interpolate


00:04:36,520 --> 00:04:38,920
which interpolates attributes very efficiently


00:04:38,980 --> 00:04:40,060
and hassle-free,


00:04:40,060 --> 00:04:42,460
as long as you keep track
of the source elements


00:04:42,760 --> 00:04:43,800
that influence each


00:04:43,820 --> 00:04:46,800
element's attribute values, and their weights.


00:04:47,940 --> 00:04:53,060
And that brings up the linear interpolation,
i.e. lerp vs weights.


00:04:53,960 --> 00:04:56,080
If you interpolate attributes linearly in


00:04:56,080 --> 00:04:57,620
VEX using lerp,


00:04:57,620 --> 00:04:59,420
you think of it differently


00:04:59,430 --> 00:05:01,300
than the weights we have to compute


00:05:01,300 --> 00:05:02,980
and store as attributes.


00:05:04,080 --> 00:05:07,140
For every new point that we create
on an existing edge,


00:05:07,140 --> 00:05:09,460
we have 2 points we have to use as the


00:05:09,460 --> 00:05:10,860
source for interpolation.


00:05:11,160 --> 00:05:15,060
Imagine these end points as pt0 and pt1.


00:05:20,500 --> 00:05:22,520
We create a new point as p.


00:05:25,200 --> 00:05:26,840
If it's right in the center,


00:05:26,840 --> 00:05:29,140
lerp would use a value of 0.5


00:05:29,140 --> 00:05:30,920
as the amount of interpolation


00:05:31,240 --> 00:05:33,639
between pt0 and pt1.


00:05:34,420 --> 00:05:37,380
When it comes to interpolation
weights we have to use,


00:05:37,380 --> 00:05:39,360
they are also 0.5 in this case


00:05:39,360 --> 00:05:43,360
for each endpoint,
pt0 and pt1 respectively.


00:05:44,200 --> 00:05:48,040
Let's say the new p is at 33 percent 
along the edge.


00:05:48,320 --> 00:05:49,400
In this case,


00:05:49,400 --> 00:05:52,580
the lerp amount would be 0.33.


00:05:53,380 --> 00:05:57,940
But the weight values would be 
0.66 and 0.33,


00:05:57,940 --> 00:06:02,560
because p is closer to pt0 
and so it has a higher weight.


00:06:02,760 --> 00:06:07,520
With lerp 0 means the first point 
fully i.e. pt0,


00:06:07,520 --> 00:06:11,440
whereas 1 means the last 
point fully i.e. pt1.


00:06:12,420 --> 00:06:15,040
With weights, if a weight value is 1,


00:06:15,040 --> 00:06:17,500
that means the point that has
that weight value


00:06:17,500 --> 00:06:19,640
would be fully influencing the result.


00:06:20,480 --> 00:06:22,760
So the closer p is to an end point,


00:06:22,760 --> 00:06:26,020
the higher the weight value of 
that end point would be.


00:06:26,740 --> 00:06:28,180
Another way to look at it,


00:06:28,180 --> 00:06:29,800
lerp and weights


00:06:29,800 --> 00:06:31,560
have a complementary relationship.


00:06:31,980 --> 00:06:34,080
So let's say lerp amount is f,


00:06:34,080 --> 00:06:36,860
and we can form a relationship
for the weights,


00:06:36,860 --> 00:06:39,920
where the first weight would be 1 minus f,


00:06:39,920 --> 00:06:42,500
and the second weight would be f.


00:06:43,080 --> 00:06:45,540
Doing so will do a proper 
interpolation of the


00:06:45,540 --> 00:06:47,540
new edge points that we will create.


00:06:48,800 --> 00:06:51,320
Let's see how to implement it in Houdini now.






----------------------------------------------------------------------------------------------------

73 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - Converting Edges to Primitives

----------------------------------------------------------------------------------------------------



00:00:04,480 --> 00:00:05,860
Create a grid geometry.


00:00:05,860 --> 00:00:08,880
It's easier to see the result on 
a grid geometry first.


00:00:09,599 --> 00:00:11,000
I select some edges


00:00:11,000 --> 00:00:14,019
to input for the edge divide operation
we will implement.


00:00:28,640 --> 00:00:30,640
Create a subnet for the edge divide.


00:00:39,280 --> 00:00:41,840
Create a string parameter for
the group parameter


00:00:41,840 --> 00:00:45,200
and an integer parameter for
the divisions parameter.


00:00:59,600 --> 00:01:02,040
Let's first see what Group Promote gives us


00:01:02,040 --> 00:01:03,380
to convert an edge group


00:01:03,380 --> 00:01:04,480
into a primitive group.


00:01:11,360 --> 00:01:12,760
As you can see,


00:01:12,760 --> 00:01:14,380
it's treating the edge group


00:01:14,380 --> 00:01:15,800
just like a point group,


00:01:15,800 --> 00:01:18,060
rather than only grouping the primitives


00:01:18,060 --> 00:01:19,600
shared by that edge.


00:01:31,280 --> 00:01:33,960
Now take a look at what attribute
wrangle gives us,


00:01:33,960 --> 00:01:36,080
with its automatic group conversion.


00:01:53,360 --> 00:01:54,440
As you can see,


00:01:54,440 --> 00:01:56,620
with a simple group assignment syntax,


00:01:56,620 --> 00:01:58,400
we are getting the shared primitives


00:01:58,400 --> 00:01:59,760
from the input edge group.


00:02:00,480 --> 00:02:03,880
We didn't even have to write any code 
using half edges to do this,


00:02:04,480 --> 00:02:08,040
it's all Houdini doing this for us. 
Not for free of course.


00:02:09,020 --> 00:02:11,720
Conversion of an edge group to a 
primitive group


00:02:11,720 --> 00:02:13,400
is a scatter operation,


00:02:13,400 --> 00:02:15,580
so it's difficult to thread effectively.


00:02:15,760 --> 00:02:17,620
The easy answers are locking,


00:02:17,620 --> 00:02:19,380
which means you don't scale.


00:02:19,380 --> 00:02:22,080
Or duplicate the destination group 
and post-merge,


00:02:22,400 --> 00:02:25,760
which means your memory usage 
inflates with the number of threads.


00:02:26,800 --> 00:02:29,320
The harder solutions are pre-bucketing.


00:02:31,520 --> 00:02:34,420
In any case, we can't do better than 
this right now.


00:02:38,720 --> 00:02:39,860
As you can see,


00:02:39,860 --> 00:02:42,040
the result of the Group Promote is the same


00:02:42,040 --> 00:02:44,920
when we convert an edge group 
into a primitive group,


00:02:45,200 --> 00:02:47,000
to converting the same edge group


00:02:47,000 --> 00:02:48,520
to a point group first


00:02:48,520 --> 00:02:51,640
and then converting this group
into a primitive group.


00:02:52,320 --> 00:02:55,120
Edges have more information than
 just their points,


00:02:55,760 --> 00:02:58,940
so edge to primitive conversion should 
take this into account.


00:02:59,760 --> 00:03:01,780
We can delete the group promote nodes.


00:03:05,200 --> 00:03:07,460
We now have 42 primitives.






----------------------------------------------------------------------------------------------------

74 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - Creating New Edge Points

----------------------------------------------------------------------------------------------------



00:00:04,640 --> 00:00:06,180
Create a point wrangle node


00:00:06,180 --> 00:00:07,760
to create the new edge points.


00:00:09,440 --> 00:00:12,480
Use the same input group of the 
subnet for this wrangle.


00:00:13,580 --> 00:00:14,840
Splitting a polygon,


00:00:14,840 --> 00:00:16,800
or dividing edges of a polygon


00:00:16,800 --> 00:00:18,520
by inserting new points,


00:00:18,520 --> 00:00:21,359
involves first creating the extra 
points we need,


00:00:21,840 --> 00:00:23,660
and then creating a new primitive


00:00:23,660 --> 00:00:25,199
that will also use the new points,


00:00:25,840 --> 00:00:28,099
and then deleting the original primitives.


00:00:28,960 --> 00:00:31,120
You can also delete the original primitives


00:00:31,120 --> 00:00:33,759
before creating the new primitive or primitives.


00:00:34,800 --> 00:00:37,180
There is no magical method to subdivide,


00:00:37,180 --> 00:00:41,520
or insert new points in a primitive 
in place, at least not in VEX.


00:00:43,440 --> 00:00:46,080
Define a string variable 
for the group name,


00:00:49,840 --> 00:00:52,520
and an integer  variable for the divisions.


00:00:52,800 --> 00:00:55,020
We are matching the default 
edge divide sop,


00:00:55,020 --> 00:00:56,840
so 2 means, 1 split.


00:00:57,440 --> 00:00:59,380
Define a variable for the fraction


00:00:59,380 --> 00:01:00,580
using the divisions.


00:01:01,360 --> 00:01:03,220
We will create all the new points


00:01:03,220 --> 00:01:05,140
at the origin, for performance.


00:01:10,000 --> 00:01:12,000
Loop over the point neighbours.


00:01:16,479 --> 00:01:18,700
We check if the current point number


00:01:18,700 --> 00:01:21,040
is less than the current neighbour point number


00:01:21,040 --> 00:01:23,199
to avoid doubling up of new points.


00:01:24,159 --> 00:01:26,120
Then we use the inedgegroup function


00:01:26,120 --> 00:01:28,800
to check if the edge between the current point


00:01:28,800 --> 00:01:30,360
and the current neighbour point


00:01:30,360 --> 00:01:32,420
is inside the input edge group.


00:01:32,880 --> 00:01:35,080
If the edge is inside the input edge group,


00:01:35,080 --> 00:01:37,120
then we proceed to create the new points.


00:01:38,159 --> 00:01:39,980
Note that the ingroup functions


00:01:39,980 --> 00:01:42,478
re-parse the group for every 1k points,


00:01:43,360 --> 00:01:44,620
so this is thus


00:01:44,620 --> 00:01:46,200
building the input edge group


00:01:46,200 --> 00:01:47,519
for every 1000 points.


00:01:48,399 --> 00:01:49,860
The cost of building this


00:01:49,860 --> 00:01:52,559
is quite large as edge groups are ordered,


00:01:53,520 --> 00:01:55,520
so it has to actually build a giant list.


00:01:56,000 --> 00:01:59,160
And then the actual cost to look up the value in the group


00:01:59,160 --> 00:02:00,640
isn't constant either,


00:02:00,640 --> 00:02:02,980
but likely at least log(n).


00:02:02,980 --> 00:02:06,460
So we are at O(n^2 log(n)) for this.


00:02:07,040 --> 00:02:08,920
So try to reduce the number of calls


00:02:08,920 --> 00:02:10,920
to these functions as much as you can,


00:02:11,600 --> 00:02:12,700
and that's also why


00:02:12,700 --> 00:02:14,920
we perform the point number comparisons


00:02:14,920 --> 00:02:16,980
before the inedgegroup function call,


00:02:17,440 --> 00:02:20,200
so take advantage of the 
short-circuit evaluation,


00:02:20,880 --> 00:02:22,400
so inedgegroup function


00:02:22,400 --> 00:02:24,980
will only be called if the first statement


00:02:24,980 --> 00:02:25,840
evaluates true.


00:02:28,480 --> 00:02:30,780
Because a division of 2 means 1 split,


00:02:30,780 --> 00:02:33,080
we start the for loop index at 1,


00:02:33,080 --> 00:02:33,920
instead of 0.


00:02:35,200 --> 00:02:38,080
Now create a new point at the 
origin inside this loop.


00:02:45,120 --> 00:02:47,599
As you can see, 
we have 43 new points.


00:02:48,720 --> 00:02:51,599
Define the weight value as s times i.


00:02:52,720 --> 00:02:54,480
Every new point we create


00:02:54,480 --> 00:02:57,359
will have 2 endpoints that make 
up the original edge,


00:02:57,680 --> 00:03:00,320
that we will pass to Attribute 
Interpolate SOP


00:03:00,320 --> 00:03:02,239
to interpolate point attributes.


00:03:03,360 --> 00:03:05,820
There will also be 2 matching weight values


00:03:05,820 --> 00:03:07,839
as we have seen in the concept drawing,


00:03:08,720 --> 00:03:11,860
1 - w and w respectively.


00:03:34,240 --> 00:03:35,240
As you can see,


00:03:35,240 --> 00:03:38,820
the new points have a pair 
of point indices and weight values.


00:03:39,780 --> 00:03:41,780
Create attribute interpolate


00:03:49,200 --> 00:03:51,120
and set Interpolate By


00:03:51,120 --> 00:03:52,780
to Point Numbers and Weights.


00:03:53,320 --> 00:03:55,920
Use the array point attributes that 
we have created.


00:04:01,700 --> 00:04:02,740
As you can see


00:04:02,740 --> 00:04:05,220
the new points are positioned 
along the edges.


00:04:05,840 --> 00:04:07,860
But we also moved the original points.


00:04:07,860 --> 00:04:10,660
So create a point group inside 
the same wrangle


00:04:10,720 --> 00:04:11,860
to group the new points.


00:04:25,520 --> 00:04:26,920
Now use this point group


00:04:26,920 --> 00:04:28,560
for the attribute interpolate node.


00:04:30,080 --> 00:04:31,220
As you can see,


00:04:31,220 --> 00:04:32,480
we got the points


00:04:32,480 --> 00:04:33,759
perfectly where we wanted.


00:04:34,880 --> 00:04:37,200
Make sure to exclude sourcept_indices


00:04:37,200 --> 00:04:39,300
and sourcept_weights attributes


00:04:39,300 --> 00:04:41,359
from the list of attributes to interpolate,


00:04:41,680 --> 00:04:43,400
to speed up the operation a bit,


00:04:43,400 --> 00:04:46,080
as we don't need these attributes 
to be interpolated.


00:04:46,880 --> 00:04:49,580
We only need them to interpolate 
other point attributes


00:04:49,580 --> 00:04:50,960
in the input geometry.


00:04:51,840 --> 00:04:55,680
Creating and interpolating the points 
is rather straightforward,


00:04:56,420 --> 00:04:58,560
but we need a way to track 
these new points


00:04:58,560 --> 00:04:59,840
in the successive steps.


00:05:01,919 --> 00:05:04,820
One way we can uniquely identify 
these points


00:05:04,820 --> 00:05:08,080
is by using the point numbers of 
the edge they belong to.


00:05:08,800 --> 00:05:11,660
We have to make sure to sort 
them by the point number


00:05:11,840 --> 00:05:14,540
so the first number is lower than 
the second point number,


00:05:14,540 --> 00:05:15,800
to find them easier.


00:05:16,400 --> 00:05:19,280
To be able to do fast look up 
of attribute values,


00:05:19,280 --> 00:05:22,040
we can only use integer 
or string attributes.


00:05:22,640 --> 00:05:24,580
Because we have 2 point attributes


00:05:24,580 --> 00:05:26,180
we have to look up as a pair,


00:05:26,180 --> 00:05:27,860
we have to use string attributes.


00:05:32,400 --> 00:05:34,660
We now have the edgepts attribute.


00:05:34,660 --> 00:05:36,760
They might be in the same order as


00:05:36,760 --> 00:05:38,520
sourcept_indices attribute


00:05:38,520 --> 00:05:39,900
but it's not guaranteed.


00:05:43,840 --> 00:05:45,520
As we increase the divisions,


00:05:45,520 --> 00:05:47,700
we are getting the appropriate 
weight values


00:05:47,700 --> 00:05:49,240
for each new edge point,


00:05:49,240 --> 00:05:51,840
depending on where the point is 
along the edge.


00:06:15,360 --> 00:06:17,340
Before we create the new polygons,


00:06:17,600 --> 00:06:20,100
we have to consider the closed 
state of the polygon.


00:06:21,920 --> 00:06:23,920
Let's see the concept of it first.






----------------------------------------------------------------------------------------------------

75 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - Rebuilding Polygons - Concept

----------------------------------------------------------------------------------------------------



00:00:04,880 --> 00:00:06,702
You can have 2 polygons like this.


00:00:07,714 --> 00:00:09,707
Oneis closed and the other one is open.


00:00:18,080 --> 00:00:19,179
As you can see,


00:00:19,179 --> 00:00:21,020
just calling primpoints function


00:00:21,020 --> 00:00:23,280
will return the same points 
for both polygons.


00:00:23,920 --> 00:00:26,270
But we need to differentiate 
betweenthese


00:00:26,270 --> 00:00:27,624
so that we can decide


00:00:27,624 --> 00:00:30,560
whether to create new points 
on the last edge of the polygon.


00:00:31,440 --> 00:00:33,007
If the polygon is closed,


00:00:33,007 --> 00:00:34,889
we will just appendthe first point


00:00:34,889 --> 00:00:37,211
at the end of the same array, 
temporarily,


00:00:37,333 --> 00:00:38,602
to make the code simpler.


00:00:40,400 --> 00:00:42,124
After we create the new points,


00:00:42,124 --> 00:00:44,080
we willdelete the last element 
from the array.


00:00:45,120 --> 00:00:47,219
Because when we pass a points array


00:00:47,219 --> 00:00:48,390
to addprimfunction,


00:00:48,390 --> 00:00:51,445
it's not the repetition of the last 
point that defines whether


00:00:51,445 --> 00:00:52,499
the polygon isclosed,


00:00:52,858 --> 00:00:56,661
rather it's another argument 
where you specify this.


00:00:56,661 --> 00:01:00,519
That's why we don't want to add 
the same point to the new polygon twice.


00:01:04,480 --> 00:01:06,701
Let's implement it in Houdini now.






----------------------------------------------------------------------------------------------------

76 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - Rebuilding Polygons - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,880 --> 00:00:07,203
Make sure to exclude the edgepts attribute


00:00:07,203 --> 00:00:09,200
from the Attribute Interpolate node.


00:00:10,400 --> 00:00:12,800
Let's see if the interpolation 
is indeed working.


00:00:13,760 --> 00:00:17,680
Create a point color attribute on
the geometry using its bounding box.


00:00:25,920 --> 00:00:29,840
As you can see, the new points
have the correct colors.


00:00:32,023 --> 00:00:34,302
If you exclude the color attribute,


00:00:34,302 --> 00:00:37,103
youcan see the new points 
lose their colors.


00:00:44,080 --> 00:00:46,704
Don't worry about the distinct 
look ofthe new points,


00:00:46,704 --> 00:00:49,287
as Houdini is drawing them slightly different


00:00:49,287 --> 00:00:51,014
because they are isolated,


00:00:51,014 --> 00:00:54,180
i.e. they don't belong to any primitives yet.


00:00:58,328 --> 00:00:59,806
Create a new primitive wrangle,


00:00:59,806 --> 00:01:03,045
but this timeuse the edge primitives 
group we have created


00:01:03,045 --> 00:01:04,417
in the beginning of this lesson.


00:01:04,960 --> 00:01:06,379
Because only the primitives


00:01:06,379 --> 00:01:09,280
who share an edgewith the input
edge group will be rebuilt.


00:01:10,320 --> 00:01:12,640
First store the array of primitive points.


00:01:16,720 --> 00:01:20,960
Use the closed primitive intrinsic, to
decide whether to append the first point,


00:01:20,960 --> 00:01:22,880
to the end of the pts array.


00:01:22,880 --> 00:01:25,440
If not, we decrement count by 1.


00:01:26,640 --> 00:01:30,480
Because we will loop over the pts
array, in the form of point pairs,


00:01:30,480 --> 00:01:35,600
if the primitive is open, we don't want to
include the last edge, as it doesn't exist.


00:01:35,600 --> 00:01:37,220
So if the primitive is closed,


00:01:37,220 --> 00:01:40,394
the old count will
allow us to read until the last point


00:01:40,394 --> 00:01:42,004
before our last append,


00:01:42,004 --> 00:01:44,664
then only read the very last 
pointas the point


00:01:44,664 --> 00:01:46,200
next to the previous last point.


00:01:47,200 --> 00:01:50,146
Copy the pts array as newpts.


00:01:50,146 --> 00:01:52,737
We don'twant to modify 
the original pts array


00:01:52,737 --> 00:01:55,117
as we insert our existing edgepoints


00:01:55,117 --> 00:01:56,480
we have created previously.


00:01:59,840 --> 00:02:01,583
Remove the original primitive,


00:02:01,583 --> 00:02:03,840
butexclude its points from the deletion.


00:02:05,040 --> 00:02:08,560
We can specify the new 
primitive type name


00:02:08,560 --> 00:02:09,535
based on the closed state we have stored.


00:02:09,535 --> 00:02:14,189
If it's closed, then it will
be "poly", else it will be "polyline".


00:02:18,080 --> 00:02:22,474
Declare the new primitive index, and
an array variable for the new vertices.


00:02:22,960 --> 00:02:25,575
We will use an overload of 
the addprim function,


00:02:25,575 --> 00:02:27,520
that returns the array of the new vertices.


00:02:28,400 --> 00:02:32,400
We need to create similar index and
weight attributes for these new vertices


00:02:32,400 --> 00:02:34,560
so they can be interpolated properly.


00:02:35,200 --> 00:02:37,440
For now, just call the addprim function.


00:02:39,600 --> 00:02:42,521
As you can see we now 
have the new polygons.


00:02:42,761 --> 00:02:45,691
But they are exactly the same 
as the original polygons,


00:02:45,691 --> 00:02:47,554
except the primitive attributes,


00:02:47,554 --> 00:02:49,808
we don't inherit them automatically.


00:02:49,808 --> 00:02:54,375
Loop over the newpts array, 
and form the same edgepts value


00:02:54,375 --> 00:02:57,926
so we can recoverthe new points 
we have created previously.


00:03:14,880 --> 00:03:19,280
We have to get the array of indices
because for divisions greater than 2,


00:03:19,280 --> 00:03:23,360
there will be multiple matches, in the
same order as we have created them.


00:03:24,000 --> 00:03:27,537
I am gonna store the pts array
on the original primitive.


00:03:54,640 --> 00:03:56,880
As you can see the order is clockwise.


00:03:58,880 --> 00:04:00,490
If the primitive is closed,


00:04:00,490 --> 00:04:04,080
remove the lastelement from 
the newpts array after the loop.


00:04:13,680 --> 00:04:15,022
If there are any matches,


00:04:15,022 --> 00:04:18,245
we want to insert theseinto the newpts array


00:04:18,245 --> 00:04:19,937
at the correct location.


00:04:28,560 --> 00:04:30,800
As you can see the order is not correct.


00:04:31,680 --> 00:04:34,658
We have to take into account 
where we arecurrently in the loop,


00:04:34,658 --> 00:04:38,149
as well as how many elements 
we have inserted previously,


00:04:38,149 --> 00:04:41,840
so thatthe current array 
location is offset properly.


00:05:15,040 --> 00:05:18,594
As you can see, the order of
the points is perfect now.


00:05:36,720 --> 00:05:39,543
But watch what happens when
we have more divisions.


00:05:51,920 --> 00:05:56,000
The first polygon has the points
correctly ordered until the second edge.


00:05:56,720 --> 00:05:59,440
In the third edge, they are 
inserted backwards.


00:06:00,240 --> 00:06:04,080
That's because we are creating the
new points starting from the point


00:06:04,080 --> 00:06:08,406
with the smaller point number, towards
the point with the larger point number.


00:06:09,200 --> 00:06:11,824
But the way the points are 
ordered in a primitive,


00:06:11,824 --> 00:06:13,814
is not guaranteed to respect this,


00:06:13,814 --> 00:06:16,621
and as such primpoints returns the points


00:06:16,621 --> 00:06:18,758
where some of themmight 
oppose the direction


00:06:18,758 --> 00:06:19,943
we have created them.


00:06:20,880 --> 00:06:21,760
That's ok.


00:06:22,400 --> 00:06:27,137
All we have to do is, to check if the
first point we stored in sourcept_indices


00:06:27,520 --> 00:06:29,600
is the same as the current point in the loop.


00:06:30,480 --> 00:06:34,640
If it's not, then the only possibility
is that they are opposing each other,


00:06:35,360 --> 00:06:38,230
and as such we have to reverse 
the indices array


00:06:38,230 --> 00:06:41,446
before inserting them into 
the newpts array.


00:07:41,920 --> 00:07:43,098
As you can see,


00:07:43,098 --> 00:07:45,545
the points are now correctlyordered.


00:07:45,545 --> 00:07:47,760
So we have successfully fixed this issue.


00:07:50,560 --> 00:07:53,863
Now we can remove the original
primitives and create the new ones.


00:08:26,800 --> 00:08:29,093
We have perfectly created the new primitives


00:08:29,093 --> 00:08:30,848
that use the new edge points.


00:08:31,568 --> 00:08:34,628
So all we have left to do, 
is to preserve the primitive,


00:08:34,628 --> 00:08:37,406
vertex attributes and group memberships.






----------------------------------------------------------------------------------------------------

77 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - Preserving & Interpolating Attributes

----------------------------------------------------------------------------------------------------



00:00:05,040 --> 00:00:07,206
We can leave the vertex 
attributes the last,


00:00:07,206 --> 00:00:09,396
they are always more tricky to set up.


00:00:09,396 --> 00:00:12,160
So change the color attribute
to a primitive attribute.


00:00:14,080 --> 00:00:17,440
As you can see, the new primitives
do not have the correct colors.


00:00:18,000 --> 00:00:20,960
Considering we end up with the same
number of primitives as before,


00:00:21,520 --> 00:00:23,920
we can just copy them from
the original primitives.


00:00:24,480 --> 00:00:28,560
But we have to make sure to match them
by original primitive indices as the


00:00:28,560 --> 00:00:31,600
new primitive indices won't
be the same as the old ones.


00:00:34,480 --> 00:00:35,591
As you can see,


00:00:35,591 --> 00:00:37,752
we also lost the edge_prims group.


00:00:37,752 --> 00:00:40,331
We need to add the new 
primitives to this group.


00:00:47,600 --> 00:00:48,880
Now we have recovered them.


00:00:49,600 --> 00:00:52,189
To copy elements by an ID attribute,


00:00:52,189 --> 00:00:54,560
we alsoneed the same attributes
on the other side.


00:00:55,280 --> 00:00:59,834
So create a new wrangle and store the
primitive number as sourceprim_index,


00:01:03,760 --> 00:01:05,680
and make sure to use the edge_prims group.


00:01:08,960 --> 00:01:12,320
Now create an attribute copy
and use the edge_prims group


00:01:12,320 --> 00:01:16,561
for both groups and set Attribute
to Match to sourceprim_index.


00:01:33,040 --> 00:01:36,729
Use star in the Attribute Name field
to match all the primitive attributes.


00:01:41,200 --> 00:01:43,661
As you can see, 
we have the correct colors.


00:01:54,149 --> 00:01:57,442
Switch the color attribute
to a vertex attribute now.


00:02:00,770 --> 00:02:04,400
As expected we get the default
color values for the new vertices.


00:02:05,760 --> 00:02:07,840
This will be a bit more involved to set up.


00:02:07,840 --> 00:02:12,129
So first store the original primitive
vertices using the primvertices function.


00:02:15,440 --> 00:02:20,160
Append the first vertex to the end of
the vtx array, just like the pts array.


00:02:20,880 --> 00:02:23,760
Make a copy of this array,
just like the pts array.


00:02:29,121 --> 00:02:32,644
Now create an integer array 
to store thevertex indices,


00:02:32,776 --> 00:02:34,436
and another integer array


00:02:34,436 --> 00:02:36,245
to store the weight indices,


00:02:36,245 --> 00:02:39,360
that we willuse when interpolating 
vertex attributes.


00:02:40,320 --> 00:02:42,703
This will be a bit different than points.


00:02:42,703 --> 00:02:46,186
As yourecall, when we 
interpolated point attributes,


00:02:46,186 --> 00:02:48,623
we only did it for the new points,


00:02:48,623 --> 00:02:53,127
where each newpoint had 2 end 
points to source attributes from.


00:02:53,920 --> 00:02:55,713
In the case of vertices,


00:02:55,713 --> 00:02:59,676
all vertices will bebrand new, 
even the old ones.


00:02:59,676 --> 00:03:02,375
Because we can't hold 
onto previous vertices,


00:03:02,375 --> 00:03:05,840
once we deletedthe original 
primitive holding the vertices.


00:03:06,480 --> 00:03:10,200
And as such, we need to handle
the original vertices differently


00:03:10,480 --> 00:03:13,120
than the vertices of the
new points that we created.


00:03:14,320 --> 00:03:15,975
For these "new" vertices,


00:03:15,975 --> 00:03:18,011
i.e. the edgevertices,


00:03:18,011 --> 00:03:20,981
we want to mark them with -1 as index


00:03:21,455 --> 00:03:24,445
so that we can distinguish them 
from the"original" vertices,


00:03:24,445 --> 00:03:26,792
i.e. the corner vertices.


00:03:27,920 --> 00:03:29,147
As you might have guesses,


00:03:29,147 --> 00:03:32,502
these edge verticeswill have 
the same quantity as the new points


00:03:32,502 --> 00:03:34,567
that are stored in the indices array.


00:03:35,440 --> 00:03:38,798
So just like how we insert the 
indices of the newpoints,


00:03:38,798 --> 00:03:40,260
into the newpts array,


00:03:40,260 --> 00:03:43,335
we will insert the indices 
of the new vertices,


00:03:43,335 --> 00:03:45,786
into the newvtxarray.


00:03:45,786 --> 00:03:48,400
Except these values will all be -1.


00:03:49,200 --> 00:03:50,480
This is not all.


00:03:50,480 --> 00:03:53,122
The corner vertices will have
a 1 to 1 match


00:03:53,122 --> 00:03:55,920
withtheir corresponding vertex 
in the newvtx array.


00:03:56,880 --> 00:04:00,726
But the edge vertices will need 
2 vertices tointerpolate from.


00:04:01,807 --> 00:04:05,203
The issue is, we don't know the 
indices of these vertices yet.


00:04:05,203 --> 00:04:08,529
They willbe revealed 
after we create a new primitive.


00:04:10,320 --> 00:04:13,840
So for now, append the current
loop index to the vtxindices.


00:04:14,560 --> 00:04:16,880
Once we know the actual vertex numbers,


00:04:16,880 --> 00:04:18,476
we can use these values


00:04:18,476 --> 00:04:20,727
to index into the newvertices array


00:04:20,727 --> 00:04:22,571
returned by the addprim function.


00:04:23,760 --> 00:04:27,760
The end points will be this
point plus the next in the array,


00:04:27,760 --> 00:04:30,190
so we just need to know where 
we were in the loop.


00:04:31,200 --> 00:04:35,440
Also remember, we need to store these
indices from the original geometry,


00:04:36,400 --> 00:04:40,320
so the new vertices will need to
reference the indices of the old vertices.


00:04:41,040 --> 00:04:44,121
That means when we do the 
look up, we will


00:04:44,121 --> 00:04:46,960
use the vtx array, not the
 new vtx array.


00:04:48,480 --> 00:04:50,004
For the vertex weights,


00:04:50,004 --> 00:04:51,627
We canuse the exact same weights


00:04:51,627 --> 00:04:54,240
as the point weights, 
for the edge vertices.


00:04:55,120 --> 00:04:58,240
So if an edge vertex points to 
the edge point 55,


00:04:58,240 --> 00:05:01,120
then it can use the weights 
values of point 55.


00:05:02,240 --> 00:05:06,240
Also make sure to remove the last element
from the new_vtx array after the loop.


00:05:07,360 --> 00:05:10,720
Now loop over the new vertices array
returned from the addprim function.


00:05:14,000 --> 00:05:17,200
First create an integer array
for the source vertex indices,


00:05:18,400 --> 00:05:21,440
and a float array for source vertex weights.


00:05:21,440 --> 00:05:25,113
These are the vertices we will
use to store these values.


00:05:44,880 --> 00:05:48,369
Also make sure to create a
vertex group for these vertices.


00:06:00,080 --> 00:06:02,320
As you can see, 
we have the vertex group now.


00:06:02,880 --> 00:06:05,840
Now let's check the elements
in the new_vtx array.


00:06:06,720 --> 00:06:10,525
If the current item in the 
new_vtx array isnot -1,


00:06:10,525 --> 00:06:13,438
we have a corner vertex, and as such,


00:06:13,760 --> 00:06:18,285
we assign the value of the current item
to the source_vertex_indices array,


00:06:23,040 --> 00:06:27,840
and set the weight value to 1, as the new
corner vertex attributes will be driven


00:06:27,840 --> 00:06:29,920
entirely by the original corner vertex.


00:06:30,800 --> 00:06:35,120
As you can see, the new corner
vertices have a single vertex number


00:06:35,120 --> 00:06:38,240
that points to the original vertex,
and the weight values are 1.


00:06:39,360 --> 00:06:42,150
Take a look at the vertex numbers
in the viewport.


00:06:58,560 --> 00:07:02,880
If you look at primitive number 39,
you can see that its corner vertices


00:07:02,880 --> 00:07:06,160
have proper source indices,
except the last 2 vertices.


00:07:07,040 --> 00:07:08,487
We expect the vertex,


00:07:08,487 --> 00:07:11,660
one before last, tohave an 
empty source vertex indices array,


00:07:11,660 --> 00:07:13,173
but not the last vertex.


00:07:13,840 --> 00:07:15,840
We will see why that is, very soon.


00:07:16,720 --> 00:07:18,651
Now let's handle the edge vertices. 


00:07:21,600 --> 00:07:25,120
As you recall this array only
includes the edge vertices,


00:07:25,120 --> 00:07:28,240
and as such we can't just use the i variable.


00:07:28,240 --> 00:07:30,960
So we have to keep track of the
current index in this array.


00:07:32,160 --> 00:07:36,080
Use this index variable, to
index into vertex_indices array,


00:07:36,080 --> 00:07:38,837
which will give us the index 
of the firstcorner vertex


00:07:38,837 --> 00:07:40,765
where this edge vertex belongs to.


00:07:41,440 --> 00:07:43,495
To get the next corner vertex,


00:07:43,495 --> 00:07:47,146
just incrementthe result we got
from the vertex_indices array.


00:07:48,160 --> 00:07:52,240
Now use the index variable to index
into the weight indices array,


00:07:52,240 --> 00:07:54,280
which will give us the index of the point


00:07:54,280 --> 00:07:55,869
thathas the current edge vertex.


00:07:58,126 --> 00:08:02,217
Using this point, get the source_pt
weights attribute value,


00:08:02,217 --> 00:08:05,126
andassign this to the source 
vertex weights array.


00:08:20,960 --> 00:08:24,059
Now you can see why we missed 
the last vertex.


00:08:24,059 --> 00:08:26,400
Because we are still using the pts count.


00:08:27,200 --> 00:08:31,280
We have to update the count variable
using the length of the new_vtx array.


00:08:34,800 --> 00:08:37,983
Doing so immediately fixes
the issue we had before.


00:08:39,280 --> 00:08:42,880
As you can see, we now have
proper source vertex indices


00:08:42,880 --> 00:08:44,736
for both the corner vertices


00:08:44,736 --> 00:08:47,581
and theedge vertices, as well 
as their weights.


00:08:54,480 --> 00:08:59,020
So create another attribute interpolate
and use these source array attributes.


00:09:12,240 --> 00:09:14,567
Make sure to exclude them
from the interpolation.


00:09:20,320 --> 00:09:22,400
And use the new_vtx edge divide group.


00:09:28,640 --> 00:09:32,320
As you can see, we perfectly
interpolated the vertex attributes.


00:09:33,200 --> 00:09:36,347
Increasing the divisions, 
the result stays thesame.


00:09:36,347 --> 00:09:40,113
Except, you might notice some
 gaps in the drawn wireframe.


00:09:40,531 --> 00:09:43,135
But don't worry this is 
just aviewport artifact,


00:09:43,135 --> 00:09:44,948
not an issue in the topology.


00:09:48,160 --> 00:09:52,560
Let's test the same operation on
open polygons, i.e. polylines.


00:09:58,560 --> 00:10:02,361
As you can see, the operator
we created, works here as well.


00:10:10,960 --> 00:10:13,079
Now make sure to delete 
all theattributes


00:10:13,079 --> 00:10:14,862
and the groups that we created.






----------------------------------------------------------------------------------------------------

78 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - Multithreading by Connectivity

----------------------------------------------------------------------------------------------------



00:00:06,063 --> 00:00:08,531
We can also add some parallelism
to this operator


00:00:08,864 --> 00:00:11,483
by looping over each connected
piece of geometry.


00:00:12,770 --> 00:00:14,532
Just rename the connectivity attribute


00:00:14,820 --> 00:00:18,145
so we don't run into naming
collisions with an existing attribute.


00:00:26,780 --> 00:00:30,276
Make a copy of the grid,
so we can try the same operation on both.


00:00:43,125 --> 00:00:45,096
As you can see, it errors out.


00:00:46,000 --> 00:00:49,179
This is because Attribute
Interpolate fails immediately


00:00:49,179 --> 00:00:50,925
if the input group is not found.


00:00:52,204 --> 00:00:54,860
So we need to make sure there
is a newpts group,


00:00:55,097 --> 00:00:58,508
which is only created if the input
edge group has any edges.


00:00:59,366 --> 00:01:01,383
In the case of the second grid geometry,


00:01:01,794 --> 00:01:04,541
it has no edges that exist in
the input edge group.


00:01:05,320 --> 00:01:08,070
But instead of checking the
edge count inside the input group,


00:01:08,219 --> 00:01:12,559
which is only possible in Python,
it would be extremely costly for us


00:01:12,559 --> 00:01:13,860
with regards to performance.


00:01:14,771 --> 00:01:19,085
So instead we can check another group
that we create, using the input edge group,


00:01:19,085 --> 00:01:21,390
and that group is edgeprims group.


00:01:22,336 --> 00:01:24,714
So use the nprimsgroup expression to check


00:01:24,714 --> 00:01:27,188
the number of elements in the
edgeprims group.


00:01:28,000 --> 00:01:32,220
Make sure to point the spare parameter to
the wrangle node that creates this group.


00:01:32,947 --> 00:01:35,394
So if the number of primitives in
this group is 0,


00:01:35,834 --> 00:01:38,895
that means, we have no edges in
the current piece of geometry,


00:01:38,895 --> 00:01:40,909
that exist in the input edge group,


00:01:41,497 --> 00:01:44,425
and as such, we will just skip all the calculations


00:01:44,592 --> 00:01:46,862
and pass the current piece of
geometry, as is,


00:01:47,370 --> 00:01:49,316
as the result of the current loop iteration.


00:01:50,421 --> 00:01:53,525
As you can see, the edge divide
operator now works perfectly.


00:01:54,265 --> 00:01:56,934
Also make sure to delete the
connectivity attribute in the end.


00:02:04,561 --> 00:02:06,554
We can also compile the entire network


00:02:06,554 --> 00:02:08,745
that could potentially give
us some improvements


00:02:08,920 --> 00:02:11,340
over the non-compilable edge divide SOP.






----------------------------------------------------------------------------------------------------

79 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - VEX vs C++

----------------------------------------------------------------------------------------------------



00:00:12,720 --> 00:00:14,983
Now let's try it on a more interestinggeometry


00:00:14,983 --> 00:00:17,038
and compare it with the default edge divide.


00:00:17,602 --> 00:00:19,888
Let's see if a VEX based edgedivide


00:00:19,888 --> 00:00:22,160
will surpass the C++ based edge divide.


00:00:23,120 --> 00:00:25,274
The default edge divide is pretty old,


00:00:25,274 --> 00:00:28,795
and we are using pretty modern 
workflows for geometry creation


00:00:28,795 --> 00:00:30,378
and attributeinterpolation.


00:00:31,353 --> 00:00:32,428
We have to put star


00:00:32,428 --> 00:00:34,592
in the group field of both operators


00:00:34,592 --> 00:00:36,414
to denote the entire geometry.


00:00:49,920 --> 00:00:52,960
As you can see we end up with
the same number of points,


00:00:52,960 --> 00:00:55,920
primitives and vertices 
using both operators.


00:00:56,720 --> 00:00:59,055
But the performance difference 
is too great.


00:00:59,440 --> 00:01:04,376
It seems like the C++ edge divide is 
24 times faster than the VEX version.


00:01:05,005 --> 00:01:08,590
How couldthis be?
We need to investigate deeper.


00:01:09,040 --> 00:01:11,120
So first turn off the compile option


00:01:11,120 --> 00:01:13,120
sowe can see what's taking
the most time.


00:01:13,760 --> 00:01:16,205
As you can see,
creating a new pointwrangle


00:01:16,205 --> 00:01:18,234
 is taking the vast majority of the time.


00:01:28,317 --> 00:01:30,717
Now you are about to see
something unimaginable.


00:01:31,280 --> 00:01:33,280
Just remove the star character
from the group field.


00:01:37,280 --> 00:01:40,164
The whole operation just became
12 timesfaster.


00:01:40,728 --> 00:01:41,705
Why is that?


00:01:42,487 --> 00:01:44,990
There is a shortcut in the 
group parsing code


00:01:44,990 --> 00:01:46,745
that checks for emptystring


00:01:46,745 --> 00:01:48,752
to immediately select all elements.


00:01:49,280 --> 00:01:51,493
This is particularly very important


00:01:51,493 --> 00:01:54,485
as edgesare so much slower 
to build and query.


00:01:55,101 --> 00:01:58,612
The ingroup functions re-parse 
the group for every 1000points.


00:01:58,612 --> 00:02:02,024
That's why we have to check for
star in the group field,


00:02:02,024 --> 00:02:05,502
and if found, to replace it with
an empty string,


00:02:05,502 --> 00:02:07,994
to immediately select all edges.


00:02:27,120 --> 00:02:29,055
So this is a more accurate benchmark,


00:02:29,055 --> 00:02:31,567
butof course we should handle 
this ourselves


00:02:31,567 --> 00:02:33,364
so the users of this operator


00:02:33,364 --> 00:02:35,691
do not fall into this trap unknowingly.


00:02:36,257 --> 00:02:39,687
Now our VEX based edge
divide operator is 2 times slower,


00:02:39,687 --> 00:02:43,845
rather than the previous 24 times.
So this is much more acceptable.


00:02:44,480 --> 00:02:47,173
And I am sure there are some cases 
where the VEXapproach


00:02:47,173 --> 00:02:50,097
will surpass the performance of 
the C++ edge divide.


00:02:50,635 --> 00:02:53,752
So now even if we use the star
character, we are fine.


00:02:54,380 --> 00:02:57,161
Though there is another place,
we need to check for this,


00:02:57,161 --> 00:02:59,715
that accountsfor the slightly
increased cook times.


00:02:59,715 --> 00:03:01,281
We will fix that soon.


00:03:02,192 --> 00:03:04,802
Now let's test a multi piecegeometry case.


00:03:04,802 --> 00:03:07,410
I copy the same geometry 10 times.


00:03:24,320 --> 00:03:27,826
As you can see the VEX version 
is slightlyfaster in this case.


00:03:28,070 --> 00:03:30,644
So 1 for 1 in terms of wins.


00:03:31,360 --> 00:03:35,440
What about interpolating multiple
attributes. Vertex attributes


00:03:35,440 --> 00:03:39,440
are often more costly to handle. So I
am just gonna create some random vertex


00:03:39,440 --> 00:03:40,210
attributes.


00:04:05,280 --> 00:04:08,774
As you can see,
we now have 100 vertex attributes.


00:04:15,434 --> 00:04:17,834
The default edge divide almost took 2 minutes.


00:04:25,200 --> 00:04:27,617
VEX operator took 13 seconds.


00:04:27,848 --> 00:04:31,511
An incredible 9times the performance
of the C++ edge divide.


00:04:37,920 --> 00:04:41,737
We have the same topology after
using each edge divide operator.


00:04:42,160 --> 00:04:46,875
There are still some parts we have to
modify, especially for performance.






----------------------------------------------------------------------------------------------------

80 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - Preserving Groups

----------------------------------------------------------------------------------------------------



00:00:05,091 --> 00:00:09,271
First create a grid geometry with
500 by 500 divisions.


00:00:09,271 --> 00:00:12,508
Profile the Edge Divide using this geometry.


00:00:16,218 --> 00:00:19,826
As you can see,
convert_edge_group_to_prim_group wrangle


00:00:19,826 --> 00:00:22,089
is taking a significant amount of time.


00:00:22,316 --> 00:00:24,906
The reason is the use of star
in the group field,


00:00:24,906 --> 00:00:26,632
just like we have seen before.


00:00:30,393 --> 00:00:33,216
If you run the same wrangle,
without the star,


00:00:33,216 --> 00:00:38,179
you can see that it takes
36 ms vs 649 ms,


00:00:38,179 --> 00:00:40,592
a staggering 18 times difference.


00:00:41,218 --> 00:00:43,499
So we also have to replace
the star here too,


00:00:43,499 --> 00:00:46,743
but this time, we will use
hscript expressions to do it.


00:01:02,089 --> 00:01:03,720
Make sure to also do it 


00:01:03,720 --> 00:01:04,463
in the group field of the


00:01:04,463 --> 00:01:06,119
CREATE NEW POINTS wrangle node.


00:01:06,472 --> 00:01:08,027
Before we replaced it,


00:01:08,027 --> 00:01:10,642
to optimize the inedgegroup VEX function,


00:01:10,642 --> 00:01:12,989
but we also have to do it for
the group parameter,


00:01:12,989 --> 00:01:17,621
so as not to incur the same performance
penalty we have experienced just now.


00:01:17,621 --> 00:01:22,156
As you can see, we have now improved
the performance significantly.


00:01:23,670 --> 00:01:27,187
Now we will create some point,
primitive and vertex groups


00:01:27,187 --> 00:01:29,693
and see if we are properly
preserving them,


00:01:29,693 --> 00:01:31,668
compared to the default Edge Divide.


00:01:31,668 --> 00:01:35,606
I will select some boundary elements
and also some elements by range,


00:01:35,606 --> 00:01:38,327
i.e. 1 out of every second element.


00:03:02,716 --> 00:03:04,796
Let's start with point groups.


00:03:05,175 --> 00:03:09,713
As you can see we have 1996
points in the point boundary group.


00:03:09,713 --> 00:03:13,727
The VEX operator has an incorrect
number of points in the original group.


00:03:15,034 --> 00:03:18,259
The default Edge Divide has the
correct number of points.


00:03:18,259 --> 00:03:21,087
When we check using a
group promote node,


00:03:21,087 --> 00:03:23,160
we can easily see that our Edge Divide


00:03:23,160 --> 00:03:26,639
is also adding the new edge
points in the original group.


00:03:26,639 --> 00:03:29,583
We have to find out which
operator causes this.


00:03:38,038 --> 00:03:40,831
As you can see, it's attribute
interpolate 1,


00:03:40,831 --> 00:03:43,774
which is the node that interpolates
the point attributes.


00:03:44,304 --> 00:03:47,424
Note that attribute interpolate
also interpolate the groups.


00:03:47,855 --> 00:03:50,193
Considering we don't delete
any original points,


00:03:50,344 --> 00:03:51,919
there is no reason to do this,


00:03:51,919 --> 00:03:54,738
nor include any new points
in the existing groups.


00:03:55,319 --> 00:03:57,912
So we have to exclude all point groups


00:03:57,912 --> 00:03:59,963
from the list of attributes to interpolate.


00:04:00,468 --> 00:04:03,298
We can do this using the
pointgrouplist expression


00:04:03,298 --> 00:04:06,209
which will return a space
separated list of point groups.


00:04:07,522 --> 00:04:11,806
We have to prefix each group with
the ^ character to denote exclusion.


00:04:42,000 --> 00:04:46,112
As you can see, we now have the
correct number of points in the point groups.


00:04:59,583 --> 00:05:01,392
Let's check the primitive groups.


00:05:08,862 --> 00:05:12,935
We have 1992 primitives in
the prim boundary group.


00:05:12,935 --> 00:05:16,982
VEX node has 0 prims but the
default Edge Divide is correct.


00:05:24,878 --> 00:05:28,810
It's the create_new_primitives wrangle
node that causes this issue.


00:05:29,062 --> 00:05:29,840
Why is that?


00:05:30,421 --> 00:05:32,215
The answer is rather simple.


00:05:32,215 --> 00:05:34,163
When we delete the original primitive,


00:05:34,163 --> 00:05:36,349
we not only lose all primitive attributes,


00:05:36,349 --> 00:05:38,510
which we copied using Attribute Copy,


00:05:38,510 --> 00:05:42,000
but we also lose all the group
memberships in the primitive groups.


00:05:42,396 --> 00:05:46,002
Unfortunately Attribute Copy
doesn't handle groups.


00:05:46,002 --> 00:05:48,572
So you might think of using
Group Copy SOP,


00:05:48,572 --> 00:05:50,225
but we can't do that either.


00:05:50,225 --> 00:05:52,978
Because the indices of the
new primitives we created,


00:05:52,978 --> 00:05:55,368
won't be the same as the
original primitives,


00:05:55,797 --> 00:05:59,791
unless the Edge Divide operation is
applied to the entire geometry.


00:06:00,170 --> 00:06:02,929
The only way to ensure this for
sub-geometry,


00:06:02,929 --> 00:06:06,000
would be to store the original
primitive number as an attribute,


00:06:06,000 --> 00:06:08,863
which we did, and sort all
the primitives


00:06:08,863 --> 00:06:10,715
using this integer attribute.


00:06:10,715 --> 00:06:12,584
We will do this later on.


00:06:12,584 --> 00:06:14,853
Now your second intuition might be


00:06:14,853 --> 00:06:16,819
to use the setprimgroup function.


00:06:16,819 --> 00:06:20,422
Again you have to consider a
variable number of primitive groups,


00:06:20,422 --> 00:06:22,299
which you can loop through in VEX


00:06:22,299 --> 00:06:25,000
and check if the current primitive
is in that group


00:06:25,000 --> 00:06:28,407
and if so, add the new primitive
to that group.


00:06:28,672 --> 00:06:31,251
But this will be very costly
performance wise.


00:06:31,467 --> 00:06:35,173
It's much faster to let Attribute
Interpolate do its thing,


00:06:35,400 --> 00:06:38,097
but this time, only for the
primitive groups.


00:06:38,728 --> 00:06:42,801
So we have to store the original
primitive number and 1 as the weight value.


00:06:43,735 --> 00:06:46,060
Store these as array primitive attributes.


00:06:46,539 --> 00:06:49,813
Unfortunately even when we
only need a single element,


00:06:49,813 --> 00:06:53,084
Attribute Interpolate still requires
an array attribute


00:06:53,084 --> 00:06:54,377
to do the interpolation.


00:07:04,976 --> 00:07:06,893
Create a new attribute interpolate


00:07:15,071 --> 00:07:19,519
and use the primitive attributes we
just created to source the interpolation from.


00:07:23,911 --> 00:07:25,701
Limit the group to edge_prims.


00:07:25,701 --> 00:07:29,174
Create a spare parameter that
points to the foreach_begin node.


00:07:33,187 --> 00:07:34,794
Use the primgrouplist function


00:07:34,794 --> 00:07:37,923
to list all the primitive groups
available on the spare input.


00:08:10,656 --> 00:08:14,202
As you can see, we properly
preserved the primitive groups.


00:08:14,202 --> 00:08:16,427
Now let's take a look at the vertex groups.


00:08:16,427 --> 00:08:20,508
We have 3988 vertices in
the vtxboundary group.


00:08:20,912 --> 00:08:24,300
The VEX node has more vertices
than the original vertex group.


00:08:24,300 --> 00:08:27,977
The default Edge Divide again
handles the vertex groups properly.


00:08:42,940 --> 00:08:46,593
As you can see the issue is similar
to what we saw with point groups.


00:08:46,593 --> 00:08:49,952
So the new vertices are also added
to the existing groups.


00:08:49,952 --> 00:08:53,324
Again the vertices will be more
tricky to handle


00:08:53,324 --> 00:08:55,560
to preserve the existing vertex groups,


00:08:55,560 --> 00:08:59,287
just like it was more work to set
them up for attribute interpolation.


00:08:59,287 --> 00:09:01,346
So after create_new_primitives,


00:09:01,346 --> 00:09:03,081
we lose all the vertex groups,


00:09:03,081 --> 00:09:05,714
because we deleted the original vertices.


00:09:05,714 --> 00:09:09,019
It's only after attribute_interpolate2 node,


00:09:09,019 --> 00:09:12,530
that we get the vertex groups back,
albeit, incorrectly.


00:09:12,909 --> 00:09:16,014
This time we can't just exclude
the vertex groups.


00:09:16,342 --> 00:09:20,581
Doing so will remove all the new
vertices out of the existing vertex groups.


00:09:20,581 --> 00:09:22,613
So how can we fix this?


00:09:22,613 --> 00:09:26,079
Any vertex that has a weight
value that's greater than 0,


00:09:26,079 --> 00:09:28,593
will be assigned to existing
vertex groups.


00:09:29,098 --> 00:09:32,420
So we not only have to use 2
attribute interpolate nodes,


00:09:32,420 --> 00:09:34,235
1 for vertex attributes


00:09:34,235 --> 00:09:35,936
and 1 for vertex groups.


00:09:35,936 --> 00:09:40,273
But also we have to store
separate indices as an array attribute


00:09:40,273 --> 00:09:43,526
for the group interpolation,
but not weights attribute.


00:09:44,586 --> 00:09:47,501
Also exclude the input vertex
group in this node,


00:09:47,501 --> 00:09:49,784
from the list of interpolated attributes.


00:09:58,720 --> 00:10:00,524
So create a separate integer array


00:10:00,524 --> 00:10:02,899
to store the indices for the
group interpolation.


00:10:06,559 --> 00:10:08,183
If they are corner vertices,


00:10:08,183 --> 00:10:10,344
then use the same vertex indices,


00:10:10,344 --> 00:10:13,667
as we want to keep these vertices
in the existing groups.


00:10:16,772 --> 00:10:18,311
But if they are edge vertices,


00:10:18,311 --> 00:10:21,470
set the indices to -1 for each
vertex pair.


00:10:21,899 --> 00:10:24,855
We have to use 2 vertex
values just like before,


00:10:24,855 --> 00:10:28,103
because we will use the same
weights for the group interpolation,


00:10:28,103 --> 00:10:32,089
and that means a pair of vertex
weights for each edge vertex.


00:10:38,615 --> 00:10:41,209
Duplicate the attribute_interpolate2 node.


00:10:45,626 --> 00:10:48,022
I will also change the order for clarity.


00:10:57,677 --> 00:11:01,064
Just put star in the Vertex
Attributes parameter for now.


00:11:02,503 --> 00:11:04,212
Set the group to edgeprims.


00:11:07,922 --> 00:11:11,559
Set element numbers to
sourcevtx_groupindices this time.


00:11:12,341 --> 00:11:15,744
But there are a lot more hurdles we
have to jump through to get this working.


00:11:15,921 --> 00:11:18,102
If you look at the vtxboundary group,


00:11:18,228 --> 00:11:21,506
it still has way more vertices
than it's supposed to.


00:11:21,506 --> 00:11:24,057
But if you bypass attribinterpolate2,


00:11:24,057 --> 00:11:27,065
you can see that we get the
correct number of vertices.


00:11:27,772 --> 00:11:29,181
How could this be?


00:11:29,509 --> 00:11:32,563
Whatever attribinterpolate2
is interpolating,


00:11:32,563 --> 00:11:34,970
the vertex_groups node
should override it,


00:11:34,970 --> 00:11:37,575
because it has star in the
vertex attributes field,


00:11:37,575 --> 00:11:39,716
which denotes all groups and attributes.


00:11:40,498 --> 00:11:43,038
Somehow the inclusion of the vertex groups


00:11:43,038 --> 00:11:44,792
in the first attribute interpolate


00:11:44,792 --> 00:11:47,688
that interpolates the vertex
attributes and groups,


00:11:47,688 --> 00:11:49,751
prevents the second attribute interpolate


00:11:49,751 --> 00:11:51,758
to properly update the vertex groups.


00:11:52,010 --> 00:11:53,635
This looks like a bug.


00:11:54,190 --> 00:11:55,850
We need to find a workaround.


00:11:56,658 --> 00:11:58,722
So we have to exclude all the vertex groups


00:11:58,722 --> 00:12:00,550
from the first attribute interpolate node.


00:12:01,080 --> 00:12:05,600
Unfortunately this time, we don't have
a vertexgrouplist expression to use.


00:12:08,806 --> 00:12:10,677
As you can see in the help file,


00:12:10,677 --> 00:12:13,611
we have this function for points, primitives,


00:12:13,611 --> 00:12:16,179
and even edges, but not for vertices.


00:12:19,460 --> 00:12:22,448
Our second line of attack would 
be to use Python,


00:12:22,448 --> 00:12:24,380
as for sure it would have it.


00:12:35,431 --> 00:12:36,406
As you can see,


00:12:36,406 --> 00:12:38,289
Python does indeed have a function


00:12:38,289 --> 00:12:40,021
that returns all vertex groups.


00:12:40,526 --> 00:12:45,054
Unfortunately compiled networks
prohibit the use of Python expressions.


00:12:45,306 --> 00:12:48,968
If they supported it, then Python
SOP as well would be compilable,


00:12:49,102 --> 00:12:50,164
but it's not.


00:12:52,638 --> 00:12:54,800
If you know about intrinsic attributes,


00:12:54,800 --> 00:12:58,350
you might discover that vertex
groups is stored as a detail attribute.


00:13:02,994 --> 00:13:06,244
And there is a details expression
function that we can use.


00:13:14,448 --> 00:13:16,159
Let's just quickly test it.


00:13:19,390 --> 00:13:22,984
I will delete one of the vertex
groups so only one will remain.


00:13:41,440 --> 00:13:42,440
As you can see,


00:13:42,440 --> 00:13:44,119
we get the name of the vertex group


00:13:44,119 --> 00:13:45,771
available on the geometry.


00:13:48,421 --> 00:13:50,694
But it doesn't with multiple groups,


00:13:50,694 --> 00:13:53,400
because unfortunately details expression


00:13:53,400 --> 00:13:55,491
does not support array attributes.


00:13:55,491 --> 00:13:58,416
We have to find yet another
way to work around this.


00:14:09,950 --> 00:14:12,729
As you can see, compiling the
network throws an error.


00:14:13,209 --> 00:14:16,006
That's because we are also not
allowed to self reference.


00:14:16,006 --> 00:14:19,229
So I will just use a spare parameter
and reference the input node.


00:14:37,603 --> 00:14:41,706
Fortunately there is another way
barring HDK, and that's VEX.


00:14:42,312 --> 00:14:43,410
In VEX,


00:14:43,410 --> 00:14:45,199
we can get this intrinsic attribute


00:14:45,199 --> 00:14:47,363
and create a space separated
string from it.


00:14:48,000 --> 00:14:51,376
Create a detail attribute and use
the detail_intrinsic function to


00:14:51,376 --> 00:14:53,891
get the vertex_groups intrinsic attribute.


00:14:53,891 --> 00:14:56,297
Then join this array elements
with space


00:14:56,297 --> 00:14:58,756
to turn them into a space-separated string.


00:15:12,382 --> 00:15:16,006
As you can see, we now have the
list of vertex groups.


00:15:17,126 --> 00:15:19,776
Select the first attribute interpolate node


00:15:19,776 --> 00:15:21,609
that interpolates vertex attributes.


00:15:21,988 --> 00:15:25,386
Add a spare input that points to
the get_vertex_groups node.


00:15:27,658 --> 00:15:31,333
Now we can read the string value
we stored as a detail attribute.


00:15:35,245 --> 00:15:37,810
Do the same for the second
attribute interpolate node


00:15:37,810 --> 00:15:40,456
that interpolates vertex groups.


00:15:42,000 --> 00:15:45,111
Because we want to include only
the vertex groups in this node,


00:15:45,111 --> 00:15:47,042
you can use the expression as is.


00:15:47,345 --> 00:15:50,630
We will exclude the vertex groups
only from the first node.


00:16:20,110 --> 00:16:23,772
As you can see, the first node doesn't
interpolate vertex groups anymore.


00:16:26,145 --> 00:16:28,818
And the second node, correctly
does it this time.


00:16:28,818 --> 00:16:32,078
So now, we have successfully
preserved the vertex groups.


00:17:15,180 --> 00:17:17,690
Finally let's look at the edge groups.


00:17:24,100 --> 00:17:25,100
As you can see,


00:17:25,100 --> 00:17:29,512
both Edge Divide operators lose all
the edges in the existing groups.


00:17:45,078 --> 00:17:46,900
If we use the edge_boundary group


00:17:46,900 --> 00:17:49,293
as the input for both Edge Divide nodes,


00:17:49,293 --> 00:17:51,864
then we preserve the untouched edges.


00:17:56,231 --> 00:17:58,871
Both operators behave the
same in this aspect.


00:17:58,871 --> 00:18:02,919
But the edges that are divided
are lost using either operator.


00:18:02,919 --> 00:18:05,862
The thing is, we can preserve
the edge groups,


00:18:05,862 --> 00:18:07,468
even on the divided edges,


00:18:07,468 --> 00:18:09,983
but edges being an oddball in Houdini,


00:18:09,983 --> 00:18:13,305
this will require us to manually
loop over all the edge groups


00:18:13,305 --> 00:18:17,838
and check if any of the edges we
are splitting, is within these groups,


00:18:17,838 --> 00:18:21,139
and if so, add all the new point pairs


00:18:21,139 --> 00:18:23,502
that are formed by the edge
divide operation,


00:18:23,502 --> 00:18:27,720
for each existing edge and for each
edge group that has them.


00:18:28,174 --> 00:18:32,265
So I don't think it's worth taking a
severe performance hit just for this.


00:18:33,300 --> 00:18:37,169
Now we will preserve the original
primitive numbers for sub-geometry.


00:18:37,396 --> 00:18:39,811
Frankly we don't need to do this,


00:18:39,811 --> 00:18:41,366
but unfortunately


00:18:41,366 --> 00:18:43,820
some built-in tools perform a lot slower


00:18:43,820 --> 00:18:46,519
if the primitive numbers keep
getting shuffled.






----------------------------------------------------------------------------------------------------

81 - Custom Subdivision Surfaces - Splitting Edges [Edge Divide] - Final Optimizations

----------------------------------------------------------------------------------------------------



00:00:05,575 --> 00:00:06,952
Let's see it in action first.


00:00:07,443 --> 00:00:09,341
I am gonna bring in the character 
head model from


00:00:09,341 --> 00:00:11,147
the adaptive subdivide chapter.


00:00:14,143 --> 00:00:15,801
Copy the edge divide subnet


00:00:15,961 --> 00:00:18,020
inside the adaptive subdivide subnet.


00:00:28,769 --> 00:00:30,204
Use the same edge group


00:00:30,204 --> 00:00:31,554
for the new edge divide,


00:00:36,000 --> 00:00:38,463
and note the cook times of 
the old edge divide


00:00:38,463 --> 00:00:40,271
and the Rewire Vertices SOP.


00:00:51,114 --> 00:00:52,709
When we use the new edge divide,


00:00:52,709 --> 00:00:54,856
the end result is exactly the same


00:00:56,084 --> 00:00:59,349
but as you can see, the cook
 time of the Rewire Vertices SOP,


00:00:59,472 --> 00:01:00,777
increased dramatically.


00:01:01,145 --> 00:01:03,976
The reason for this is the 
shuffled primitive numbers.


00:01:04,700 --> 00:01:06,978
So let's just save the original 
primitive number


00:01:06,978 --> 00:01:09,806
before the edge divide using 
a primitive wrangle.


00:01:14,890 --> 00:01:16,776
Use this attribute in a sort SOP


00:01:16,776 --> 00:01:19,807
to re-sort the primitives back to 
their original indices.


00:01:29,584 --> 00:01:33,161
Now as you can see, the cook 
times also went back to normal.


00:01:33,714 --> 00:01:36,862
That's also why, in the beginning 
of this video, I said


00:01:37,095 --> 00:01:39,913
"the road to performance is not 
always the shortest".


00:01:40,257 --> 00:01:43,803
It might require you to perform 
extra steps and processes


00:01:43,803 --> 00:01:46,374
that might look convoluted 
and unnecessary,


00:01:46,374 --> 00:01:48,421
except for the ultimate performance,


00:01:48,875 --> 00:01:51,569
a price we are willing to pay 
most of the time.


00:01:52,527 --> 00:01:55,621
So let's implement it inside the 
edge divide properly.


00:01:55,805 --> 00:01:58,524
Store the original primitive numbers 
just like before.


00:01:59,052 --> 00:02:03,660
But we only want to do this, if 
the input group contains sub-geometry,


00:02:03,660 --> 00:02:06,227
i.e. not the entire geometry.


00:02:07,050 --> 00:02:09,767
If the edge divide is applied
 to the entire geometry,


00:02:09,767 --> 00:02:11,760
the primitive numbers won't change,


00:02:11,760 --> 00:02:15,503
as they will be removed and
 created in the same order.


00:02:15,699 --> 00:02:18,410
So create a switch node with 
a spare parameter


00:02:18,410 --> 00:02:21,787
that points to 
convert_edge_group_to_prim_group node.


00:02:25,909 --> 00:02:29,333
We will check if the edge_prims
group has the same number of primitives


00:02:29,333 --> 00:02:32,739
as the total number of primitives 
on the entire geometry.


00:02:33,206 --> 00:02:37,183
And if not, that means, we only apply 
the edge divide on sub-geometry,


00:02:37,392 --> 00:02:40,341
and only then we want to store the 
original primitive numbers.


00:02:48,772 --> 00:02:53,498
Duplicate the same switch node and 
connect it to the last attribute interpolate node.


00:02:53,498 --> 00:02:55,920
This time, we will only evaluate the sort node


00:02:55,920 --> 00:02:58,673
if we are applying the operation 
on a sub-geometry.


00:03:23,047 --> 00:03:26,046
As you can see, the cook times 
are still optimal as before.


00:03:36,942 --> 00:03:38,650
We don't have any holes either.


00:03:38,822 --> 00:03:42,253
So that means the new edge divide 
perfectly replaces the old one.


00:03:42,597 --> 00:03:46,833
Now that we have successfully implemented
 splitting the edges of polygons using VEX,


00:03:47,324 --> 00:03:50,726
subdividing polygons using edges 
will be much simpler,


00:03:50,726 --> 00:03:53,574
as we have created a fast and 
robust framework


00:03:53,574 --> 00:03:55,497
to achieve this sort of operation.


00:03:56,037 --> 00:03:58,639
All we have to do extra, is to 
create a new point


00:03:58,639 --> 00:04:00,578
at the center of each input polygon


00:04:00,971 --> 00:04:04,305
and instead of creating a single 
polygon in place of the old one,


00:04:04,305 --> 00:04:08,433
we will create multiple polygons using 
the new polygon point at the center.


00:04:09,219 --> 00:04:11,980
The number of new polygons depends 
on the number of edges


00:04:11,980 --> 00:04:13,183
of the original polygon.


00:04:13,907 --> 00:04:16,086
Triangles will create 3 new polygons


00:04:16,086 --> 00:04:19,321
while quads will create 4 new 
polygons and so on.


00:04:19,910 --> 00:04:22,778
After this, the interpolation of the attributes


00:04:22,778 --> 00:04:25,444
will require minimal changes 
to get them working.


00:04:26,574 --> 00:04:29,569
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

82 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Introduction

----------------------------------------------------------------------------------------------------



00:00:04,930 --> 00:00:10,318
The natural progression from Edge 
Divide to bilinear subdivision is quite linear.


00:00:11,101 --> 00:00:15,033
That's also why I started with 
Edge Divide first, as a stepping stone,


00:00:15,033 --> 00:00:19,386
both in terms of relevancy to the 
previous topics that led up to that moment,


00:00:19,701 --> 00:00:23,350
as well as logically dividing up the
 tasks to fully implement


00:00:23,504 --> 00:00:25,685
Catmull Clark subdivision in VEX.


00:00:26,404 --> 00:00:29,355
The biggest challenges are 
not the algorithms per se,


00:00:29,605 --> 00:00:32,920
but setting up the code framework 
to split up polygon edges,


00:00:33,093 --> 00:00:37,929
rebuild polygons, interpolate point,
 primitive, vertex attributes


00:00:37,929 --> 00:00:39,863
and preserve group memberships.


00:00:40,608 --> 00:00:44,121
Once you create this setup, the rest
 is quite straightforward,


00:00:44,307 --> 00:00:46,383
especially for bilinear subdivision.


00:00:46,967 --> 00:00:50,061
The only differences are, to create 
an additional point


00:00:50,144 --> 00:00:52,022
for each closed input polygon,


00:00:52,080 --> 00:00:55,707
and to create multiple new faces 
using the new edge points


00:00:55,900 --> 00:00:57,272
and the new face point.


00:00:57,875 --> 00:01:00,292
Granted there will be considerable new code


00:01:00,292 --> 00:01:02,338
to handle the interpolation 
of the new elements,


00:01:02,697 --> 00:01:06,153
that's quite different than how 
VEX Edge Divide SOP does it,


00:01:06,589 --> 00:01:09,458
but it's still a small hurdle 
relatively speaking.


00:01:10,377 --> 00:01:12,290
Let's see the concept of 
the algorithm first.






----------------------------------------------------------------------------------------------------

83 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Concept

----------------------------------------------------------------------------------------------------



00:00:05,068 --> 00:00:08,800
One of the things we have to do 
is to preserve literal input groups.


00:00:09,250 --> 00:00:13,403
This has a particular significance 
because when we are dealing with iterations,


00:00:13,853 --> 00:00:16,733
the input group has to survive 
the current iteration,


00:00:16,733 --> 00:00:20,386
and reference the new subdivided
primitives in the next iteration.


00:00:20,968 --> 00:00:24,990
If the input group is a named group, 
then it will be properly updated


00:00:25,459 --> 00:00:29,635
as you recall, we handle all named 
groups, except the edge groups.


00:00:30,254 --> 00:00:36,212
But if the input group is a literal group,
like 1 2 3, then just referencing the same indices


00:00:36,409 --> 00:00:39,232
will subdivide the wrong primitives
in the next iteration.


00:00:39,814 --> 00:00:43,985
That's why we have to pay attention 
to the literal input groups specifically.


00:00:44,529 --> 00:00:47,408
In any case, we will create a copy 
of the input group,


00:00:47,568 --> 00:00:51,554
whether it's a named group or a
 literal group or a combination of both,


00:00:51,817 --> 00:00:53,217
to get our bases covered.


00:00:54,362 --> 00:00:58,593
The next step is to create new face 
points for each closed polygon.


00:01:03,444 --> 00:01:07,850
Then instead of a point group to create 
the new edge points using a point wrangle,


00:01:08,085 --> 00:01:10,536
we will use a primitive group to do the same,


00:01:10,940 --> 00:01:14,867
but we will still use an edge group in 
the code to check for valid edges.


00:01:18,463 --> 00:01:21,970
If we don't do this, then the 2 points 
of 2 neighbour primitives


00:01:21,970 --> 00:01:25,403
as shown here would be included
 in the point wrangle.


00:01:26,000 --> 00:01:29,220
There would be no other way to 
check the inclusion of these edges,


00:01:29,220 --> 00:01:33,979
without using an edge group. 
Another thing we have to pay attention to is


00:01:33,979 --> 00:01:37,404
the handling of neighbour primitives 
for sub-geometry.


00:01:38,070 --> 00:01:41,670
This means, if the entire geometry 
is being subdivided,


00:01:41,848 --> 00:01:44,612
we can subdivide all the polygons 
the same way.


00:01:45,316 --> 00:01:47,707
But if a sub-geometry is to be subdivided,


00:01:47,707 --> 00:01:52,083
then we first have to subdivide the input 
polygons using the new method of splitting,


00:01:52,336 --> 00:01:57,444
but also recreate any polygon that shares 
an edge with the polygons in the input group.


00:01:58,345 --> 00:02:01,096
We can't subdivide these polygons 
using the new method,


00:02:01,096 --> 00:02:06,147
as they won't have new face points, 
only new edge points, and as such,


00:02:06,391 --> 00:02:11,071
the subdivision rules will be the same 
as the Edge Divide SOP, we created previously.


00:02:11,662 --> 00:02:15,766
When it comes to open polygons, 
the subdivision rules will also be the same


00:02:15,766 --> 00:02:19,085
as the Edge Divide SOP, as they 
won't have new face points.


00:02:19,657 --> 00:02:22,540
Overall, the biggest changes are going 
to be the new method of


00:02:22,540 --> 00:02:26,303
creating the new polygons and how 
we interpolate the existing attributes.


00:02:35,369 --> 00:02:37,859
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

84 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Modeling Test Geometry

----------------------------------------------------------------------------------------------------



00:00:04,883 --> 00:00:09,285
In the first chapter of bilinear subdivision, 
we will model our test geometry.


00:00:09,814 --> 00:00:13,574
The idea is to create a geometry that will 
have a challenging topology


00:00:13,766 --> 00:00:17,393
so that we can hit all the rules and edge 
cases that we have to handle.


00:00:17,802 --> 00:00:22,362
Implementing the same operation on a 
simple geometry like a box is very easy,


00:00:22,855 --> 00:00:26,733
but once you add tough cases like 
subdividing a subset of a geometry


00:00:26,733 --> 00:00:30,692
where the input polygons have open 
edges and their neighbour polygons


00:00:30,692 --> 00:00:35,083
also have open edges, and some of 
the polygons only share a single point,


00:00:35,083 --> 00:00:39,066
not an edge, and the input groups form 
a closed loop by themselves


00:00:39,066 --> 00:00:44,328
except one polygon, that's left out, 
it forces us to perfect our code and network


00:00:44,484 --> 00:00:47,483
to be able to handle all of these precisely.


00:00:48,000 --> 00:00:51,756
So, create a standard 10 by 10 grid 
with 10 by 10 divisions.


00:00:52,501 --> 00:00:54,607
Deform some of the points and primitives,


00:00:54,607 --> 00:00:57,346
so we can see if we are subdividing 
it correctly,


00:00:57,502 --> 00:01:00,008
if we only subdivide a subset 
of the geometry.


00:01:00,525 --> 00:01:04,785
This will come into play once we start 
implementing Catmull-Clark subdivision,


00:01:04,785 --> 00:01:08,541
as bilinear subdivision does not change 
the positions of the points.


00:01:09,600 --> 00:01:14,132
Then make sure to have the grid intersect 
with another copy of it, by a single point.


00:01:19,500 --> 00:01:24,039
Fuse the entire geometry, as we want these
polygons to actually share a single point.


00:01:24,532 --> 00:01:28,445
Now we will delete some of the polygons 
to have a lot of holes in places


00:01:28,541 --> 00:01:33,244
that will disturb the smooth surface created
by the Catmull-Clark subdivision later on.


00:01:34,950 --> 00:01:39,456
I will also create a challenging input 
primitive group when we test sub-geometry.


00:01:53,895 --> 00:01:58,084
I will create another primitive group and 
2 point groups to test the functionality


00:01:58,084 --> 00:01:59,945
that preserves existing groups.


00:02:24,469 --> 00:02:29,119
Color the entire geometry using the bounding
 box so we can test attribute interpolation


00:02:29,119 --> 00:02:30,371
when the time comes.


00:02:31,332 --> 00:02:35,760
For now, we will bypass the color node 
and use random primitive colors.


00:02:43,884 --> 00:02:47,916
I will also get a chunk of the geometry 
and piece it together at different places


00:02:48,096 --> 00:02:50,447
to create even more challenging edge cases.


00:02:51,348 --> 00:02:54,981
The idea is to create a point that's shared 
by multiple primitives


00:02:55,173 --> 00:02:58,463
while also having a lot of open edges, 
that's very different


00:02:58,463 --> 00:03:02,859
than having a point that has primitives
but not shared edges for example.


00:03:41,882 --> 00:03:45,076
I will also deform the entire geometry 
using the mountain SOP.


00:03:45,593 --> 00:03:50,085
Having perfectly rectangular quads might
give you false confidence in your algorithm


00:03:50,674 --> 00:03:55,203
so having them deformed will reveal, 
if we are actually smoothing polygons correctly,


00:03:55,203 --> 00:03:56,867
for catmull clark subdivision.


00:03:58,838 --> 00:04:02,104
Make sure to fuse the new pieces 
so that they are actually attached


00:04:02,104 --> 00:04:03,298
at the anchor points.


00:04:08,395 --> 00:04:09,468
As you can see,


00:04:09,468 --> 00:04:11,812
we have successfully fused the 
3 anchor points.


00:04:16,046 --> 00:04:18,997
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

85 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Starting from Edge Divide

----------------------------------------------------------------------------------------------------



00:00:04,640 --> 00:00:07,752
In this chapter we will bring in the
latest edge divide node


00:00:07,752 --> 00:00:09,271
we have created previously,


00:00:09,440 --> 00:00:10,800
and build on top of it.


00:00:11,440 --> 00:00:13,982
So create an integer parameter 
for the iterations


00:00:14,227 --> 00:00:16,258
and a menu parameter for the algorithm.


00:00:22,720 --> 00:00:26,111
We will have bilinear and
Catmull-Clark in the menu parameter.


00:00:35,808 --> 00:00:38,368
Set the input edge group to 
be a primitive group.


00:00:42,000 --> 00:00:46,800
I will also replace all the edge divide
suffixes with subdivide for clarity.


00:00:55,840 --> 00:00:59,200
Unfortunately, using Find and
Replace doesn't save the changes.


00:01:00,880 --> 00:01:01,910
As you can see,


00:01:01,910 --> 00:01:03,670
the replace action isreverted back


00:01:03,670 --> 00:01:05,200
once we select a different node.


00:01:06,000 --> 00:01:09,093
So we have to edit the code inside 
the wranglenode manually, 


00:01:09,093 --> 00:01:10,582
to force update the code.


00:02:35,840 --> 00:02:37,785
I hard code divisions to 2,


00:02:37,785 --> 00:02:40,752
because subdivisionalways 
splits edges in 2


00:02:40,752 --> 00:02:41,953
for each iteration.


00:02:41,953 --> 00:02:44,771
We can also get rid of the 
divisions parameter now.


00:02:59,840 --> 00:03:02,711
I also rename edgeprims to orig_prims,


00:03:07,280 --> 00:03:09,373
and change Group Type to Primitives.


00:03:10,911 --> 00:03:14,672
Replaceall edgeprims group 
references with orig_prims.


00:03:32,560 --> 00:03:34,315
Make sure to update the group name


00:03:34,315 --> 00:03:35,850
insidethe switch expression.


00:03:38,448 --> 00:03:39,979
Also add a condition that checks


00:03:39,979 --> 00:03:42,001
if the iterations is greater than 0.


00:03:42,281 --> 00:03:45,561
Otherwise, we just return the input 
geometry as is.


00:03:56,800 --> 00:03:59,174
Now we are ready to create the 
new face points.


00:03:59,920 --> 00:04:02,644
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

86 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Creating New Face Points

----------------------------------------------------------------------------------------------------



00:00:04,720 --> 00:00:06,400
Now we will create the new face points.


00:00:07,040 --> 00:00:08,127
You can create them


00:00:08,127 --> 00:00:10,305
before the new edge pointsor after them.


00:00:10,926 --> 00:00:14,090
I chose to create them before
so their point indices


00:00:14,090 --> 00:00:17,464
come before the edgepoints, 
the same way OpenSubdiv does it.


00:00:18,169 --> 00:00:21,369
So create a new primitive wrangle
and use the orig_prims group.


00:00:24,560 --> 00:00:28,000
First we need to check if the
current primitive is closed,


00:00:28,198 --> 00:00:31,798
and if not, we just skip it,
by performing an early return.


00:00:38,320 --> 00:00:40,618
The same way we create 
the new edge points,


00:00:40,618 --> 00:00:43,248
wewill create the new face 
points at the origin.


00:00:48,315 --> 00:00:51,336
Add them to a separate
point group called face_pts.


00:00:52,320 --> 00:00:55,760
It will be important to differentiate
them from the edge points.


00:00:56,640 --> 00:01:00,400
We need to track them in successive
steps also, but this time


00:01:00,400 --> 00:01:04,682
we don't have edge points to reference,
but rather the primitive they belong to.


00:01:05,360 --> 00:01:08,640
So store the current primitive
index on the new face point.


00:01:09,360 --> 00:01:11,840
We will use it to retrieve these 
points later on.


00:01:13,680 --> 00:01:18,640
Now we need to create the source point
indices for point attribute interpolation.


00:01:19,680 --> 00:01:23,680
So first get the points of the current
primitive using the primpoints function.


00:01:24,560 --> 00:01:29,440
Considering the new face point lies at the
center of the polygon, it will use equal


00:01:29,440 --> 00:01:35,611
weights for each point the polygon has. And
that means 1 divided by the number of points.


00:01:48,560 --> 00:01:51,735
So save the indices and the
weights arrays on the points.


00:02:05,873 --> 00:02:08,913
As you can see, the face
points are accumulated here.


00:02:12,480 --> 00:02:14,960
Now we have to add the face_pts group to the


00:02:14,960 --> 00:02:17,840
Attribute Interpolate node
that interpolates points.


00:02:20,960 --> 00:02:25,280
Keep new_pts as is for now. It
will become edge_pts later on.


00:02:26,720 --> 00:02:29,494
As you can see, the face 
points are transformed


00:02:29,494 --> 00:02:31,619
to the center of the polygon they belong to.


00:02:34,880 --> 00:02:37,741
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

87 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Creating New Edge Points

----------------------------------------------------------------------------------------------------



00:00:04,400 --> 00:00:07,532
We already have the code for 
creating newedge points,


00:00:07,923 --> 00:00:09,620
we just have to modify it a bit.


00:00:10,160 --> 00:00:11,235
But before that


00:00:11,235 --> 00:00:12,695
we need to create an edgegroup.


00:00:12,979 --> 00:00:15,260
Because we don't have an input 
edge group anymore.


00:00:15,904 --> 00:00:18,990
So create a Group Promote
node to convert orig_prims


00:00:18,990 --> 00:00:21,501
to an edge group, 
called orig_prim_edges.


00:00:22,183 --> 00:00:23,520
Keep the original group


00:00:23,520 --> 00:00:26,335
and make sure to toggle Include 
Only Elements


00:00:26,335 --> 00:00:28,754
Entirely Contained in Original Group,


00:00:29,076 --> 00:00:32,218
as we don'twant the edges go 
beyond the original primitives.


00:00:33,100 --> 00:00:35,900
Now we will modify the
create_new_edge_points wrangle node.


00:00:36,720 --> 00:00:40,322
First replace the group used
by inedgegroup function.


00:00:47,760 --> 00:00:50,240
Rename new_pts to edge_pts.


00:00:52,000 --> 00:00:55,793
Update all new_pts references with edge_pts.


00:01:15,840 --> 00:01:20,037
Also add face_pts to Group Delete
node to delete this group as well.


00:01:30,461 --> 00:01:34,781
Finally, use the orig_prims group in the
create_new_edge_points wrangle node.


00:01:35,689 --> 00:01:40,173
Any primitive in the input primitive
group, will have their edges split up.


00:01:42,480 --> 00:01:45,323
And with that said,
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

88 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Creating New Closed Polygons - Concept

----------------------------------------------------------------------------------------------------



00:00:04,640 --> 00:00:06,800
We are ready to create the new
polygons to replace the input polygons.


00:00:07,360 --> 00:00:08,822
But before we can do this,


00:00:08,822 --> 00:00:10,928
we need to decideon the order of points


00:00:10,928 --> 00:00:12,884
we will use for each polygon.


00:00:13,456 --> 00:00:16,132
There are some rules we have to follow,


00:00:16,225 --> 00:00:18,493
such as the direction of the order of points.


00:00:19,440 --> 00:00:22,365
If we want the normal of the 
polygon to face us,


00:00:22,498 --> 00:00:25,253
we have to pick the points in a clockwise fashion.


00:00:25,586 --> 00:00:27,586
This is the convention in Houdini.


00:01:01,200 --> 00:01:03,810
Here we can see the order of the points


00:01:03,810 --> 00:01:05,904
Subdivide uses for each polygon.


00:01:06,543 --> 00:01:09,582
Note that this seems variable 
with different topology


00:01:09,582 --> 00:01:12,813
buthere we can analyse it
using a single polygon.


00:01:13,520 --> 00:01:17,840
The order of points picked is different
between different subdivision algorithms


00:01:17,840 --> 00:01:22,347
i.e. OpenSubdiv Catmull Clark
vs Houdini Catmull Clark.


00:01:22,960 --> 00:01:28,640
Here we are looking at OpenSubdiv Catmull
Clark. The way Houdini Catmull Clark picks


00:01:28,640 --> 00:01:31,520
the points would require more
conditionals in the code,


00:01:31,520 --> 00:01:35,213
as it doesn't seem as consistent
as OpenSubdiv Catmull Clark.


00:01:35,840 --> 00:01:39,254
So I will use the same point 
order as OpenSubdiv.


00:01:39,254 --> 00:01:42,080
This will make the polygon 
creation code simpler.


00:01:43,760 --> 00:01:45,360
Let's see the concept of it first.


00:01:47,440 --> 00:01:50,392
I am just gonna draw the 
same subdivided pentagon.


00:02:09,160 --> 00:02:11,960
So let's write down the point
indices for each polygon.


00:02:20,906 --> 00:02:26,186
I am coloring original points as
blue, the new edge points as green,


00:02:31,520 --> 00:02:34,880
and the face point, i.e.
the center point as yellow.


00:02:35,600 --> 00:02:38,960
Just from the colors, you
can see a pattern already.


00:02:38,960 --> 00:02:41,065
And that's what I meant when I said,


00:02:41,065 --> 00:02:43,848
thiswill make the polygon 
creation code simpler.


00:02:43,848 --> 00:02:47,910
Because we won't have complex
statements for the order of points.


00:03:03,440 --> 00:03:09,120
I will differentiate the edge points
in the same polygon as ep0 and ep1,


00:03:09,120 --> 00:03:12,160
meaning, edge point 0 and edge point 1.


00:03:13,280 --> 00:03:16,018
If you look at ep0 and ep1,


00:03:16,018 --> 00:03:18,691
you can see that
they are offset by 1 element


00:03:18,691 --> 00:03:19,453
from each other.


00:03:20,093 --> 00:03:21,533
So how can we structure this?


00:03:22,240 --> 00:03:24,214
If you think about it for a minute,


00:03:24,214 --> 00:03:26,213
you cancome up with a simple formula


00:03:26,213 --> 00:03:26,880
like this one.


00:03:27,440 --> 00:03:28,400
Which means:


00:03:28,400 --> 00:03:30,727
you take the current edge point index,


00:03:30,727 --> 00:03:33,840
youadd the edge point count 
minus 1 to this,


00:03:34,307 --> 00:03:37,427
and then perform modulo 
operation on the result


00:03:37,773 --> 00:03:39,421
using the edge point count.


00:03:40,000 --> 00:03:46,080
So if the current edge point index is 0,
then 0 + 4, because edge point count is 5,


00:03:46,080 --> 00:03:48,160
but we subtract 1 from this.


00:03:48,160 --> 00:03:50,186
4 modulo 5 is 4.


00:03:52,560 --> 00:03:56,840
The next iteration will be
5 modulo 5, which will be 0.


00:03:57,440 --> 00:04:02,626
Then 6 modulo 5, 
which will be 1, and so on.


00:04:08,347 --> 00:04:09,947
These are the array indices.


00:04:10,560 --> 00:04:13,719
So if we get the array elements 
from theseindices,


00:04:13,719 --> 00:04:16,726
for the first iteration, ep0 becomes 6,


00:04:16,726 --> 00:04:19,028
because it's the first element 
in thearray,


00:04:19,028 --> 00:04:21,528
and ep1 will be the fifth element


00:04:21,528 --> 00:04:23,790
because array indices start from 0,


00:04:23,790 --> 00:04:28,000
and assuch 4 means the fifth element. 
And that's 10.


00:04:28,853 --> 00:04:35,493
So just like in the point indices table,
we had 6 for ep0 and we got 10 for ep1.


00:04:36,080 --> 00:04:39,840
If you look at the second
iteration, we will have 1 and 0,


00:04:39,840 --> 00:04:44,160
where the second edge point is
7 and the first edge point is 6.


00:04:44,160 --> 00:04:46,893
Again, just like in the point indices table.


00:04:47,600 --> 00:04:50,266
The formula we came up with is working fine.


00:04:50,720 --> 00:04:55,760
So that means, all we have to do is,
for each polygon we are going to create,


00:04:55,760 --> 00:04:58,018
is to first pick the original point,


00:04:58,018 --> 00:05:02,160
and thenthe first edge point ep0, 
then the center point,


00:05:02,800 --> 00:05:07,617
and finally the second edge point
ep1, using the same formula.


00:05:13,534 --> 00:05:16,497
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

89 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Creating New Closed Polygons - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,560 --> 00:00:08,080
Now we are ready to implement the
new polygon creation logic in VEX.


00:00:08,960 --> 00:00:13,120
Remove the old polygon creation code
from the active network and set it aside.


00:00:14,000 --> 00:00:17,334
Create a new primitive wrangle
and use the orig_prims group.


00:00:32,480 --> 00:00:36,070
We can copy some of the code from
the old polygon creation code.


00:00:48,720 --> 00:00:55,325
First off, retrieve the center point, i.e. the
face point, using the primcenterpt attribute.


00:01:13,559 --> 00:01:17,239
Rename edgepts, as we will now
have an array with the same name.


00:01:18,080 --> 00:01:21,200
Instead of an integer array,
we will now use the overload


00:01:21,200 --> 00:01:24,190
of findattribval function that 
returns an integer.


00:01:24,800 --> 00:01:30,000
The reason is, unlike Edge Divide, we will
only have a single point for each edge,


00:01:30,000 --> 00:01:32,842
and as such we don't need 
to use an array anymore.


00:01:33,920 --> 00:01:36,720
Gather these indices in edge_pts array.


00:01:39,440 --> 00:01:42,800
I will also bypass some of the
attribute interpolate nodes


00:01:42,800 --> 00:01:44,560
as we won't have some of the groups yet.


00:01:47,520 --> 00:01:52,240
If the current polygon is closed, we removed
the last item we added to the pts array.


00:01:53,040 --> 00:01:56,240
We will write the same code as
before that decides whether to


00:01:56,240 --> 00:01:58,640
create a closed polygon or an open polyline.


00:02:03,680 --> 00:02:06,666
Now remove the current
primitive but keep the points.


00:02:09,250 --> 00:02:12,210
Create a new integer array
for the new point indices.


00:02:12,880 --> 00:02:17,360
We define an integer variable that
stores how many polygons to create,


00:02:17,360 --> 00:02:20,480
which is based on the number of
points in the current polygon.


00:02:21,360 --> 00:02:26,400
So 3 polygons for a triangle, and
4 polygons for a quad, and so on.


00:02:27,360 --> 00:02:31,120
But if the polygon is open, then we
will only create a single polyline.


00:02:33,840 --> 00:02:35,984
Now we define an integer variable


00:02:35,984 --> 00:02:38,743
for the newpolygon and an integer array


00:02:38,743 --> 00:02:41,132
for the new vertices, just like before.


00:02:41,868 --> 00:02:44,481
These will bepassed to the 
addprim function.


00:02:45,120 --> 00:02:49,840
We will start creating the new polygons in
a for loop, using the primcount variable.


00:02:51,760 --> 00:02:56,000
Just like in the concept, we use the
same formula to define the last index.


00:03:00,246 --> 00:03:04,726
If the polygon is closed, then we
define 4 points as we have seen before.


00:03:05,440 --> 00:03:09,742
The first point is the current 
original point fromthe pts array.


00:03:10,347 --> 00:03:13,638
As you can see, we are using i as the index.


00:03:14,125 --> 00:03:18,358
So in the next iteration, it will
use the next point in the pts array


00:03:18,358 --> 00:03:19,162
and so on.


00:03:20,320 --> 00:03:25,908
The second point is the current edge
point from the edge_pts array, using i.


00:03:26,622 --> 00:03:28,782
The third point is the center point.


00:03:29,520 --> 00:03:34,480
And the fourth point is the last_index
indexed into the edge_pts array.


00:03:35,879 --> 00:03:39,319
We can now create an array that
holds all of these indices.


00:03:42,400 --> 00:03:45,840
Now pass this array and the type
name to the addprim function.


00:03:51,760 --> 00:03:55,440
As you can see, we have successfully
subdivided the input polygons.


00:03:58,454 --> 00:04:02,134
The primitive colors are lost,
but we will restore them later on.


00:04:03,330 --> 00:04:07,490
We have just completed one of the key
parts of the subdivision operation.


00:04:08,720 --> 00:04:11,550
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

90 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Creating New Open Polygons - Concept

----------------------------------------------------------------------------------------------------



00:00:04,655 --> 00:00:08,327
We have to handle open polygons 
differently than closed polygons.


00:00:11,173 --> 00:00:14,526
After we create the new edge points, 
what we have to do is


00:00:14,725 --> 00:00:17,933
to interleave the original points with 
the new edge points.


00:00:29,450 --> 00:00:32,599
We have to pay attention to open 
polygons with shared points,


00:00:32,793 --> 00:00:33,993
which we will see soon.


00:00:45,559 --> 00:00:48,000
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

91 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Creating New Open Polygons - Implementation

----------------------------------------------------------------------------------------------------



00:00:05,280 --> 00:00:08,822
First create an Ends SOP to
unroll polygons with new points.


00:00:22,187 --> 00:00:24,427
Store the edge points count in a variable.


00:00:26,880 --> 00:00:30,472
Create a for loop using the count
variable we defined early on.


00:00:35,680 --> 00:00:39,120
We have to include the count
itself because as you recall


00:00:39,120 --> 00:00:43,212
if the polygon is not closed,
then we subtract 1 from the count.


00:00:43,925 --> 00:00:46,005
First append the current original point.


00:00:47,520 --> 00:00:49,568
Then append the current edge point,


00:00:49,568 --> 00:00:51,079
to interleavethem both.


00:00:51,079 --> 00:00:55,734
But only if the current for loop index f,
is less than the edge count.


00:00:56,158 --> 00:00:59,970
Otherwiseyou would be adding a 
non-existent element to the array,


00:01:00,352 --> 00:01:04,480
but also it would be an extra
point that shouldn't be in the polyline.


00:01:48,480 --> 00:01:53,040
As you can see, we have perfectly
handled all open polygon cases,


00:01:53,040 --> 00:01:56,400
whether they are open, or unrolled
with or without shared points.


00:02:00,202 --> 00:02:04,362
We can now move onto preserving
groups and interpolating attributes.


00:02:06,080 --> 00:02:08,908
And with that said,
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

92 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Preserving Primitive Groups & Interpolating Primitive Attributes

----------------------------------------------------------------------------------------------------



00:00:05,068 --> 00:00:08,597
Right off the bat, you can see that 
we no longer have any primitives


00:00:08,597 --> 00:00:10,049
in the orig_prims group.


00:00:11,282 --> 00:00:13,055
So we have to fix this first.


00:00:13,452 --> 00:00:17,227
Just like VEX Edge Divide SOP, 
we store the current primitive index


00:00:17,227 --> 00:00:20,340
as an integer but also inside an integer array,


00:00:20,340 --> 00:00:23,505
and a corresponding weights array 
with a value of 1.


00:00:24,080 --> 00:00:26,285
Store these values on the new primitive.


00:00:47,211 --> 00:00:50,253
Make sure to also add the new primitive 
to the orig_prims group.


00:00:57,344 --> 00:01:00,182
As you can see, we got the primitive 
colors back.


00:01:01,133 --> 00:01:05,269
I will also move Attribute Copy at 
the end of the Attribute Interpolate chain.


00:01:05,886 --> 00:01:07,578
This will be necessary later on.


00:01:08,069 --> 00:01:10,607
Update the references to the Attribute Copy


00:01:10,607 --> 00:01:13,345
with the node that came before 
that previously.


00:01:44,217 --> 00:01:49,221
You can see open or closed polygons, 
we preserve primitive colors perfectly.


00:02:07,827 --> 00:02:11,454
You can also see the input primitive 
groups that are properly updated.


00:02:12,614 --> 00:02:16,907
You can verify this by coloring the primitives inside a primitive group


00:02:16,907 --> 00:02:18,179
before and after.


00:02:37,741 --> 00:02:40,494
Point Groups are properly updated as well.


00:02:41,560 --> 00:02:45,485
We need to do some clean up to make 
sure we don't keep any of the attributes


00:02:45,485 --> 00:02:48,642
or groups we create during 
the subdivide operation.


00:03:13,423 --> 00:03:16,250
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

93 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Preserving Vertex Groups & Interpolating Vertex Attributes for Closed Polygons - Concept

----------------------------------------------------------------------------------------------------



00:00:04,880 --> 00:00:06,490
We are back with vertices.


00:00:07,148 --> 00:00:09,794
As you recallthey are always 
more work to handle.


00:00:10,486 --> 00:00:14,086
Fortunately, they are going to be very
similar to points for our problem.


00:00:15,440 --> 00:00:17,747
Using the same pentagon polygon example,


00:00:17,881 --> 00:00:20,937
let'slook at primitive 0, and list its points.


00:00:34,444 --> 00:00:36,259
We will figure out the vertex indices


00:00:36,259 --> 00:00:39,324
andvertex weights to use 
for vertex interpolation.


00:00:43,680 --> 00:00:46,800
So for the first vertex, that point 0 is using,


00:00:46,876 --> 00:00:50,796
we will use the original vertex, the
primitive is using for this point.


00:00:51,920 --> 00:00:54,633
The next vertex, that point 6 is using,


00:00:54,633 --> 00:00:57,602
willuse the current vertex and the next vertex,


00:00:57,840 --> 00:01:01,046
just like the point version, 
which was using thecurrent point


00:01:01,046 --> 00:01:02,943
and the next point if you recall.


00:01:04,880 --> 00:01:08,200
The next vertex, 
that the center point is using,


00:01:08,200 --> 00:01:13,360
will use all the vertices of the primitive,
equally, again, just like the point version.


00:01:16,000 --> 00:01:19,640
Finally, the last vertex, that point 10 is using,


00:01:19,640 --> 00:01:24,286
will use the last vertex and the next,
again, just like the point version.


00:01:28,000 --> 00:01:31,280
The weight values for the
original vertex will be 1,


00:01:31,280 --> 00:01:34,560
as it will be fully influenced
by this vertex alone.


00:01:35,280 --> 00:01:40,480
The new edge vertices will be
0.5 and 0.5 for each end vertex.


00:01:41,440 --> 00:01:43,396
Lastly the center vertex


00:01:43,396 --> 00:01:45,032
will use uniform weights


00:01:45,032 --> 00:01:48,490
for equal distribution of 
vertex attribute values.


00:01:57,520 --> 00:02:00,588
So if we list these vertex indices, they will


00:02:00,588 --> 00:02:02,357
be like so for primitive 0.


00:02:03,006 --> 00:02:06,174
Note that I am using point indices 
here for clarity


00:02:06,355 --> 00:02:09,781
but theactual indices will be 
using vertex indices.


00:02:16,560 --> 00:02:21,468
Primitive 1 will use the same rules
and end up with these vertex indices.


00:02:39,920 --> 00:02:42,928
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

94 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Preserving Vertex Groups & Interpolating Vertex Attributes for Closed Polygons - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,880 --> 00:00:06,640
I am going to bring in the vertex colors.


00:00:08,640 --> 00:00:12,145
We are implementing the closed
polygon vertex implementation first.


00:00:26,560 --> 00:00:28,828
The same as the polygon creation code,


00:00:28,993 --> 00:00:33,445
we performthe same loop from 
0 to 4, 4 being exclusive.


00:00:43,280 --> 00:00:46,967
Define array variables for
source vertex indices


00:00:48,677 --> 00:00:49,406
weights


00:00:54,960 --> 00:00:56,172
and group indices.


00:00:56,742 --> 00:00:59,790
You might recall
this from the VEX Edge Divide SOP.


00:01:00,400 --> 00:01:05,760
Just like in the concept, the first vertex
will use the original vertex index fully,


00:01:05,760 --> 00:01:08,305
which are stored in the vtx array.


00:01:16,569 --> 00:01:21,369
Assign the same indices for the group
indices, for vertex group interpolation.


00:01:23,840 --> 00:01:26,480
The next vertex will be the new edge vertex,


00:01:32,320 --> 00:01:35,840
so we interpolate between the
current vertex and the next vertex.


00:01:38,261 --> 00:01:41,621
Weights are equally
distributed with values of 0.5.


00:01:42,409 --> 00:01:45,689
We don't want these vertices
in the existing vertex groups,


00:01:46,160 --> 00:01:50,560
so we set their group indices
to -1 for each edge vertex.


00:01:51,680 --> 00:01:56,880
For the center vertex, we use all 
theoriginal vertices in the vtx array,


00:01:56,880 --> 00:01:58,217
except the last element.


00:01:58,762 --> 00:02:02,297
For vertex interpolation, 
we have toexclude the last element


00:02:02,297 --> 00:02:04,335
by slicing the vtx array.


00:02:05,095 --> 00:02:09,659
As yourecall, if the polygon is closed, 
we append the first vertex


00:02:09,659 --> 00:02:12,960
to the end ofthe vtx array, to 
make the code simpler.


00:02:15,266 --> 00:02:17,692
The weights array will be the same as the


00:02:17,692 --> 00:02:20,497
center point so we can just reference that.


00:02:26,181 --> 00:02:29,781
For the vertex groups, this
vertex will also be excluded.


00:02:33,301 --> 00:02:38,501
Finally, the last edge vertex will use
the same logic as the first edge vertex,


00:02:39,120 --> 00:02:43,810
except it will use the last_index, that
we have stored above, and the next.


00:02:52,560 --> 00:02:56,308
Store all of these arrays on the new vertices.


00:02:56,308 --> 00:02:59,523
The overload of the addprim 
function that we are using,


00:02:59,523 --> 00:03:01,214
makes it very easy for us


00:03:01,214 --> 00:03:03,840
toaccess the indices of the new vertices.


00:03:27,360 --> 00:03:31,120
Lastly, add the new vertices 
to the new_vtx group


00:03:31,120 --> 00:03:34,400
that will be used by the Attribute
Interpolate nodes downstream.


00:03:39,840 --> 00:03:42,960
We can now enable the vertex 
attributes interpolate nodes.


00:03:44,160 --> 00:03:48,000
As you can see, we have successfully
interpolated the vertex attributes.


00:03:49,600 --> 00:03:52,530
I am just gonna create a vertex 
group to test it.


00:04:15,360 --> 00:04:19,200
As you can see, we have successfully
preserved the input vertex group.


00:04:19,760 --> 00:04:25,205
300 vertices before subdivide
and 300 vertices after subdivide.


00:04:34,080 --> 00:04:35,994
The same vertices are highlighted,


00:04:36,184 --> 00:04:38,590
excepttheir indices are different of course.


00:04:39,301 --> 00:04:42,741
We are now ready to do the same
operation, for open polygons.


00:04:44,560 --> 00:04:47,299
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

95 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Preserving Vertex Groups & Interpolating Vertex Attributes for Open Polygons

----------------------------------------------------------------------------------------------------



00:00:04,670 --> 00:00:09,695
The case of vertex interpolation and 
group preservation for open polygons


00:00:09,695 --> 00:00:11,304
is much easier to implement.


00:00:11,945 --> 00:00:16,153
But just like in VEX Edge Divide SOP, 
we have to build our own index array.


00:00:16,756 --> 00:00:20,357
So first create an integer array 
for vertex indices.


00:00:24,119 --> 00:00:27,352
Append the original vertex index 
for the original points,


00:00:30,490 --> 00:00:35,870
and append -1 for the new edge vertices 
so we can handle them later on.


00:00:50,279 --> 00:00:52,146
Loop over the new vertices array.


00:01:04,113 --> 00:01:09,610
If the current vertex index is not -1, 
then the vertex belongs to an original point.


00:01:10,511 --> 00:01:13,897
So we can use the same code for 
handling original vertices.


00:01:17,160 --> 00:01:21,128
Just make sure to change the index 
value with the vertex_index value.


00:01:21,769 --> 00:01:26,611
For the edge vertices, we can also use 
the same code for handling edge vertices.


00:01:27,187 --> 00:01:31,609
This time we will use our local index 
value and increment it manually,


00:01:31,609 --> 00:01:36,860
so that the next edge vertex we come 
across uses the correct original vertex index,


00:01:36,860 --> 00:01:37,811
and the next.


00:02:25,246 --> 00:02:29,559
As you can see, we have perfectly
interpolated the existing vertex colors


00:02:29,559 --> 00:02:30,790
for open polygons.


00:02:32,090 --> 00:02:34,715
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

96 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Implementing Iterations

----------------------------------------------------------------------------------------------------



00:00:04,458 --> 00:00:06,774
Before we can go forward 
with the next topics,


00:00:07,064 --> 00:00:09,393
we need to add the iterations functionality.


00:00:10,000 --> 00:00:13,292
So create a for feedback loop network 
and reference the iterations


00:00:13,292 --> 00:00:14,789
parameter from the subnet.


00:00:38,334 --> 00:00:40,527
As you can see, everything works great.


00:00:41,233 --> 00:00:45,071
That means the entire operation 
works without affecting the subsequent


00:00:45,071 --> 00:00:46,468
iterations negatively in any way.


00:01:21,834 --> 00:01:24,549
It also works just as well on open polygons.


00:01:29,918 --> 00:01:34,670
But there is still something we have to 
fix wrt iterations which we will do next.


00:01:36,749 --> 00:01:39,391
And with that said,
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

97 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Preserving Literal Groups

----------------------------------------------------------------------------------------------------



00:00:04,573 --> 00:00:07,544
When we use literal groups like 1 2 3,


00:00:07,544 --> 00:00:11,442
you will realize that this requires 
a special handling of these cases


00:00:11,709 --> 00:00:14,129
when performing more than 1 iteration.


00:00:15,399 --> 00:00:19,720
Take a look at these primitives, 9, 17, 25.


00:00:26,050 --> 00:00:28,896
As you can see, we just subdivided 
them once.


00:00:29,564 --> 00:00:33,628
But as you can imagine, now these
primitives refer to other primitives,


00:00:33,628 --> 00:00:36,641
other than the original ones we have 
before subdivision.


00:00:37,214 --> 00:00:41,382
So in the subsequent iterations, we 
end up subdividing wrong primitives


00:00:41,563 --> 00:00:46,682
and it goes on until the last iteration. 
Surely we can't propagate the changes


00:00:46,682 --> 00:00:49,279
to a literal group, as they are just numbers.


00:00:49,881 --> 00:00:54,262
So instead we have to create a new 
primitive group using the input group,


00:00:54,548 --> 00:00:59,261
regardless of whether they are literal 
groups or named groups, ad-hoc groups


00:00:59,261 --> 00:01:00,344
or mixed groups.


00:01:01,251 --> 00:01:04,782
So create a primitive wrangle after 
the compile begin node.


00:01:10,546 --> 00:01:14,148
Copy the same group expression 
from the create_prim_group wrangle node,


00:01:14,339 --> 00:01:16,743
and create a group called source_prims.


00:01:27,687 --> 00:01:31,385
We can now use this new group, 
in the create_prim_group wrangle node.


00:01:32,091 --> 00:01:34,592
Note that we still need to have 
the orig_prims group


00:01:34,592 --> 00:01:37,258
as it's created per connected piece.


00:01:43,021 --> 00:01:47,739
As you can see, the literal group is now
 properly updated through each iteration,


00:01:47,739 --> 00:01:51,827
and as such we are subdividing the 
correct primitives iteratively.


00:01:56,302 --> 00:01:59,871
Make sure to update the group delete 
nodes, so we delete all groups


00:02:00,000 --> 00:02:02,083
with the _subdivide suffix


00:02:06,006 --> 00:02:10,107
but keep the source_prims group, 
as we need it to survive each iteration.


00:02:14,000 --> 00:02:16,560
Only in the very end, we can delete it as well.


00:02:21,771 --> 00:02:24,256
I will also delete 
source_vfx_group_indices attribute.


00:02:36,935 --> 00:02:39,541
We are very close to finishing 
the bilinear subdivision.


00:02:40,830 --> 00:02:43,295
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

98 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Creating Neighbour Primitives

----------------------------------------------------------------------------------------------------



00:00:04,640 --> 00:00:06,752
Beyond gaining a deeper understanding of


00:00:06,752 --> 00:00:09,171
Catmull-Clark subdivision surfaces algorithm


00:00:09,171 --> 00:00:11,664
and various processes to make it happen,


00:00:11,664 --> 00:00:14,388
eventhough we already have this 
operator in Houdini,


00:00:14,388 --> 00:00:16,653
we are also fixing some of the problems


00:00:16,653 --> 00:00:19,668
that exist in the built-in subdivide operator.


00:00:20,035 --> 00:00:23,879
By far the biggest issue,
is the breaking up of the geometry.


00:00:24,601 --> 00:00:27,881
Even though the Houdini Catmull-Clark
implementation has an option


00:00:28,080 --> 00:00:30,261
to preserve sub-geometry boundaries,


00:00:30,353 --> 00:00:33,532
it doesn'tscale in performance 
at higher subdivision levels,


00:00:33,716 --> 00:00:34,941
unlike OpenSubdiv.


00:00:35,675 --> 00:00:38,612
But OpenSubdiv while being very performant,


00:00:38,612 --> 00:00:40,476
it doesn't preserve these boundaries.


00:00:41,012 --> 00:00:43,588
So it's a trade off that an artist has to face,


00:00:43,680 --> 00:00:46,332
unlessthey have the technical 
skills to remedy this.


00:00:47,040 --> 00:00:50,537
In our implementation, 
we will have the best ofboth worlds.


00:00:51,241 --> 00:00:54,324
And even if you don't end up 
using the operator we've created,


00:00:54,324 --> 00:00:58,692
the understandingwe gain from
accomplishing this operator is priceless.


00:00:59,350 --> 00:01:03,122
As you can see, even though
the subdivided geometry looks normal,


00:01:03,122 --> 00:01:07,228
when you start moving the new edge 
points thathave other primitive neighbours


00:01:07,228 --> 00:01:12,106
that are not in the input primitive group, 
you realize thatthey are not properly fused.


00:01:12,718 --> 00:01:17,514
That's because we don't take the neighbour 
primitives into accountwhile subdividing.


00:01:18,187 --> 00:01:21,000
And we can't just grow the input 
primitive group of course,


00:01:21,092 --> 00:01:24,146
as we have tohandle these 
neighbour primitives differently.


00:01:24,797 --> 00:01:27,303
All we have to do is to recreate 
these primitives


00:01:27,303 --> 00:01:29,997
after we have done subdividing the geometry.


00:01:30,640 --> 00:01:33,943
But we still need to handle interpolating 
allincoming attributes


00:01:33,943 --> 00:01:35,926
and preserving all incoming groups.


00:01:36,584 --> 00:01:40,464
Fortunately we have already done this
with the VEX Edge Divide SOP.


00:01:41,015 --> 00:01:43,570
So we will use that code to accomplish this,


00:01:43,570 --> 00:01:47,904
with a minormodification. 
So first create a switch node


00:01:54,880 --> 00:01:57,369
with an expression that will return true,


00:01:57,369 --> 00:02:00,088
ifthe number of primitives in the 
orig_prims group


00:02:00,088 --> 00:02:04,515
is not the same as the total number of
primitives in the current connected piece.


00:02:05,680 --> 00:02:10,160
If they are the same, that means, we are
operating on the entire connected piece,


00:02:10,160 --> 00:02:13,120
and as such we don't have to
worry about neighbour primitives.


00:02:14,240 --> 00:02:15,777
To get the neighbour primitives,


00:02:15,777 --> 00:02:19,254
we will use oneof the newer group nodes,
 called Group Expand.


00:02:26,960 --> 00:02:30,094
So turn on the Require Primitives 
Share Edgeoption


00:02:30,094 --> 00:02:32,325
to expand primitives using Edges.


00:02:33,120 --> 00:02:35,117
Call this group edge_prims.


00:02:37,840 --> 00:02:40,978
Get the difference between this 
group and theorig_prims group,


00:02:40,978 --> 00:02:43,521
and call it nearby_prims group.


00:02:58,904 --> 00:03:02,504
We have to include this group in
the source_prims_index wrangle node,


00:03:03,864 --> 00:03:08,984
primitive_groups attribute inter polate
node and attribute copy node.


00:03:14,717 --> 00:03:17,757
Duplicate the same switch node
we have created previously,


00:03:17,772 --> 00:03:21,052
and connect it after the first
attribute interpolate node.


00:03:21,997 --> 00:03:24,769
It's ideal to recreate the nearby primitives,


00:03:24,769 --> 00:03:27,197
after we move all of the original 
points in place.


00:03:28,000 --> 00:03:30,474
So connect the old VEX Edge Divide polygon


00:03:30,474 --> 00:03:32,640
creation wrangle node we set aside,


00:03:32,747 --> 00:03:36,907
to this switch node. I will rename
it to create_neighbour_prims.


00:03:38,560 --> 00:03:40,578
Create an object merge node that references


00:03:40,578 --> 00:03:42,482
the create_new_edge_points wrangle node,


00:03:42,482 --> 00:03:44,578
and connect it to the second input of


00:03:44,578 --> 00:03:46,400
the create_neighbour_prims wrangle node.


00:03:47,200 --> 00:03:51,120
This is very important. We will
reference this node in code,


00:03:51,120 --> 00:03:53,842
so that we can get the original vertex indices.


00:04:02,800 --> 00:04:07,009
First off, define a variable that references 
thesource_prim_index attribute.


00:04:07,774 --> 00:04:10,292
We have to use this, instead of the @primnum,


00:04:10,292 --> 00:04:13,494
because we now havemore 
primitives than before subdivision.


00:04:14,214 --> 00:04:16,267
So when we are querying the second input,


00:04:16,267 --> 00:04:18,411
we can't use the current primitive indices,


00:04:18,518 --> 00:04:21,153
but rather the original primitive indices


00:04:21,153 --> 00:04:24,860
so that they would refer to the 
originalprimitives in the second input.


00:04:25,166 --> 00:04:27,786
You don't have to replace it 
on removeprim line.


00:04:28,321 --> 00:04:30,863
We replace it in the primindicesarray line,


00:04:30,863 --> 00:04:33,893
because the Attribute Interpolate 
node interpolates


00:04:33,893 --> 00:04:36,493
fromthe geometry before the subdivision.


00:04:36,960 --> 00:04:40,480
That's why we also change the
primvertices and primintrinsic


00:04:40,480 --> 00:04:42,771
lines to query the second input instead.


00:04:43,360 --> 00:04:46,240
For primpoints, we can use the first input,


00:04:46,240 --> 00:04:48,236
because the primitive point indices


00:04:48,236 --> 00:04:50,800
remainthe same before and 
after the subdivision.


00:04:51,600 --> 00:04:55,040
For primvertices, we need
to query the second input,


00:04:55,040 --> 00:04:59,040
because the new vertex indices are
different than the original vertices.


00:04:59,920 --> 00:05:01,280
For primintrinsic,


00:05:01,280 --> 00:05:05,600
it shouldn't matter either way but I changed
it to use the second input, in this case.


00:05:06,720 --> 00:05:10,720
Also rename the primitive group
to nearby_prims and make sure to


00:05:10,720 --> 00:05:13,360
use this group as the input group for this node.


00:05:22,160 --> 00:05:24,987
Now as you can see, we have perfectly preserved


00:05:24,987 --> 00:05:26,919
the boundaries of the input primitive group.


00:05:27,680 --> 00:05:30,183
All the neighbour primitives are rebuilt


00:05:30,183 --> 00:05:32,800
to usethe new points of the 
subdivided primitives.


00:05:33,840 --> 00:05:39,520
We haven't introduced any holes unlike
OpenSubdiv. This is exactly what we wanted


00:05:39,520 --> 00:05:43,040
and is much more useful when
performing geometry operations


00:05:43,040 --> 00:05:46,852
rather than ending up with broken
up polygons all over the geometry.


00:05:54,160 --> 00:05:57,440
You can also exclude these
attributes from Attribute Copy


00:05:57,440 --> 00:06:00,975
to get a bit of performance, as
we don't need to copy them over.


00:06:06,960 --> 00:06:09,902
Let's compare it to OpenSubdiv
bilinear subdivision.


00:06:22,084 --> 00:06:24,637
As you can see, 
the geometry is broken up,


00:06:24,637 --> 00:06:28,104
just like what wehad before, 
until we fixed it in this chapter.


00:06:28,800 --> 00:06:31,360
OpenSubdiv Catmull Clark has 
the exact same


00:06:31,360 --> 00:06:35,760
problem which makes it far less useful
in production when using sub-geometry.


00:06:36,000 --> 00:06:39,287
As this is the case with any form 
of adaptive subdivision.


00:06:40,080 --> 00:06:44,480
Fortunately we won't have this
issue for our custom Subdivide SOP,


00:06:44,480 --> 00:06:48,160
even when we start implementing
Catmull Clark subdivision surfaces.


00:06:49,840 --> 00:06:52,960
We are almost finished with our
bilinear subdivision operator.


00:06:54,560 --> 00:06:57,700
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

99 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Final Changes

----------------------------------------------------------------------------------------------------



00:00:04,481 --> 00:00:06,322
We only have a few things left to do.


00:00:07,128 --> 00:00:09,586
First off, turn on Enable Compiling.


00:00:10,392 --> 00:00:12,330
As you can see, it errors out.


00:00:12,668 --> 00:00:15,365
That's because the last Attribute 
Interpolate node


00:00:15,365 --> 00:00:17,461
still references the outer foreach node.


00:00:20,751 --> 00:00:23,405
We need to reference the inner 
foreach node to fix this.


00:00:24,246 --> 00:00:26,995
I am gonna create a null node to 
reference this instead.


00:00:42,177 --> 00:00:45,261
Another thing we will do is to 
create an object merge node


00:00:45,261 --> 00:00:48,513
that references the same null node 
we just created,


00:00:48,513 --> 00:00:52,625
and then use this as the second input 
for the Attribute Interpolate nodes.


00:00:53,535 --> 00:00:57,388
This way we don't have to exclude 
each extra attribute one by one.


00:01:10,790 --> 00:01:13,543
Create another object merge 
node that references


00:01:13,543 --> 00:01:15,129
source_prim_index wrangle node,


00:01:16,334 --> 00:01:19,743
and use it as the second input 
for the Attribute Copy node.


00:01:49,052 --> 00:01:51,597
As you can see, everything works 
perfectly now.


00:02:12,331 --> 00:02:15,034
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

100 - Custom Subdivision Surfaces - Implementing Bilinear Subdivision - Testing On Complex Geometry

----------------------------------------------------------------------------------------------------



00:00:04,694 --> 00:00:08,010
We are now ready to test bilinear 
subdivision on a real model.


00:00:08,912 --> 00:00:12,265
I brought this model, but feel free 
to use any geometry that you like.


00:00:13,105 --> 00:00:15,777
I also added some point colors 
using the bounding box.


00:00:17,085 --> 00:00:20,435
As you can see, it perfectly 
subdivides the input geometry.


00:00:34,374 --> 00:00:38,251
We can also use a primitive group 
to test sub-geometry subdivision,


00:00:41,684 --> 00:00:43,147
which works just as well.


00:00:44,113 --> 00:00:47,031
We have started with a VEX-based 
Edge Divide SOP,


00:00:47,220 --> 00:00:51,555
and step by step, implemented
bilinear subdivision using VEX.


00:00:52,457 --> 00:00:54,726
Through careful analysis and planning,


00:00:55,076 --> 00:00:57,475
we have solved every issue 
we have encountered


00:00:57,776 --> 00:01:01,191
and created a very robust bilinear 
subdivision operation.


00:01:01,877 --> 00:01:05,548
Now we can confidently build 
Catmull Clark subdivision surfaces


00:01:05,548 --> 00:01:08,731
on top of this, without worrying 
about the foundation,


00:01:09,067 --> 00:01:13,261
which is the advantage of abstracting 
and decoupling each distinct operation


00:01:13,261 --> 00:01:17,771
into its own generic operation, 
and building new functionality using these,


00:01:18,180 --> 00:01:19,753
once they prove to be robust.


00:01:20,830 --> 00:01:24,710
That's what makes working in Houdini 
so versatile and powerful.


00:01:25,504 --> 00:01:26,707
See you guys later.


00:01:26,707 --> 00:01:28,301
In Catmull-Clark Subdivision.






----------------------------------------------------------------------------------------------------

101 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Introduction

----------------------------------------------------------------------------------------------------



00:00:03,149 --> 00:00:10,955
"The limit Catmull-Clark surface of a cube 
can never approach an actual sphere, 
as it's bi-cubic interpolation and a sphere would be quadric."


00:00:13,311 --> 00:00:17,010
Subdivision surfaces are 
piecewise parametric surfaces


00:00:17,010 --> 00:00:20,374
defined over meshes of arbitrary topology.


00:00:20,987 --> 00:00:26,019
It's an algorithm that maps from a 
surface to another more refined surface,


00:00:26,019 --> 00:00:30,261
where the surface is described as a 
set of points and a set of polygons


00:00:30,261 --> 00:00:32,422
with vertices at those points.


00:00:33,090 --> 00:00:37,333
The resulting surface will always 
consist of a mesh of quadrilaterals.


00:00:38,143 --> 00:00:41,195
The most iconic example is to start 
with a cube


00:00:41,195 --> 00:00:44,641
and converge to a spherical surface, 
but not a sphere.


00:00:45,341 --> 00:00:47,983
Because the limit Catmull-Clark 
surface of a cube


00:00:47,983 --> 00:00:50,138
can never approach an actual sphere,


00:00:50,138 --> 00:00:54,318
as it's bi-cubic interpolation and 
a sphere would be quadric.


00:00:54,994 --> 00:00:58,673
The only difference between 
Catmull-Clark and linear subdivision


00:00:58,673 --> 00:01:01,602
is the choice of positions for 
the new vertices.


00:01:02,459 --> 00:01:05,130
Whereas linear subdivision simply 
takes a uniform


00:01:05,130 --> 00:01:07,101
average of the old vertex positions,


00:01:07,392 --> 00:01:10,346
Catmull-Clark uses a very carefully-designed


00:01:10,346 --> 00:01:15,265
weighted average to ensure that the 
surface converges to a smooth surface,


00:01:15,265 --> 00:01:17,813
as the levels of subdivisions increase.


00:01:18,481 --> 00:01:21,863
In these chapters, we will focus 
on computing the weights


00:01:21,863 --> 00:01:24,560
for the new edge points and the 
original points,


00:01:24,560 --> 00:01:28,683
handling various types of geometry 
such as closed surfaces,


00:01:28,683 --> 00:01:34,031
open surfaces, open polygons, 
open polygon curves, mixed topology,


00:01:34,031 --> 00:01:39,699
non-manifold geometry, optimizations 
and finally sampling the surface normals


00:01:39,699 --> 00:01:41,628
from the subdivision limit surface.


00:01:42,281 --> 00:01:44,751
Our implementation of 
Catmull-Clark subdivision


00:01:44,751 --> 00:01:47,216
will be based on OpenSubdiv in Houdini


00:01:47,216 --> 00:01:48,997
with small changes and improvements.


00:01:49,893 --> 00:01:52,067
And with that said, let's dive in.






----------------------------------------------------------------------------------------------------

102 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Closed Surfaces - Rules

----------------------------------------------------------------------------------------------------



00:00:03,089 --> 00:00:07,918
"The real progress is not by speed but momentum."


00:00:08,960 --> 00:00:13,527
First off we will separate topology
changes from point transformations,


00:00:13,798 --> 00:00:18,080
meaning, we will first modify the
topology, then update the point positions.


00:00:19,200 --> 00:00:21,600
So the first stage is: topology changes.


00:00:22,640 --> 00:00:26,160
Now let's look at the formal rules
and illustrate them one by one.


00:00:28,000 --> 00:00:32,160
The wikipedia page for the Catmull-Clark
subdivision defines the rules as such.


00:00:33,280 --> 00:00:36,400
Step 1:
For each face, add a face point.


00:00:43,390 --> 00:00:46,910
Step 2:
For each edge, add an edge point.


00:00:54,960 --> 00:00:56,080
Step 3:


00:00:56,080 --> 00:00:59,579
Connect each new face point 
to the new edge points


00:00:59,579 --> 00:01:02,682
of all original edges defining 
the original face.


00:01:04,720 --> 00:01:08,800
Connect each new vertex point
to the new edge points of all


00:01:08,800 --> 00:01:11,840
original edges incident on the original vertex.


00:01:13,840 --> 00:01:16,800
Define new faces as enclosed by edges.


00:01:20,722 --> 00:01:24,882
This is basically bilinear subdivision,
so we have done all of this.


00:01:25,680 --> 00:01:28,000
The second stage is: moving points.


00:01:29,120 --> 00:01:32,880
What separates Catmull-Clark 
subdivisionfrom bilinear subdivision is,


00:01:32,880 --> 00:01:36,240
where we will move the new edge
points and the original points.


00:01:37,760 --> 00:01:38,800
Step 1:


00:01:38,800 --> 00:01:43,964
Set each face point to be the average of
all original points for the respective face.


00:01:47,921 --> 00:01:50,481
Again this is the same as 
bilinear subdivision.


00:01:51,600 --> 00:01:55,036
Step 2:
Set each edgepoint to be the average 


00:01:55,036 --> 00:01:58,720
of the two neighbouringface points
and its two original endpoints.


00:02:03,495 --> 00:02:06,775
This is where we start to diverge
from bilinear subdivision.


00:02:07,760 --> 00:02:14,560
Let's call the edge point, e, its face
points f0 and f1, and its neighbour points


00:02:14,560 --> 00:02:18,657
that are the original points
in the geometry, p0 and p1.


00:02:20,560 --> 00:02:22,971
The new position of the edge point e,


00:02:22,971 --> 00:02:28,777
will bep0, plus p1, plus f0, plus f1, 
divided by 4.


00:02:29,440 --> 00:02:35,575
Or we can just say p0, plus
p1, plus f0, plus f1, over 4.


00:02:36,401 --> 00:02:39,291
If you apply the same operation 
to all edgepoints,


00:02:39,291 --> 00:02:41,681
their positions will end up like this.


00:02:46,640 --> 00:02:49,520
Step 3:
For each original point P,


00:02:49,520 --> 00:02:51,806
take the average F of all n


00:02:51,806 --> 00:02:55,933
(recently created)face points 
for faces touching P,


00:02:55,933 --> 00:03:00,105
and take the average R of 
all n edge midpoints


00:03:00,105 --> 00:03:02,576
for (original)edges touching P,


00:03:02,576 --> 00:03:07,336
where each edge midpoint is the 
average of its two endpoint vertices


00:03:07,336 --> 00:03:11,216
(notto be confused with new "edge points" above).


00:03:12,000 --> 00:03:17,600
(Note that from the perspective of a vertex
P, the number of edges neighboring P is also


00:03:17,600 --> 00:03:23,815
the number of adjacent faces, hence n).
Move each original point to the point:


00:03:24,240 --> 00:03:29,954
F + 2R + (n-3) * P
---------------------------
n


00:03:30,720 --> 00:03:38,177
This is the barycenter of P, R and F
with respective weights (n  3  ), 2 and 1.


00:03:42,400 --> 00:03:45,153
Let's call the original point p, 
the edge points


00:03:45,153 --> 00:03:51,884
e0, e1, e2 and the face points 
f0, f1, and f2.


00:03:53,840 --> 00:03:56,314
Let's call the new point position, pnew,


00:03:56,314 --> 00:03:58,656
becausewe will also use the 
original point position.


00:03:59,760 --> 00:04:03,200
Even though the wikipedia
article uses different letters,


00:04:03,200 --> 00:04:07,337
I will go by the more commonly used 
lettersfor the Catmull-Clark interpolation.


00:04:08,000 --> 00:04:16,018
pnew is Q over n, plus 2R over n,
plus (n minus 3) times S, over n.


00:04:16,961 --> 00:04:19,121
But what are Q R and S?


00:04:20,442 --> 00:04:22,202
Q is the average of the face points.


00:04:25,120 --> 00:04:27,391
R is the average of the edge points.


00:04:27,391 --> 00:04:31,414
Notto be confused by the edge
point position computed above.


00:04:31,414 --> 00:04:34,750
We are talking about the
average of the original edge positions.


00:04:35,760 --> 00:04:37,817
S is the original point position.


00:04:38,720 --> 00:04:42,811
n is the valence of a vertex, 
which is the numberof vertices


00:04:42,811 --> 00:04:45,376
connected to this vertex by an edge.


00:04:46,080 --> 00:04:49,128
For a simple cube, each original point will


00:04:49,128 --> 00:04:52,676
have a valence of 3. And 3 minus 3 is 0,


00:04:52,676 --> 00:04:56,897
and as such the contribution from
the original point becomes 0.


00:04:58,240 --> 00:05:01,454
If you apply the same operation 
to all originalpoints,


00:05:01,454 --> 00:05:03,680
their positions will end up like this.


00:05:04,720 --> 00:05:08,960
Note that this step and the previous
step are independent of each other,


00:05:08,960 --> 00:05:12,800
meaning they can be done in
parallel, which is perfect for VEX.


00:05:13,840 --> 00:05:16,947
Combining both operations on 
the same geometry


00:05:16,947 --> 00:05:19,819
will yield the familiar subdivided cube shape.


00:05:24,880 --> 00:05:27,889
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

103 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Closed Surfaces - Gathering Edge & Face Points - Concept

----------------------------------------------------------------------------------------------------



00:00:04,320 --> 00:00:07,616
We will now take a look at how to 
interpolatethe new edge points


00:00:07,616 --> 00:00:10,189
and the original points for our purposes,


00:00:10,189 --> 00:00:14,122
as we are doingthings differently than 
a standard textbook implementation,


00:00:14,577 --> 00:00:17,714
because thisis VEX and we want 
the max performance.


00:00:18,560 --> 00:00:20,474
To interpolate the new edge points,


00:00:20,474 --> 00:00:23,298
otherimplementations might 
keep track of the faces


00:00:23,298 --> 00:00:26,597
that belong to each edge point as well as the edge theyare on,


00:00:26,851 --> 00:00:29,550
and get its end points using this edge.


00:00:30,382 --> 00:00:32,000
But we don't care about these,


00:00:32,158 --> 00:00:36,211
as the contribution of the original
points and the face points are equal.


00:00:37,120 --> 00:00:42,880
We only need to gather all of these in a
single array. Fortunately, this is quite easy


00:00:42,880 --> 00:00:47,722
as all of these points, p0, p1, f0, and f1


00:00:47,836 --> 00:00:50,274
are all edge neighbours to the 
new edge point.


00:00:51,120 --> 00:00:55,056
That means we can just use the neighbours
function for each new edge point.


00:01:00,105 --> 00:01:04,585
Interpolating the original points is a bit
more involved but still very easy to do.


00:01:05,440 --> 00:01:09,840
This time we need to differentiate between
the edge points and the face points.


00:01:10,880 --> 00:01:14,640
But don't keep track of any of these,
so how can we know which is which.


00:01:16,240 --> 00:01:19,920
All we have to do is to
first get the edge points e0,


00:01:19,920 --> 00:01:23,558
e1 and e2, the same way we 
have seen previously,


00:01:23,558 --> 00:01:25,612
which is by using the neighbours function.


00:01:26,480 --> 00:01:30,160
If you recall from the previous
chapters, we have seen how to get


00:01:30,160 --> 00:01:32,114
point neighbours using primitives


00:01:32,114 --> 00:01:33,120
insteadof edges.


00:01:34,066 --> 00:01:35,660
So we will do the same here,


00:01:35,660 --> 00:01:38,370
which is by gathering the 
primitives of theoriginal point,


00:01:38,370 --> 00:01:40,173
using the point_prims function.


00:01:40,720 --> 00:01:44,320
Then gather all the points of
these primitives in another array,


00:01:45,440 --> 00:01:48,534
and finally remove all the edge
points from thisarray,


00:01:48,709 --> 00:01:52,095
which will leave us with the 
face points. Simple as that.


00:01:52,880 --> 00:01:54,018
As you can see,


00:01:54,018 --> 00:01:57,360
all these concepts andtechniques we 
have seen throughout the course,


00:01:57,439 --> 00:01:59,164
they build on top of each other,


00:01:59,164 --> 00:02:02,159
as they aremeant to be used in 
conjunction with each other.


00:02:03,198 --> 00:02:05,746
That's the technical skill I am trying to convey


00:02:05,746 --> 00:02:08,922
throughout this course, not how to do X or Y,


00:02:09,045 --> 00:02:12,603
but increase your technical capacity
and problem solving prowess.


00:02:14,320 --> 00:02:17,291
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

104 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Closed Surfaces - Gathering Edge & Face Points - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,000 --> 00:00:08,735
We will continue from where we left 
off with the last bilinear subdivision chapter.


00:00:09,258 --> 00:00:11,902
Delete everything except the 
subdivide subnet.


00:00:12,665 --> 00:00:15,878
To truly appreciate the 
Catmull-Clark subdivision algorithm,


00:00:15,878 --> 00:00:18,550
we have to start with the most 
basic geometry first,


00:00:18,550 --> 00:00:21,317
and see it converge to a smoother surface.


00:00:22,069 --> 00:00:23,424
So create a box node.


00:00:29,955 --> 00:00:32,586
We can now switch to the 
Catmull-Clark algorithm


00:00:32,670 --> 00:00:35,640
so we can implement the 
Catmull-Clark related functionality.


00:00:36,403 --> 00:00:40,875
First turn off Enable Compiling so 
we can debug the network easier.


00:00:41,701 --> 00:00:45,117
Create a group promote node and 
convert the orig_prims group


00:00:45,117 --> 00:00:47,530
to a point group called orig_pts.


00:00:50,468 --> 00:00:53,408
Connect this node right after the
first Group Promote node.


00:00:54,443 --> 00:00:57,748
Create a switch node and link the 
algorithm to this node


00:00:58,000 --> 00:01:00,863
and make sure to connect the newly 
created group promote node


00:01:00,863 --> 00:01:05,625
to the second input, so it's only cooked 
if the algorithm is set to Catmull-Clark.


00:01:06,587 --> 00:01:09,101
I will also color the Catmull-Clark
related nodes


00:01:09,101 --> 00:01:12,460
in yellow to differentiate them 
easier in the network editor.


00:01:13,526 --> 00:01:14,728
Duplicate the switch node


00:01:14,728 --> 00:01:17,919
and connect it after the
create_new_primitives attribute wrangle node.


00:01:19,058 --> 00:01:22,276
Create another attribute wrangle 
node to get the edge neighbours,


00:01:24,962 --> 00:01:27,783
and connect this to the second 
input of the switch node.


00:01:28,807 --> 00:01:30,854
As we have seen previously in the concept,


00:01:30,854 --> 00:01:33,638
we get the edge neighbours the 
exact same way


00:01:33,638 --> 00:01:36,793
for both the original points and the 
new edge points.


00:01:37,525 --> 00:01:40,873
So set the group to orig_pts and 
edge_pts groups.


00:01:41,542 --> 00:01:45,747
Call the neighbours function and store 
the resulting array as edge_neighbours.


00:01:48,776 --> 00:01:51,753
As you can see, we have the edge 
neighbours attribute now.


00:01:52,715 --> 00:01:55,505
Create another wrangle node, 
to get the face neighbours.


00:01:56,080 --> 00:01:59,496
This time we will only run this on 
the original points


00:02:00,000 --> 00:02:02,082
so set the group to orig_pts.


00:02:04,496 --> 00:02:06,614
Store the edge neighbours in a variable.


00:02:11,650 --> 00:02:15,547
First get the primitives of the current 
point using the point_prims function.


00:02:18,000 --> 00:02:19,462
Loop over these primitives


00:02:21,740 --> 00:02:24,550
and get its points using the 
prim_points function.


00:02:26,682 --> 00:02:28,122
Loop over these points.


00:02:30,000 --> 00:02:33,124
If the current primitive_point is
not the current point,


00:02:33,270 --> 00:02:36,024
it's not found in the edge_points array,


00:02:37,853 --> 00:02:40,345
it's not found in the face_points array


00:02:42,383 --> 00:02:45,917
and it's inside the face_pts group 
we have created previously,


00:02:46,816 --> 00:02:49,869
then append this point to the 
face_points array.


00:02:50,925 --> 00:02:53,729
The last part is important for sub-geometry.


00:02:54,293 --> 00:02:57,864
In this case, you don't want to include 
the primitive points of the primitives,


00:02:57,864 --> 00:03:00,934
that are not included in the input 
primitive group.


00:03:02,000 --> 00:03:04,067
Store the resulting face points.


00:03:08,247 --> 00:03:10,846
As you can see we have the face points now.


00:03:13,375 --> 00:03:16,268
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

105 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Closed Surfaces - Computing Weights for New Edge Points - Concept

----------------------------------------------------------------------------------------------------



00:00:04,211 --> 00:00:07,841
Now we will look at the most important 
part of Catmull-Clark subdivision,


00:00:07,841 --> 00:00:11,332
and that's computing the weights we 
will use for the new edge points.


00:00:12,209 --> 00:00:15,074
As usual we start with the easier 
elements first


00:00:15,204 --> 00:00:17,147
and gradually build up to the end result.


00:00:18,000 --> 00:00:21,235
As you recall, we don't compute 
the point positions manually.


00:00:22,000 --> 00:00:24,732
We compute the weights for 
each contributing element


00:00:24,886 --> 00:00:28,069
so that the Attribute Interpolate nodes 
do everything for us.


00:00:28,774 --> 00:00:32,486
That means we have to precisely figure 
out what weight values to use


00:00:32,486 --> 00:00:34,000
for each contributing element.


00:00:35,204 --> 00:00:38,396
Computing the weights for the new 
edge points is very straight forward.


00:00:39,457 --> 00:00:43,557
As you recall, their final position is the
 average of all of their point neighbours


00:00:43,557 --> 00:00:44,408
using edges.


00:00:44,645 --> 00:00:47,163
That would make the contribution 
of each of the neighbours,


00:00:47,276 --> 00:00:48,838
1 over the edge count.


00:00:49,751 --> 00:00:51,803
So if an edge point has 4 neighbours,


00:00:51,975 --> 00:00:54,850
each of them would contribute only 1 over 4,


00:00:54,850 --> 00:00:58,357
and as such their weight would be 0.25 each.


00:01:00,039 --> 00:01:02,651
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

106 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Closed Surfaces - Computing Weights of New Edge Points - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,400 --> 00:00:09,600
First create a Group Combine node
for non-boundary edge points. For now


00:00:09,600 --> 00:00:11,680
it will be the same as the edge_pts group.


00:00:12,400 --> 00:00:15,191
Later on we will exclude some 
points from this.


00:00:15,819 --> 00:00:18,068
But it's better to start using this group now.


00:00:18,960 --> 00:00:21,600
Duplicate the same Catmull-Clark 
switch node and


00:00:21,600 --> 00:00:24,400
connect it after the bilinear
attribute interpolate node.


00:00:25,520 --> 00:00:29,440
This is where we will compute the new
weights for the Catmull-Clark subdivision,


00:00:29,440 --> 00:00:32,800
right after we have finished
performing bilinear interpolation.


00:00:33,942 --> 00:00:35,462
So create an attribute wrangle


00:00:39,680 --> 00:00:40,800
and set the group to the


00:00:40,800 --> 00:00:43,840
new non_boundary_edge_points
group we have just created.


00:00:44,960 --> 00:00:48,960
Make sure to update the node connections
of the nodes that come after the bilinear


00:00:48,960 --> 00:00:52,074
attribute interpolate node, to
use this wrangle node instead.


00:00:53,840 --> 00:00:57,840
As shown in the concept, we first
get the point neighbours using edges,


00:00:59,914 --> 00:01:02,474
and divide 1 by the length of this array.


00:01:07,600 --> 00:01:10,305
For each neighbour element
in the neighbours array,


00:01:10,480 --> 00:01:14,320
append the same weight value we have
just computed so that the number of


00:01:14,320 --> 00:01:15,890
weight values in the weights array


00:01:16,000 --> 00:01:18,110
is the same as the number 
of neighbour elements.


00:01:19,600 --> 00:01:22,320
Store both of these arrays as array attributes.


00:01:25,120 --> 00:01:27,860
Duplicate the bilinear attribute 
interpolate node


00:01:27,943 --> 00:01:30,323
and connect it right after this wrangle node.


00:01:32,080 --> 00:01:35,120
This is where we will perform
the Catmull-Clark interpolation


00:01:35,120 --> 00:01:38,320
for the original points and
the non-boundary edge points.


00:01:39,520 --> 00:01:41,944
Make sure to update the 
spare input references


00:01:42,000 --> 00:01:43,976
to the wrangle node just before itself.


00:01:45,120 --> 00:01:50,460
Update the point attributes list to exclude
the sourcept_indices, sourcept_weights,


00:01:54,640 --> 00:02:00,353
edge_pts, edge_neighbours,
face_pts, and primcenterpt.


00:02:12,191 --> 00:02:14,431
And set the group to non-boundary 
edge points.


00:02:18,400 --> 00:02:21,290
As you can see, we got the exact 
same result as


00:02:21,290 --> 00:02:22,530
the concept drawing,


00:02:23,278 --> 00:02:25,980
so only the original points 
are left to be interpolated,


00:02:25,980 --> 00:02:28,308
to converge to a spherical surface.


00:02:30,240 --> 00:02:33,091
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

107 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Closed Surfaces - Computing Weights for Original Points - Concept

----------------------------------------------------------------------------------------------------



00:00:04,480 --> 00:00:07,840
Computing the weights for the original
points is a lot more involved.


00:00:08,640 --> 00:00:11,010
Looking at the formula we 
have seen previously,


00:00:11,010 --> 00:00:12,584
we have to transform it in a way


00:00:12,584 --> 00:00:15,306
that can be used by the attribute 
interpolate node,


00:00:15,306 --> 00:00:18,971
so thesame interpolation is applied 
to all numerical attributes,


00:00:18,971 --> 00:00:22,104
rather than us manually
applying the same formula ourselves.


00:00:22,961 --> 00:00:23,921
How can we do this?


00:00:24,640 --> 00:00:26,316
Let's start with Q first.


00:00:28,218 --> 00:00:30,356
Q is the averageof all the face points.


00:00:30,978 --> 00:00:35,495
In this case we divide the sum of 
all theface point positions, by 3.


00:00:35,495 --> 00:00:36,951
But it's not always 3.


00:00:36,951 --> 00:00:39,227
It depends on the number of
face points,


00:00:39,365 --> 00:00:41,545
so we actually divide the sum by n.


00:00:43,040 --> 00:00:47,363
So Q is f0 plus f1, plus f2, over n.


00:00:49,520 --> 00:00:53,029
Now let's substitute the new Q, 
in Q overn,


00:00:53,029 --> 00:00:55,190
in the original Catmull-Clark formula.


00:01:03,600 --> 00:01:10,757
Following basic algebra rules, we arrive
at f0 plus f1, plus f2, over n squared.


00:01:12,480 --> 00:01:16,728
As you know f0, f1, and f2 
are the face points,


00:01:16,931 --> 00:01:19,163
so we can just call them that collectively.


00:01:19,840 --> 00:01:23,280
We will pass them as an array to
the attribute interpolate node,


00:01:23,280 --> 00:01:27,600
which will multiply each element by the
weight value we provide for each element.


00:01:28,560 --> 00:01:33,998
So we can write the formula as: 1 over n
squared, multiplied by the face points.


00:01:40,641 --> 00:01:44,081
So the weight of the face points
becomes: 1 over n squared.


00:01:44,880 --> 00:01:49,040
So for each face point that an original
point has, we will add this weight


00:01:49,040 --> 00:01:50,575
value to the weights array,


00:01:50,575 --> 00:01:53,526
that will beused by the attribute 
interpolate node.


00:01:54,160 --> 00:01:55,563
Now let's look at R.


00:01:56,899 --> 00:01:58,870
R is theaverage of all the edge points.


00:01:59,520 --> 00:02:03,680
Just like the face points, in this
case, we are dividing them by 3,


00:02:03,680 --> 00:02:05,201
but it's not always 3.


00:02:05,662 --> 00:02:07,928
It depends on the number of
edge points,


00:02:08,481 --> 00:02:10,642
so we actually divide the sum by n.


00:02:11,544 --> 00:02:14,664
So e0 plus e1, plus e2, over n.


00:02:17,361 --> 00:02:20,777
Now let's substitute the new R, 
in 2R overn,


00:02:20,777 --> 00:02:22,839
in the original Catmull-Clark formula.


00:02:25,063 --> 00:02:28,350
Again following basic algebra rules,
we arriveat


00:02:28,972 --> 00:02:33,351
2 times (e0 plus e1, plus e2) over n squared.


00:02:36,400 --> 00:02:40,721
As you know e0, e1 and e2 
are the edge points,


00:02:40,963 --> 00:02:43,282
so we can just call them that collectively.


00:02:46,720 --> 00:02:50,193
So we can write the formula as: 
2 over nsquared,


00:02:50,193 --> 00:02:51,680
multiplied by the edge points.


00:02:56,160 --> 00:03:01,582
So the weight of the edge points
becomes: 2 over n squared.


00:03:05,760 --> 00:03:09,878
Finally let's look at S, which
is the original point position.


00:03:19,235 --> 00:03:21,795
Let's call it p, 
just like in the drawing above.


00:03:22,972 --> 00:03:24,985
That makes the weight of 
the originalpoint,


00:03:24,985 --> 00:03:26,732
which is known as the control weight,


00:03:31,601 --> 00:03:33,534
n minus 3, over n.


00:03:34,400 --> 00:03:37,023
Because when we add the index 
of the originalpoint


00:03:37,023 --> 00:03:41,032
to the list of points to interpolate from,
 attribute interpolate


00:03:41,032 --> 00:03:42,775
willuse its position directly,


00:03:42,948 --> 00:03:46,573
so we don't need to alter the Catmull-Clark
formula for the original point.


00:03:47,784 --> 00:03:51,464
That's why we can use the exact
same weight as in the formula.


00:03:54,400 --> 00:03:57,213
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

108 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Closed Surfaces - Computing Weights of Original Points - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,160 --> 00:00:06,116
First create an attribute wrangle node


00:00:06,116 --> 00:00:09,248
to computethe catmull clark 
weights for the original points.


00:00:21,280 --> 00:00:24,000
Store references to the edge
neighbours that contain the


00:00:24,000 --> 00:00:27,380
edge points of the original
points and the face points.


00:00:30,160 --> 00:00:33,440
Let's define n which is
valence, as the edge count.


00:00:34,400 --> 00:00:36,736
Let's define the weight values just like what


00:00:36,736 --> 00:00:38,960
we have seen previously in 
the concept video.


00:00:39,920 --> 00:00:42,705
Face weight is 1 over n squared.


00:00:44,628 --> 00:00:47,348
Edge weight is 2 over n squared.


00:00:49,382 --> 00:00:52,524
Control weight is n minus 3, over n.


00:00:53,280 --> 00:00:58,800
Now define an integer array for the point
indices and a float array for the weights.


00:00:59,680 --> 00:01:04,880
Populate them with the current point and
control weight respectively. The first


00:01:04,880 --> 00:01:07,920
elements will be the original
point itself and its weight.


00:01:08,720 --> 00:01:11,343
Now append the edge points to 
the index array,


00:01:12,115 --> 00:01:14,182
and the edge weight to the weights array.


00:01:16,480 --> 00:01:19,610
Then do the same for the face
points and the face weight.


00:01:29,120 --> 00:01:33,040
At this point, each original
point has all the point indices


00:01:33,040 --> 00:01:34,737
and the appropriate weight values


00:01:34,737 --> 00:01:37,839
to interpolatefrom, using the 
attribute interpolate node.


00:01:39,280 --> 00:01:42,066
Include the original points group, orig_pts


00:01:42,066 --> 00:01:44,400
inthe Catmull-Clark attribute 
interpolate node.


00:01:45,280 --> 00:01:48,514
As you recall, the transformation 
of theoriginal points


00:01:48,514 --> 00:01:51,381
and the edge points are independent 
of each other,


00:01:51,381 --> 00:01:53,277
sothey can be done concurrently.


00:01:53,947 --> 00:01:56,102
This is the moment we have been waiting for.


00:01:56,720 --> 00:01:58,506
Before we increase the iterations,


00:01:58,506 --> 00:02:02,780
let's firstadd bounding box colors to 
show that all numerical point attributes


00:02:02,780 --> 00:02:06,412
are properly interpolated
using the Catmull-Clark interpolation.


00:02:09,920 --> 00:02:11,137
As you can see,


00:02:11,137 --> 00:02:14,529
we have the exact same result as
the built-in Subdivide SOP.


00:02:14,893 --> 00:02:16,364
This is remarkable.


00:02:16,845 --> 00:02:18,285
You might notice a slight difference,


00:02:18,800 --> 00:02:21,008
but this is just the viewport triangulation


00:02:21,008 --> 00:02:23,898
that's influenced by the difference 
in the order of the points


00:02:23,898 --> 00:02:27,840
we have chosen vs the
point order used by the default subdivide.


00:02:28,720 --> 00:02:33,040
I mentioned before, the point order used
by the default Subdivide seems to differ,


00:02:33,040 --> 00:02:34,856
when not using a single polygon.


00:02:35,741 --> 00:02:38,989
In any case, triangulation is not 
something we have to worry about.


00:02:39,736 --> 00:02:41,336
Let's increase the iterations now.


00:02:41,840 --> 00:02:44,331
As we increase the iterations, 
you can seethat we


00:02:44,331 --> 00:02:46,319
are converging to a spherical surface.


00:02:47,040 --> 00:02:50,960
It's really rewarding to see our
hard work finally becoming a reality.


00:02:51,680 --> 00:02:53,440
If I am not wrong, this is probably


00:02:53,440 --> 00:02:57,089
the first public implementation of
Catmull-Clark subdivision in VEX.


00:03:06,160 --> 00:03:10,080
You can see we have the exact same
result as the default subdivide SOP.


00:03:10,080 --> 00:03:12,846
Not just position but other point attributes


00:03:12,846 --> 00:03:13,980
like color as well.


00:03:14,638 --> 00:03:17,046
That proves that we have 
successfully performed


00:03:17,046 --> 00:03:20,408
Catmull-Clark interpolation
on all numerical point attributes,


00:03:20,408 --> 00:03:21,568
not just P.


00:03:21,568 --> 00:03:24,255
Which is why using the attribute 
interpolate nodes


00:03:24,255 --> 00:03:26,925
gives us, not only maximum performance,


00:03:26,925 --> 00:03:30,081
they also make handling of 
multiple attributes a breeze,


00:03:30,455 --> 00:03:32,982
something that takes a lot of 
work to do in VEX.


00:03:33,680 --> 00:03:37,200
We have reached a milestone. 
Seeing a simple cube


00:03:37,279 --> 00:03:40,479
evolve to a bi-cubic surface is
quite a technical achievement.


00:03:41,360 --> 00:03:43,640
Everything that comes after this, will build


00:03:43,640 --> 00:03:46,616
on the robust base we have created so far.


00:03:47,840 --> 00:03:50,849
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

109 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Attribute Interpolation - Concept

----------------------------------------------------------------------------------------------------



00:00:04,000 --> 00:00:06,580
We have implemented 
Catmull-Clark interpolation


00:00:06,580 --> 00:00:08,611
for all numerical point attributes.


00:00:08,611 --> 00:00:10,436
But what about other attributes?


00:00:11,314 --> 00:00:13,476
This is what we will talk about 
in this chapter.


00:00:21,538 --> 00:00:24,413
As you recall, we don't interpolate 
the primitive attributes.


00:00:25,774 --> 00:00:28,766
For the vertex attributes, we have 
to make a decision


00:00:28,766 --> 00:00:31,032
as to which type of interpolation to use.


00:00:31,660 --> 00:00:35,279
Even the subdivide node treats 
the vertex attributes differently


00:00:35,279 --> 00:00:36,959
based on the algorithm used.


00:00:37,740 --> 00:00:40,575
Houdini Catmull-Clark uses 
Catmull-Clark interpolation


00:00:40,575 --> 00:00:43,292
for the vertex attributes which 
we will see soon.


00:00:43,292 --> 00:00:47,227
Whereas OpenSubdiv Catmull-Clark 
uses bilinear interpolation,


00:00:49,686 --> 00:00:50,991
which is what we will use.


00:00:51,700 --> 00:00:52,906
You might wonder why?


00:00:53,268 --> 00:00:58,235
Intuitively it feels natural to treat 
vertex attributes as point attributes,


00:00:58,678 --> 00:01:02,802
but this actually limits the versatility 
of the operator rather than enhance it.


00:01:03,664 --> 00:01:07,824
As you know, there are more vertices 
than there are points in most geometry.


00:01:08,726 --> 00:01:12,491
Houdini Catmull-Clark algorithm 
implicitly fuses the vertices


00:01:12,491 --> 00:01:13,967
so they act like points.


00:01:14,957 --> 00:01:16,645
So if you have a vertex attribute


00:01:16,645 --> 00:01:19,699
that has the same values as 
the point position attribute P,


00:01:19,699 --> 00:01:23,735
applying Houdini Catmull-Clark 
algorithm on this vertex attribute


00:01:23,735 --> 00:01:27,338
will give you the exact values as
applying them on the point positions.


00:01:28,280 --> 00:01:29,773
But if you really want this,


00:01:29,773 --> 00:01:32,540
then you might as well promote 
these vertex attributes


00:01:32,540 --> 00:01:35,734
to point attributes, apply 
Catmull-Clark subdivision


00:01:35,734 --> 00:01:37,798
and then promote them back to vertices.


00:01:37,798 --> 00:01:39,741
You would get the exact same result.


00:01:40,361 --> 00:01:44,350
As you recall, our implementation of 
Catmull-Clark subdivision is based on


00:01:44,350 --> 00:01:47,006
OpenSubdiv in Houdini, and as such


00:01:47,006 --> 00:01:51,279
we are also matching the behaviour 
of OpenSubdiv for vertex attributes.


00:01:53,450 --> 00:01:56,000
And with that said, see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

110 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Attribute Interpolation - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,501 --> 00:00:08,115
First create an attribute wrangle 
node before the subdivide node


00:00:08,115 --> 00:00:11,204
and add a new attribute that's the same as P.


00:00:12,160 --> 00:00:15,145
Create another wrangle node 
after the subdivide node,


00:00:15,145 --> 00:00:18,538
and read the copy of P, and write it back to P.


00:00:25,672 --> 00:00:28,929
As you can see, both point 
positions are identical.


00:00:29,268 --> 00:00:31,312
Because the attribute interpolate node


00:00:31,312 --> 00:00:35,399
applied the exact same Catmull-Clark
interpolation to both attributes.


00:00:36,245 --> 00:00:39,619
You can see, the color attribute 
is also properly interpolated


00:00:39,619 --> 00:00:40,794
using Catmull-Clark.


00:00:41,391 --> 00:00:45,011
Now create a new vertex wrangle 
node that stores a copy of P.


00:00:50,786 --> 00:00:54,792
Use point split SOP to split the 
geometry using vertices.


00:00:55,608 --> 00:00:58,440
Now we have the same number of 
points and vertices.


00:00:59,107 --> 00:01:02,993
Promote the vertex attribute we have 
just created to a point attribute.


00:01:03,660 --> 00:01:07,927
Create another attribute wrangle node 
to read from the previously-vertex attribute


00:01:08,146 --> 00:01:09,726
and write it back to P.


00:01:20,984 --> 00:01:23,889
As you can see, Houdini 
Catmull-Clark algorithm


00:01:23,889 --> 00:01:25,930
applies Catmull-Clark interpolation


00:01:25,930 --> 00:01:27,560
to the vertex attributes,


00:01:28,068 --> 00:01:29,702
just like the point attributes.


00:01:34,511 --> 00:01:38,953
When we switch to OpenSubdiv, you 
can see the vertices are not interpolated


00:01:38,953 --> 00:01:40,978
using the Catmull-Clark interpolation,


00:01:41,376 --> 00:01:44,001
but rather bilinear interpolation.


00:01:44,634 --> 00:01:45,966
But there is more than this.


00:01:46,942 --> 00:01:50,509
OpenSubdiv gives us several options 
for vertex attributes.


00:01:51,296 --> 00:01:54,581
The Smooth Everywhere option 
doesn't seem very practical to me.


00:01:55,368 --> 00:01:57,429
All the other options look the same.


00:02:02,364 --> 00:02:06,823
If we compare it to OpenSubdiv Bilinear, 
you can see that the result is identical


00:02:06,823 --> 00:02:10,577
to the OpenSubdiv Catmull-Clark 
for the vertex attributes.


00:02:11,622 --> 00:02:16,168
So you know it's bilinear interpolation 
that's used for the vertices, nothing else.


00:02:34,566 --> 00:02:37,041
Let's look at what our subdivide 
node gives us.


00:02:38,276 --> 00:02:41,390
Exactly the same result as 
OpenSubdiv Catmull-Clark.


00:02:42,475 --> 00:02:44,126
So the only thing we have to do,


00:02:44,126 --> 00:02:48,805
to match OpenSubdiv for the vertex 
attribute interpolation, is to do nothing.


00:02:50,229 --> 00:02:53,786
Probably the easiest implementation 
we have ever done in this course.


00:02:54,662 --> 00:02:57,591
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

111 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Boundary Interpolation Rules for New Edge Points - Concept

----------------------------------------------------------------------------------------------------



00:00:04,222 --> 00:00:05,834
In catmull clark subdivision,


00:00:05,834 --> 00:00:08,922
handling manifoldpolygon 
surfaces is only the beginning.


00:00:09,463 --> 00:00:11,857
Now we will look at the 
boundaryinterpolation rules


00:00:12,000 --> 00:00:13,272
for the new edge points.


00:00:13,980 --> 00:00:17,177
Not always you can find allthe 
information you need in one place. 


00:00:18,032 --> 00:00:21,770
As you have seen the Wikipedia
article barely touches the surface.


00:00:22,380 --> 00:00:25,186
So I find another website called Rosetta Code


00:00:25,249 --> 00:00:28,882
that dives a bit deeper into the
catmull clark subdivision algorithm.


00:00:28,882 --> 00:00:30,831
As it's described on this page:


00:00:32,428 --> 00:00:35,965
On the border of a hole the
subdivision occurs as follows:


00:00:37,738 --> 00:00:40,270
for the edges that are on the border of a hole,


00:00:40,270 --> 00:00:42,808
the edge point is just the middle of the edge.


00:00:47,254 --> 00:00:48,806
That means we only take the


00:00:48,806 --> 00:00:53,506
average of p0 and p1, which is what
bilinear interpolation already gives us.


00:00:59,506 --> 00:01:01,505
So no Catmull-Clark interpolation.


00:01:01,505 --> 00:01:04,262
Whichalso means, we exclude these points


00:01:04,262 --> 00:01:05,842
from the non-boundary edge points.


00:01:08,575 --> 00:01:10,945
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

112 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Boundary Interpolation Rules for New Edge Points - Implementation

----------------------------------------------------------------------------------------------------



00:00:06,080 --> 00:00:07,760
Delete one of the faces of the box.


00:00:15,600 --> 00:00:18,706
Using the default subdivide
SOP, this is what we should get.


00:00:22,880 --> 00:00:25,150
As you can see our Subdivide node


00:00:25,150 --> 00:00:27,058
doesn'thandle the border points correctly.


00:00:27,600 --> 00:00:28,467
So let's fix that.


00:00:29,680 --> 00:00:34,000
Copy the default subdivide node and
paste it inside our subdivide subnet


00:00:34,000 --> 00:00:38,688
so we can see it as a reference in template
mode, without having to pin the viewport.


00:00:39,600 --> 00:00:44,000
As you can see, our border points are
shrinked away from the border, as is.


00:00:44,848 --> 00:00:47,088
First we need to get the border points.


00:00:48,000 --> 00:00:51,177
We will do it the same way we did
it for the adaptive subdivide.


00:00:52,080 --> 00:00:55,600
So first create a primitive wrangle
node, and connect it right after


00:00:55,600 --> 00:01:00,248
the create_new_primitives wrangle
node. Set the group to origprims.


00:01:01,520 --> 00:01:07,114
Create an integer attribute for each primitive
and set it to 1. I call it is_orig_prim.


00:01:08,080 --> 00:01:12,306
Create a Group from Attribute Boundary
node and set the group to orig_prims.


00:01:16,480 --> 00:01:20,400
Set attribute to is_orig_prim. If you recall,


00:01:20,400 --> 00:01:24,815
having this attribute was critical to get
the correct borders from a primitive group.


00:01:25,680 --> 00:01:28,644
I call the output group orig_prim_border_edges.


00:01:34,320 --> 00:01:38,906
Promote this edge group to a point
group, called orig_prim_border_pts.


00:01:54,560 --> 00:01:56,996
You can see, we have the border edges now.


00:02:01,200 --> 00:02:04,414
The non_boundary_edge_points 
group has 12 points.


00:02:13,760 --> 00:02:17,168
As you can see, the border
points are also included.


00:02:18,000 --> 00:02:21,840
We need to exclude these points from
the non_boundary_edge_points group.


00:02:22,640 --> 00:02:25,015
So select the non_boundary_edge_points node


00:02:25,015 --> 00:02:27,707
and subtract the orig_prim_border_pts group.


00:02:30,240 --> 00:02:33,013
As you can see, the border points are gone.


00:02:38,118 --> 00:02:40,518
We now have 8 points instead of 12.


00:02:42,000 --> 00:02:44,160
Once we cook the end node in the network,


00:02:44,160 --> 00:02:48,000
you can see the edge points are at
the correct positions as OpenSubdiv.


00:02:49,600 --> 00:02:52,843
Now it's time to fix the
original points at the borders.






----------------------------------------------------------------------------------------------------

113 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Boundary Interpolation Rules for Original Points - Concept

----------------------------------------------------------------------------------------------------



00:00:04,121 --> 00:00:05,954
Let's continue from the same website.


00:00:07,592 --> 00:00:10,194
- for the vertex points that are 
on the border of a hole,


00:00:10,194 --> 00:00:12,910
the new coordinates are 
calculated as follows:


00:00:13,227 --> 00:00:15,330
We are talking about the 
original points here.


00:00:17,602 --> 00:00:19,749
- in all the edges the point belongs to,


00:00:19,749 --> 00:00:22,585
only take in account the 
middles of the edges


00:00:22,585 --> 00:00:24,122
that are on the border of the hole


00:00:24,835 --> 00:00:28,547
That means, for any original point 
that is on the border of a hole,


00:00:28,547 --> 00:00:32,222
only consider the edge points, 
that are also on the border of a hole.


00:00:33,569 --> 00:00:36,812
- calculate the average between 
these points (on the hole boundary)


00:00:36,812 --> 00:00:39,577
and the old coordinates 
(also on the hole boundary).


00:00:40,995 --> 00:00:43,911
This just means, take the average 
of the edge points


00:00:43,911 --> 00:00:46,242
that are on the border of a hole, 
and then


00:00:46,242 --> 00:00:48,799
take the average between 
this average position


00:00:48,799 --> 00:00:50,638
and the original point position.


00:00:51,792 --> 00:00:54,544
- For edges and vertices not next to a hole,


00:00:54,685 --> 00:00:57,092
the standard algorithm from above is used.


00:00:58,087 --> 00:01:00,719
Any points that are not on the 
border of a hole,


00:01:00,719 --> 00:01:02,762
no special handling is required.


00:01:03,651 --> 00:01:05,587
Let's break all of this down now.


00:01:12,627 --> 00:01:15,812
For the original point p that's on 
the border of a hole,


00:01:15,812 --> 00:01:19,636
we will only consider e0 and e1. 
Take their average.


00:01:19,636 --> 00:01:23,023
And then take the average 
between this average position


00:01:23,023 --> 00:01:24,962
and the original point position.


00:01:26,045 --> 00:01:27,573
Let's call it p_new.


00:01:28,938 --> 00:01:29,988
pnew is :


00:01:29,988 --> 00:01:35,388
p + (e0 + e1) / 2
---------------------------



00:01:36,586 --> 00:01:41,759
This becomes:
p / 2 + (e0 + e1) / 4


00:01:42,199 --> 00:01:48,880
which becomes:
(1 / 2) * p + (1/4) * (e0 + e1)


00:01:49,400 --> 00:01:51,382
Therefore the control weight is


00:01:54,851 --> 00:01:57,146
1/2, which is 0.5


00:01:57,815 --> 00:01:59,180
The edge weight is


00:02:02,147 --> 00:02:04,999
1/4, which is 0.25


00:02:06,443 --> 00:02:08,127
The face weight is 0


00:02:08,928 --> 00:02:12,428
We won't consider any face points 
for any original point,


00:02:12,428 --> 00:02:14,147
that's on the border of a hole.


00:02:15,943 --> 00:02:18,893
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

114 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Boundary Interpolation Rules for Original Points - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,000 --> 00:00:05,920
Continuing from where we left off.


00:00:05,920 --> 00:00:09,680
Create a new attribute wrangle
node after the Group Promote node


00:00:09,680 --> 00:00:11,440
we created in the previous lesson.


00:00:12,560 --> 00:00:14,160
Set group to orig_pts.


00:00:15,680 --> 00:00:21,040
Create an attribute called is_boundary_pt,
that will check if the current point is


00:00:21,040 --> 00:00:25,924
in the orig_prim_border_pts group, that
we have created in the previous lesson.


00:00:26,927 --> 00:00:31,204
If the current point is in this group,
that means, it's a boundary point.


00:00:32,160 --> 00:00:35,288
As you can see, point 6 is a boundary point.


00:00:37,377 --> 00:00:41,593
Butpoint 5 is not. Point 3 is 
also a boundary point.


00:00:42,640 --> 00:00:45,682
Create a new attribute wrangle 
node, and connectit


00:00:45,682 --> 00:00:47,832
right after the face_points wrangle node.


00:00:50,960 --> 00:00:52,825
Set group to orig_pts.


00:00:53,600 --> 00:00:56,080
This is where we will handle boundary points.


00:00:57,280 --> 00:01:00,884
First off, check if the current
point is a boundary point.


00:01:01,920 --> 00:01:05,206
Define a new integer array
for the open edge points.


00:01:06,000 --> 00:01:09,760
If the current point is a boundary
point, for every edge point that


00:01:09,760 --> 00:01:14,400
belongs to the current point, we define
an edge between each pair of points,


00:01:14,400 --> 00:01:18,862
and check if these edges are inside
the orig_prim_border_edges group.


00:01:19,760 --> 00:01:22,880
Only if they are inside the
orig_prim_border_edges group,


00:01:22,880 --> 00:01:26,347
we append this edge point
to the openedgepts array.


00:01:27,280 --> 00:01:30,112
Finally replace the edge_neighbours attribute


00:01:30,112 --> 00:01:32,439
with the new open_edge_points array.


00:01:34,480 --> 00:01:37,025
Let's first check the 
edge_neighbours before the


00:01:37,025 --> 00:01:39,524
handle_boundary_points 
attribute wrangle node.


00:01:44,480 --> 00:01:50,712
As you can see, for point 6, we have
18, 23, and 24 as the edge neighbours.


00:01:51,840 --> 00:01:55,280
After we run the code inside
the handle_boundary_points


00:01:55,280 --> 00:01:59,360
attribute wrangle node, 23 is removed
from the edge neighbours array.


00:02:00,550 --> 00:02:04,310
So we gathered the correct edge neighbours
for the original boundary points.


00:02:05,360 --> 00:02:07,136
But that alone is not enough.


00:02:07,920 --> 00:02:09,920
If you recall from the concept drawing,


00:02:09,920 --> 00:02:13,280
we also need to use different weight
values for the boundary points.


00:02:14,320 --> 00:02:15,030
So select


00:02:15,030 --> 00:02:17,275
compute_catmull_clark_weights_for_original_points 


00:02:17,275 --> 00:02:20,634
wrangle node, and overwrite 
the default weightvalues,


00:02:20,634 --> 00:02:23,009
if the current point is a boundary point.


00:02:25,360 --> 00:02:28,404
If so, control weight becomes 0.5.


00:02:28,404 --> 00:02:33,409
Edge weightbecomes 0.25, 
and face weight becomes 0.


00:02:35,200 --> 00:02:37,022
As soon as we commit this change,


00:02:37,360 --> 00:02:41,799
you can see that all the points
perfectly match OpenSubdiv subdivision.


00:02:42,320 --> 00:02:43,514
Isn't this perfect?


00:02:44,400 --> 00:02:46,404
Let's try it with higher iterations. 


00:03:06,640 --> 00:03:11,651
You can see, we are still perfectly
matching OpenSubdiv Catmull-Clark subdivision.


00:03:14,800 --> 00:03:18,480
Now we are gonna do something, even
better than the default subdivide SOP.


00:03:19,360 --> 00:03:23,840
Instead of deleting a polygon, we will
just exclude it from the input group.


00:03:31,360 --> 00:03:35,200
As you can see, the excluded
polygon in our subdivide node


00:03:35,200 --> 00:03:39,520
perfectly conforms to the boundary of
the Catmull-Clark subdivided geometry.


00:03:39,520 --> 00:03:43,360
Whereas OpenSubdiv just blasts
out the excluded polygon,


00:03:43,360 --> 00:03:46,160
leaving a hole where the
excluded polygon used to be.


00:03:47,295 --> 00:03:51,340
We have no open edges,
but OpenSubdiv subdivision has.


00:03:52,320 --> 00:03:56,611
Definitely a massive improvement
over OpenSubdiv Catmull-Clark subdivision.


00:03:57,440 --> 00:04:00,160
We will improve it even more
in the upcoming lessons.


00:04:02,316 --> 00:04:05,315
And with that said, see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

115 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Open Surfaces - Handling Corner Points

----------------------------------------------------------------------------------------------------



00:00:04,198 --> 00:00:07,643
In the previous lesson, we have
handled a very common case. 


00:00:08,382 --> 00:00:12,640
Now we will look at another common case
of corner points on open surfaces.


00:00:13,520 --> 00:00:16,080
So create a grid with the
usual bounding box colors.


00:00:20,480 --> 00:00:22,456
Deform it with a mountain node.


00:00:46,560 --> 00:00:51,040
As you can see, the corner points in
our Subdivide node is interpolated,


00:00:51,040 --> 00:00:53,517
whereas in OpenSubdiv, they are fixed.


00:01:04,467 --> 00:01:06,372
Select the is_boundary wranglenode.


00:01:07,312 --> 00:01:11,154
I will also rename it, as it will deal 
with not only boundarypoints


00:01:11,154 --> 00:01:13,041
but also corner points as well.


00:01:13,840 --> 00:01:17,680
Define an integer variable
called is_closed and set it to 1,


00:01:17,680 --> 00:01:21,280
and another integer variable
called prim_count set to 0.


00:01:22,400 --> 00:01:27,280
What we will do is, to get all the primitives
of a point using the pointprims function,


00:01:27,280 --> 00:01:28,441
and loop over them.


00:01:29,318 --> 00:01:33,745
If any of theseprimitives is open, 
then we consider it open.


00:01:42,207 --> 00:01:44,927
Store the is_closed attribute
on the current point.


00:01:45,946 --> 00:01:48,426
We will also use it in other nodes later on.


00:01:49,503 --> 00:01:52,463
Then loop over the point
primitives again, and this time


00:01:53,114 --> 00:01:55,914
check if each primitive is
inside the orig_prims group.


00:01:56,960 --> 00:02:00,960
We only consider the primitives if
they are in the input prims group.


00:02:00,960 --> 00:02:04,136
Otherwise don't increment
the primitive_count variable.


00:02:06,320 --> 00:02:07,918
If is_closed is true,


00:02:07,918 --> 00:02:09,696
and primcount is lessthan 2,


00:02:09,696 --> 00:02:13,171
then we exclude the current point 
from the orig_pts group.


00:02:14,207 --> 00:02:17,538
This means these points won't
have Catmull-Clark interpolation


00:02:17,612 --> 00:02:18,458
applied to them.


00:02:19,520 --> 00:02:21,440
If you look at the corner points,


00:02:21,440 --> 00:02:25,040
you can see that they only have 1
primitive which is a closed primitive,


00:02:25,040 --> 00:02:26,849
and the primitive count is 1,


00:02:26,849 --> 00:02:30,617
and as such itwould satisfy the 
condition we have just defined.


00:02:31,600 --> 00:02:36,000
As soon as we commit this change, the
corner points snap to where they need to be.


00:02:36,960 --> 00:02:40,990
We have perfectly matched
OpenSubdiv Catmull-Clark subdivision again.


00:02:42,000 --> 00:02:43,840
Let's increase the iterations now.


00:02:58,480 --> 00:03:01,520
Also make sure to exclude these new
attributes we have just created,


00:03:01,520 --> 00:03:04,083
from the Catmull-Clark attribute 
interpolate node.


00:03:16,880 --> 00:03:19,711
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

116 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Handling Non-Manifold Topology

----------------------------------------------------------------------------------------------------



00:00:03,782 --> 00:00:06,494
Continuing from where we left off, this time


00:00:06,494 --> 00:00:08,928
we will use the complex 
geometry we modelled


00:00:08,928 --> 00:00:10,983
in the bilinear subdivision chapters.


00:00:11,842 --> 00:00:15,323
So go inside that object and create 
a null at the end of the chain


00:00:15,323 --> 00:00:16,745
before the subdivide node.


00:00:26,871 --> 00:00:30,237
Bring in this geometry inside 
the Catmull-Clark-subd object


00:00:30,237 --> 00:00:32,439
and connect it to the subdivide nodes.


00:00:42,391 --> 00:00:45,863
As you can see, we get some 
spikes in our subdivide node.


00:00:45,863 --> 00:00:50,400
That seems to happen only in the 
non-manifold areas of the input geometry.


00:00:51,162 --> 00:00:55,651
OpenSubdiv seems to exclude these 
from the Catmull-Clark interpolation


00:00:55,651 --> 00:00:59,875
so they are fixed at the same point 
positions as the input geometry.


00:00:59,875 --> 00:01:02,991
So we need to do the same to 
match OpenSubdiv.


00:01:03,892 --> 00:01:08,306
You can see, our bilinear subdivision 
implementation works as expected.


00:01:08,306 --> 00:01:11,234
It's just the application of the 
Catmull-Clark interpolation


00:01:11,234 --> 00:01:13,828
throws off these non-manifold points.


00:01:14,701 --> 00:01:17,791
I will just create a blendshape 
between the bilinear subdivision


00:01:17,791 --> 00:01:20,971
and Catmull-Clark subdivision 
to see the changes better.


00:01:22,079 --> 00:01:24,805
We can not use the default 
subdivide for blending


00:01:24,805 --> 00:01:28,527
because of the difference in 
point indices between the geometries.


00:01:29,441 --> 00:01:31,877
You can see how the points get transformed


00:01:31,877 --> 00:01:35,765
from bilinear interpolation to 
Catmull-Clark interpolation very clearly.


00:01:36,873 --> 00:01:40,469
We need to figure out a way to 
exclude these non-manifold points.


00:01:41,467 --> 00:01:44,256
Let's first take a look at the 
orig_prim_border_edges.


00:01:56,467 --> 00:01:59,595
Set Geometry Spreadsheet to only 
show the boundary points.


00:02:01,604 --> 00:02:06,310
If you look at point 0, you can see it 
has 4 point neighbours using edges.


00:02:08,139 --> 00:02:11,843
If you look at point 9, you can see 
that it has 0 edge neighbours.


00:02:18,698 --> 00:02:20,625
That's because, as you recall,


00:02:20,625 --> 00:02:24,233
we exclude it from the original 
points group, orig_pts.


00:02:25,341 --> 00:02:29,129
Because it's a corner point, in that, 
all of its polygons


00:02:29,129 --> 00:02:35,144
are closed and its primitive count included 
in the input primitive group, is less than 2.


00:02:36,000 --> 00:02:40,282
If you look at point 5, it also has 
4 point neighbours using edges.


00:02:40,878 --> 00:02:44,454
Point 11 also has 4 point neighbours 
using edges.


00:02:44,454 --> 00:02:46,300
But it's not a boundary point.


00:02:52,388 --> 00:02:56,044
Point 30 also has 4 point neighbours
using edges.


00:02:57,111 --> 00:03:01,901
Point 35 has 0 point neighbours using 
edges. Because it's a corner point.


00:03:04,437 --> 00:03:07,613
Point 25 has 2 point neighbours using edges.


00:03:07,613 --> 00:03:11,149
Because it's a boundary point 
but not a corner point.


00:03:12,147 --> 00:03:16,588
All of this, just to show you that, 
we can isolate these non-manifold points


00:03:16,588 --> 00:03:18,592
using open_edge_points count.


00:03:19,742 --> 00:03:22,448
So inside the 
handle_boundary_points wrangle node,


00:03:22,448 --> 00:03:26,744
add another if statements inside 
the if statement inside the for loop.


00:03:27,631 --> 00:03:30,116
If the open_edge_points count is 2,


00:03:30,116 --> 00:03:32,942
and we are about to add another 
border point to this array,


00:03:32,942 --> 00:03:37,597
then exclude this point from the 
orig_pts group, and break the loop.


00:03:38,414 --> 00:03:42,300
Effectively what we are doing is, removing any point


00:03:42,300 --> 00:03:44,313
that has more than 2 open edges,


00:03:44,313 --> 00:03:46,833
that are in the orig_prim_border_edges group,


00:03:46,833 --> 00:03:48,404
from the original points group.


00:03:49,540 --> 00:03:52,625
Once we cook the end node in the chain, 
you can see that


00:03:52,625 --> 00:03:55,506
we have successfully excluded the 
non-manifold points


00:03:55,506 --> 00:03:57,600
from the Catmull-Clark interpolation


00:03:57,600 --> 00:04:02,077
and perfectly matched OpenSubdiv 
Catmull-Clark subdivision once again.


00:04:03,185 --> 00:04:05,609
We are now blending between the bilinear


00:04:05,609 --> 00:04:08,258
and Catmull-Clark subdivision seamlessly.


00:04:17,464 --> 00:04:21,522
Now lets try subdividing sub-geometry,
instead of the entire geometry.


00:04:22,797 --> 00:04:26,213
I am just going to add random 
primitive colors to the geometry first


00:04:27,183 --> 00:04:32,150
and then color group A with a constant 
color to see the subdivision more clearly.


00:04:41,283 --> 00:04:43,607
Use group A for both subdivide nodes.


00:04:49,786 --> 00:04:53,353
As you can see, the default subdivide 
breaks up the geometry


00:04:53,353 --> 00:04:56,421
around the group boundaries, 
leaving holes everywhere.


00:04:57,529 --> 00:05:00,621
Our subdivide node again 
handles this correctly


00:05:00,621 --> 00:05:02,853
and perfectly preserves the boundaries.


00:05:04,058 --> 00:05:06,826
It's much easier to see this in 
wireframe mode.


00:05:08,087 --> 00:05:11,467
As you can see, our subdivide node, 
gets all the neighbour primitives


00:05:11,467 --> 00:05:15,425
to perfectly conform to the primitives 
in the input primitive group.


00:05:16,714 --> 00:05:21,855
This improvement alone makes our custom
subdivide node worth using in production.


00:05:31,330 --> 00:05:34,384
If we apply the Catmull-Clark subdivision 
to the entire geometry,


00:05:34,384 --> 00:05:37,999
both nodes have identical results 
as we have seen before.


00:05:39,994 --> 00:05:43,184
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

117 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Open Polygons - Computing Weights of Original Points - Reverse Engineering OpenSubdiv

----------------------------------------------------------------------------------------------------


F1
00:00:04,560 --> 00:00:06,663
Sometimes there is not 
enough documentation


00:00:06,663 --> 00:00:09,600
available for a particular 
algorithm or solution,


00:00:10,480 --> 00:00:12,205
you are left with either inspecting


00:00:12,205 --> 00:00:14,324
source code of open 
course implementations


00:00:14,324 --> 00:00:18,417
that you can find online, or if you
have a working version of the product,


00:00:18,417 --> 00:00:21,398
you can try to reverse engineer 
its innerworkings.


00:00:22,160 --> 00:00:25,600
Houdini is a very open and
highly versatile 3d software,


00:00:25,600 --> 00:00:29,840
where you can see the inner workings of
a lot of operators in the form of HDAs.


00:00:30,560 --> 00:00:33,123
But there are still a significant 
amount ofoperators


00:00:33,123 --> 00:00:36,096
written in C++, that come in compiled form.


00:00:36,096 --> 00:00:38,931
It's not as straightforward 
to decipherthe


00:00:38,931 --> 00:00:41,120
inner workings of such black box operators.


00:00:42,000 --> 00:00:45,200
But even then, you can perform
some simple experiments


00:00:45,200 --> 00:00:47,680
to deduce a lot of valuable data from that.


00:00:48,720 --> 00:00:51,840
For this current problem
of handling open polygons,


00:00:51,840 --> 00:00:54,827
initially I couldn't find any 
document or paper


00:00:54,827 --> 00:00:56,720
that described the approach to achieve this.


00:00:57,834 --> 00:01:01,594
We already know how to apply Catmull-Clark
subdivision and interpolation.


00:01:02,590 --> 00:01:04,990
So all we need is the weights
to use for each point.


00:01:06,016 --> 00:01:09,456
Fortunately we have the default
Subdivide SOP we can turn to.


00:01:10,328 --> 00:01:12,699
We can easily figure out 
which points influence


00:01:12,699 --> 00:01:15,688
other points, and by how much, 
very precisely.


00:01:16,568 --> 00:01:18,888
Let's start by creating some open polygons.


00:01:20,000 --> 00:01:23,590
I will just isolate 2 adjacent polygons 
from thegrid geometry


00:01:23,590 --> 00:01:25,651
we have used in the previous lesson.


00:01:25,900 --> 00:01:30,089
Create an Ends SOP and connect it
to the adjacent polygon geometry.


00:01:32,320 --> 00:01:35,440
Set Close-U parameter to
Unroll with Shared Points.


00:01:36,480 --> 00:01:39,465
Connect the Ends node to the subdivide nodes.


00:01:47,600 --> 00:01:50,605
As you can see, what our 
subdivide nodecreates


00:01:50,605 --> 00:01:53,819
is nothing like OpenSubdiv 
Catmull-Clark subdivision.


00:01:53,907 --> 00:01:57,928
So something in our implementation
is missing to handle open polygons.


00:01:58,720 --> 00:02:00,467
We will figure it out soon enough.


00:02:01,200 --> 00:02:03,438
Create an Edit node and insert it between


00:02:03,438 --> 00:02:05,760
the Ends node and the default 
Subdivide node.


00:02:06,888 --> 00:02:10,648
We will reverse engineer the default
Subdivide operator in 3 stages.


00:02:11,600 --> 00:02:15,440
The first stage is to figure out
which point on the subdivision cage


00:02:15,440 --> 00:02:18,400
is affecting which point
on the subdivided geometry.


00:02:19,440 --> 00:02:21,928
We also need to reset y component of P,


00:02:22,816 --> 00:02:26,416
so we can see the amount of influence
each point has more clearly.


00:02:27,390 --> 00:02:30,910
So create an attribute wrangle and
connect it after the Ends node.


00:02:33,360 --> 00:02:36,960
Also make sure to pin the geometry
spreadsheet to the subdivide node


00:02:36,960 --> 00:02:40,160
so we can see the point positions
of the subdivided geometry.


00:02:41,040 --> 00:02:44,821
As you can see, all the Y 
components of P is 0.


00:02:48,400 --> 00:02:50,269
Now take a look at this corner point.


00:02:50,467 --> 00:02:53,718
It ispoint number 2 on the 
subdivided geometry.


00:02:54,480 --> 00:02:58,480
We will figure out which points on the
subdivision cage affects its position


00:02:58,480 --> 00:03:00,000
on the subdivided geometry.


00:03:00,770 --> 00:03:03,361
You can see this point has no effect.


00:03:04,110 --> 00:03:05,630
But this point affects it.


00:03:09,280 --> 00:03:10,720
This point has no effect.


00:03:11,520 --> 00:03:13,120
This point also doesn't affect it.


00:03:15,200 --> 00:03:18,160
This point does affect it also.


00:03:18,160 --> 00:03:20,427
Now we will figure out the 
amount of influence


00:03:20,427 --> 00:03:22,320
these points have on the corner points.


00:03:25,520 --> 00:03:29,760
As you can see, moving this point all
the way on top of the corner point


00:03:29,760 --> 00:03:33,600
on the subdivision cage, moves the
point on the subdivided geometry,


00:03:33,600 --> 00:03:36,960
at the same level as the corner
point on the subdivision cage.


00:03:37,912 --> 00:03:41,454
This tells us the influence
of this point is quite small.


00:03:47,368 --> 00:03:49,688
It's the same for the other neighbour point.


00:03:54,400 --> 00:03:57,202
The influence of the original point's 
is much higher.


00:03:58,560 --> 00:04:01,760
Feels like the point on the
subdivided geometry moves about


00:04:01,760 --> 00:04:04,195
half the amount of the distance 
travelled bythe


00:04:04,195 --> 00:04:06,240
original point on the subdivision cage.


00:04:07,280 --> 00:04:09,920
Now we will try to figure
out the actual weight values.


00:04:11,040 --> 00:04:14,640
As you recall, all Y components
of the point positions are 0,


00:04:15,680 --> 00:04:18,184
so we can clearly see the 
amount of contribution


00:04:18,184 --> 00:04:20,320
each point on the subdivision 
cage will make.


00:04:21,360 --> 00:04:23,901
Set this point's Y position to 1.


00:04:23,901 --> 00:04:25,179
Why 1?


00:04:25,179 --> 00:04:28,011
Becausewhatever weight value 
is multiplied by 1,


00:04:28,011 --> 00:04:29,842
will show us the full weight value.


00:04:30,640 --> 00:04:35,360
As you can see, setting the Y component of
this point that's on the subdivision cage,


00:04:35,360 --> 00:04:40,400
to 1, got the Y component of the original
point on the subdivided geometry to


00:04:40,400 --> 00:04:43,212
0.166667,


00:04:43,212 --> 00:04:46,960
that's affected by the point weare 
transforming on the subdivision cage.


00:04:48,190 --> 00:04:49,550
This value looks familiar.


00:04:50,560 --> 00:04:52,885
Doing the same thing for the 
other neighbour point


00:04:52,885 --> 00:04:56,015
of the same original point, 
doubled up the contribution,


00:04:56,015 --> 00:05:00,560
so that means, they bothcontribute 
equally to the final point position.


00:05:01,680 --> 00:05:04,498
Let's now do the same for the original point,


00:05:04,498 --> 00:05:08,563
but this time instead of 1, we will use -1.


00:05:08,563 --> 00:05:12,640
So that the contribution of this point
will be subtracted from the other 2.


00:05:14,160 --> 00:05:19,044
As you can see, the result went 
from 1/3 to -1/3.


00:05:20,240 --> 00:05:21,953
By simple arithmetic rules,


00:05:21,953 --> 00:05:27,016
we can see thatthe total contribution 
of the previous 2 points is 1/3.


00:05:27,016 --> 00:05:31,268
So each point will
have half that, which is 1/6.


00:05:33,520 --> 00:05:37,384
That leaves the contribution
of the original point to 2/3.


00:05:38,880 --> 00:05:41,040
If you reset the point positions back,


00:05:41,040 --> 00:05:45,180
you can see the contribution of
the original point is indeed 2/3.


00:05:46,320 --> 00:05:51,520
So your intuition might be, let's just
use these weights as is. But recall that


00:05:51,520 --> 00:05:55,280
we are not using the point neighbours
on the subdivision cage, but rather,


00:05:55,280 --> 00:05:57,281
the midpoints on these edges,


00:05:57,281 --> 00:05:58,226
and as such,


00:05:58,226 --> 00:06:00,620
this will change the weight 
values completely.


00:06:01,520 --> 00:06:04,658
As a general rule you can 
make a guess to assume


00:06:04,658 --> 00:06:06,799
the weights of the point 
neighbours will increase,


00:06:06,799 --> 00:06:09,743
as the edge points are
closer to the original point,


00:06:09,743 --> 00:06:12,160
than the point neighbours 
on the subdivision cage.


00:06:13,200 --> 00:06:15,410
Let's break this all down now.


00:06:30,400 --> 00:06:33,680
Let's call the original point
on the subdivision cage A,


00:06:33,680 --> 00:06:36,852
and the neighbour points on
the subdivision cage B and C.


00:06:37,980 --> 00:06:40,940
Call the original point on
the subdivided geometry P.


00:06:41,840 --> 00:06:44,504
The weight of A is 2/3.


00:06:44,960 --> 00:06:48,211
The weight sum of B and C is 1/3.


00:06:48,880 --> 00:06:52,880
So the weight of B and C is 1/6 individually.


00:06:54,560 --> 00:07:02,160
P is 2/3 times A, plus 1/6
times B, plus 1/6 times C.


00:07:02,160 --> 00:07:03,255
is equal to:


00:07:03,255 --> 00:07:07,225
2A/3, plus (B+C)/6


00:07:08,160 --> 00:07:09,078
is equal to:


00:07:09,220 --> 00:07:12,231
4A + B + C
-------------------



00:07:14,400 --> 00:07:16,971
Let's call the edge points X and Y.


00:07:17,680 --> 00:07:20,381
X is (A+B)/2


00:07:20,880 --> 00:07:23,537
Y is (A+C)/2


00:07:24,438 --> 00:07:26,678
B is 2x - A


00:07:27,594 --> 00:07:29,994
and C is 2y - A


00:07:30,880 --> 00:07:34,000
If we substitute these in
the first equation, we get:


00:07:35,040 --> 00:07:43,095
(4A + 2x - A + 2y - A) over 6


00:07:51,360 --> 00:07:58,367
If you solve it all the way, you
can simplify it as 1/3 * (A+X+Y)


00:07:58,720 --> 00:08:02,978
That means the weight of
each of these points is 1/3.


00:08:03,360 --> 00:08:05,637
So control weight is 1/3.


00:08:06,000 --> 00:08:07,402
And edge weight is


00:08:09,660 --> 00:08:19,740
((2/3) /total_open_polygon_count) * open_polygon_count


00:08:20,400 --> 00:08:21,677
What does this mean?


00:08:22,400 --> 00:08:24,930
It means we don't just equally distribute


00:08:24,930 --> 00:08:27,390
the total edge weight among 
all edge points.


00:08:28,240 --> 00:08:32,480
Their influence is directly linked to
how many open polygons do they share


00:08:32,480 --> 00:08:36,560
among the total number of open
polygons shared by each edge point.


00:08:37,600 --> 00:08:40,567
We will see this in the next 
example more clearly.


00:08:41,360 --> 00:08:44,800
In this case, each edge point
only has a single open polygon.


00:08:45,586 --> 00:08:49,522
So total_open_polygon_count, 
TOPC is 2.


00:08:50,160 --> 00:08:54,960
open_polygon_count for B and C 
is 1 for each.


00:08:55,760 --> 00:08:58,800
So in this case their contribution 
is the same.


00:09:00,000 --> 00:09:03,680
Finally face weight is 0,
as there are no face points.


00:09:04,690 --> 00:09:07,466
Let's look at another example in Houdini.


00:09:11,120 --> 00:09:15,200
First undo all the changes we
made so we can start from 0 again.


00:09:16,160 --> 00:09:20,870
We will look at which points on the
subdivision cage influence this point.


00:09:23,360 --> 00:09:25,840
As you can see this point is affecting it.


00:09:26,720 --> 00:09:28,720
This point is also affecting it.


00:09:29,520 --> 00:09:31,120
This point doesn't affect it.


00:09:34,292 --> 00:09:35,972
This point also doesn't affect it.


00:09:37,440 --> 00:09:39,360
This point is also affecting it.


00:09:40,000 --> 00:09:43,559
Now let's see how much they
affect the point in question.


00:09:58,720 --> 00:10:02,240
It looks like the center neighbour
point is affecting our point


00:10:02,240 --> 00:10:04,016
more than the side neighbour points.


00:10:04,800 --> 00:10:07,252
Now let's try to figure out
the actual weight values.


00:10:08,240 --> 00:10:10,160
Using the same method as before,


00:10:10,160 --> 00:10:14,736
just set the Y component of this point
to 1 to see the final contribution.


00:10:15,680 --> 00:10:24,480
As you can see, the final Y value is
just a bit more than 0.08, which is 1/12.


00:10:27,200 --> 00:10:30,320
If we do the same thing for
the other side neighbour point,


00:10:30,320 --> 00:10:34,620
the final Y value doubles up,
so they have the same weight.


00:10:35,600 --> 00:10:40,160
Interestingly once we set the Y 
componentof the center neighbour point to 1,


00:10:40,160 --> 00:10:42,960
the final Y component doubles 
up once again.


00:10:44,000 --> 00:10:46,853
That means the center neighbour 
point has as


00:10:46,853 --> 00:10:49,252
much weight as the sum of 
both side neighbours.


00:10:50,000 --> 00:10:51,608
Or to put it in another way,


00:10:51,608 --> 00:10:55,506
the center point neighbourweight 
is double the side neighbour points.


00:10:56,480 --> 00:11:00,800
Set the Y component of the original
point on the subdivision cage to -1.


00:11:01,840 --> 00:11:04,080
Now we see the same behaviour as before.


00:11:05,040 --> 00:11:07,209
The contribution of the original point


00:11:07,209 --> 00:11:09,820
is doublethe contribution of 
all the neighbour points.


00:11:13,360 --> 00:11:17,756
You can see the weight value of
the center neighbour point is 1/6.


00:11:20,640 --> 00:11:22,595
Let's break all of this down now.


00:11:27,819 --> 00:11:31,327
Let's call the original point
on the subdivision cage A,


00:11:31,327 --> 00:11:35,699
and the neighbour points on 
thesubdivision cage B, C and D.


00:11:38,480 --> 00:11:40,962
The weight of A is 2/3.


00:11:41,600 --> 00:11:45,618
The weight sum of B, 
C and D is 1/3.


00:11:46,400 --> 00:11:50,300
As we have seen in Houdini, 
the weight of B is 1/6


00:11:50,300 --> 00:11:54,606
while the weight of C and D 
is 1/12 individually.


00:11:56,480 --> 00:11:59,440
P is the original point on
the subdivided geometry.


00:12:00,080 --> 00:12:07,909
So P is 2A/3 + B/6 + C/12 + D/12.


00:12:08,614 --> 00:12:09,630
which is equal to:


00:12:09,718 --> 00:12:16,524
8A/12 + 2B/12 + (C+D)/12


00:12:17,053 --> 00:12:18,333
which is equal to:


00:12:18,333 --> 00:12:23,296
(8A + 2B + C + D) over 12


00:12:25,520 --> 00:12:28,869
Let's call the edge points X, Y and Z.


00:12:29,760 --> 00:12:32,329
X is (A+B)/2


00:12:33,280 --> 00:12:35,708
Y is (A+C)/2


00:12:36,080 --> 00:12:38,415
Z is (A+D)/2


00:12:39,280 --> 00:12:41,360
So B is 2x - A


00:12:41,920 --> 00:12:46,949
C is 2y - A
D is 2z - A


00:12:47,710 --> 00:12:50,750
If we substitute these in
the first equation we get:


00:12:51,280 --> 00:13:00,698
(8A + 2 (2X - A) + 2Y - A + 2Z - A) over 12


00:13:25,440 --> 00:13:28,398
Simplifying this equation.
We end up with:


00:13:28,575 --> 00:13:37,399
1/3 * A +1/3 * X + 1/6 * Y + 1/6 * Z


00:13:39,440 --> 00:13:44,068
So the control weight is 1/3


00:13:45,920 --> 00:13:47,660
And edge weight is


00:13:47,660 --> 00:13:55,255
((2/3) /total_open_polygon_count) * open_polygon_count


00:13:55,255 --> 00:13:57,327
Same as before.


00:13:59,280 --> 00:14:02,657
For point C, 
open_polygon_count is 1.


00:14:02,657 --> 00:14:06,000
Forpoint D, 
open_polygon_count is also 1.


00:14:06,640 --> 00:14:10,343
But for point B, 
open_polygon_countis 2.


00:14:10,343 --> 00:14:12,160
Because it has 2 open polygons.


00:14:12,880 --> 00:14:15,580
That makes total_open_polygon_count 4.


00:14:17,760 --> 00:14:20,914
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

118 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Open Polygons - Computing Weights of Original Points - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,320 --> 00:00:08,000
Finally the fun part of the whole
process: the implementation of course.


00:00:08,800 --> 00:00:11,360
You will see it's actually
very straight forward now


00:00:11,360 --> 00:00:14,080
that we have the correct weights
to use for the original points.


00:00:15,040 --> 00:00:18,480
Continuing from where we left
off, we will first modify the code


00:00:18,480 --> 00:00:20,880
inside the compute catmull
clark weights wrangle node.


00:00:21,760 --> 00:00:23,362
Define an integer array


00:00:23,362 --> 00:00:26,270
that will store thenumber of open 
primitives for each edge point.


00:00:27,200 --> 00:00:30,480
Now the default weights belong
to the case of closed polygons,


00:00:30,480 --> 00:00:32,457
and as such add an if statement


00:00:32,512 --> 00:00:35,095
to encapsulatethe default weights 
we have defined here.


00:00:36,000 --> 00:00:40,480
We will now handle the case of open
polygons, so add an else statement first.


00:00:41,440 --> 00:00:42,962
Inside the else statement,


00:00:42,962 --> 00:00:46,226
define the weightswe have computed 
from the previous lesson.


00:00:47,040 --> 00:00:49,930
As you recall, control weight is 1/3.


00:00:51,275 --> 00:00:53,094
Edge weight is 2/3.


00:00:53,866 --> 00:00:57,053
We will multiplythe final value 
for each edge point


00:00:57,141 --> 00:00:59,608
just before adding it to
the weights array later on.


00:01:00,480 --> 00:01:01,901
Face weight is 0.


00:01:06,160 --> 00:01:08,640
Inside the loop, define an integer variable


00:01:08,640 --> 00:01:11,840
to store the open primitive
count for the current point.


00:01:12,400 --> 00:01:16,080
Get all the primitives of the current
point using the point_prims function.


00:01:17,600 --> 00:01:18,960
Loop over these primitives.


00:01:22,480 --> 00:01:25,181
For each primitive, first check if the current


00:01:25,181 --> 00:01:27,280
primitive is inside the orig_prims group.


00:01:28,640 --> 00:01:32,800
If the current primitive is not in the
input primitive group, we don't count it.


00:01:33,760 --> 00:01:38,330
If it's inside the input primitive group,
then we check if it's a closed primitive.


00:01:42,480 --> 00:01:45,760
If it's not closed, only
then we increment it by 1.


00:01:46,640 --> 00:01:48,160
After the loop is finished,


00:01:48,160 --> 00:01:52,369
append the final primitive count to the
primitive count array, edge_prim_count.


00:01:53,120 --> 00:01:55,680
Set edge weight to the previous edge weight


00:01:55,680 --> 00:01:59,572
divided by the sum of all edge
primitives in the edge_prim_count array.


00:02:02,000 --> 00:02:04,111
Now we will do a few more modifications.


00:02:04,993 --> 00:02:08,474
First offadd an index variable to 
the foreach loop here.


00:02:12,640 --> 00:02:16,640
Add an if statement that checks if
the current edge point is closed,


00:02:16,640 --> 00:02:19,307
and run the original append edge_weights


00:02:19,307 --> 00:02:22,240
toweights array line only if 
is_closed is true.


00:02:23,120 --> 00:02:27,200
If not, then we are dealing with open
polygons just like what we see here.


00:02:28,160 --> 00:02:31,680
In this case, we don't just want
to append the edge weight as is,


00:02:31,680 --> 00:02:34,377
but multiply it with the number 
of openprimitives


00:02:34,377 --> 00:02:36,205
that belong to the current edge point,


00:02:36,282 --> 00:02:38,397
which is stored in the 
edge_prim_count array.


00:02:39,200 --> 00:02:42,640
That's why we need the loop
index so we can access the value


00:02:42,640 --> 00:02:45,360
from the edge_prim_count array
for the current edge point.


00:02:46,240 --> 00:02:49,680
As soon as we commit this change,
you can see the original points


00:02:49,680 --> 00:02:52,080
perfectly match OpenSubdiv point positions.


00:02:53,120 --> 00:02:58,080
It's remarkable how we reverse engineered
the point weights used in OpenSubdiv for


00:02:58,080 --> 00:02:59,125
open polygons,


00:02:59,577 --> 00:03:02,833
and deduce the weight values
with complete accuracy,


00:03:02,833 --> 00:03:05,064
and adapted it to our implementation,


00:03:05,064 --> 00:03:07,462
just by simply playing with
the point positions


00:03:07,462 --> 00:03:09,092
and observing the outcome.


00:03:09,520 --> 00:03:11,231
This is a very useful skill


00:03:11,231 --> 00:03:13,273
that can help you
solve some problems


00:03:13,273 --> 00:03:15,008
using black box solutions,


00:03:15,008 --> 00:03:17,437
when there isn't a lot of 
documentationavailable


00:03:17,437 --> 00:03:20,201
or you want to match the 
behaviour of a certain tool


00:03:20,278 --> 00:03:21,757
youdon't have the source code of.


00:03:22,800 --> 00:03:27,120
But as you can see, the edge points
require additional interpolation


00:03:27,120 --> 00:03:28,859
to fully match OpenSubdiv,


00:03:29,906 --> 00:03:32,380
which is what we will do next.


00:03:34,800 --> 00:03:37,707
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

119 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Open Polygons - Computing Weights of New Edge Points - Concept

----------------------------------------------------------------------------------------------------



00:00:04,160 --> 00:00:06,824
We will perform the same 
experiments to figureout


00:00:06,824 --> 00:00:08,800
the point weights for the new edge points.


00:00:09,680 --> 00:00:11,356
So let's take a look at point 1,


00:00:11,356 --> 00:00:14,720
and see whichpoints on the 
subdivision cage affects it.


00:00:18,000 --> 00:00:21,747
As you can see, 
point 2 and 0 affects point 1.


00:00:39,360 --> 00:00:41,982
Point 4 and 6 also affects it,


00:00:41,982 --> 00:00:44,369
but indirectly.Meaning


00:00:44,369 --> 00:00:46,693
it's not that these points affect point 1,


00:00:46,693 --> 00:00:49,529
but rather they affect point 2 and 0


00:00:49,529 --> 00:00:52,276
respectively, which in turn affects point 1.


00:00:53,120 --> 00:00:55,710
Let's see how much they influence point 1.


00:01:03,600 --> 00:01:07,037
You can see that point 2 and
0, affects it quite strongly.


00:01:07,920 --> 00:01:10,368
It feels like they affect it by almost half.


00:01:19,752 --> 00:01:23,032
The influence of point 4 and 6 
feels a lot weaker.


00:01:23,680 --> 00:01:27,188
Now in the wrangle node 
where we set P.y to0,


00:01:27,330 --> 00:01:30,462
we will also change Z for specific points.


00:01:31,200 --> 00:01:34,380
Set P.z of point 0 and 1 to 0.


00:01:44,960 --> 00:01:48,000
Set P.z of point 2 and 3 to 1.


00:01:55,760 --> 00:01:59,920
First let's look at the bilinear
subdivision using our Subdivide node.


00:02:00,880 --> 00:02:03,840
We will keep an eye on point 0, 1 and 6.


00:02:04,880 --> 00:02:07,841
In the default subdivide, 
the point numbersare different.


00:02:08,409 --> 00:02:10,756
They are point 0, 1 and 2.


00:02:11,680 --> 00:02:14,182
So if you look at the Z component 
of thesepoints,


00:02:14,324 --> 00:02:16,142
you can see some familiar values,


00:02:19,840 --> 00:02:23,040
namely 1/6 for the original corner points


00:02:30,240 --> 00:02:32,910
and 1/24 for the new edge point.


00:02:42,640 --> 00:02:44,548
Let's break all of this down.


00:03:00,160 --> 00:03:01,885
Call the new edge point E.


00:03:03,615 --> 00:03:07,367
Call the original points on the 
subdividedgeometry A and B.


00:03:09,845 --> 00:03:13,765
As you can see the weight of 
A and B is1/6 each.


00:03:14,385 --> 00:03:16,663
So we can just call it weight_1.


00:03:18,122 --> 00:03:23,304
So 1/6 * weight_1 + 1/6 * weight_1 +


00:03:28,122 --> 00:03:31,304
The weight of E is 0.


00:03:32,073 --> 00:03:33,541
So let's call it weight_2.


00:03:41,066 --> 00:03:46,310
The P.Z value of E on the
subdivided geometry is 1/24.


00:03:48,880 --> 00:03:52,258
So the whole equation is equal to 1/24.


00:04:04,800 --> 00:04:06,000
Solving this equation...


00:04:06,000 --> 00:04:09,000
we get 8 weight_1 is equal to 1, 


00:04:09,786 --> 00:04:12,647
so: weight_1 is equal to 1/8.


00:04:13,840 --> 00:04:17,040
That means A and B have a 
weight of 1/8 each.


00:04:17,760 --> 00:04:20,934
And as such, the weight of 
E becomes 6/8.


00:04:27,680 --> 00:04:30,960
Later on I found a document
online that re-confirmed the


00:04:30,960 --> 00:04:33,760
weights I deduced from 
OpenSubdiv to be accurate.


00:04:34,640 --> 00:04:38,320
This is an image I put together
from the document I found online


00:04:38,320 --> 00:04:41,840
that was describing the
subdivision process for curves.


00:04:42,640 --> 00:04:46,240
As you can see, the Catmull-Clark
weights given in the last part


00:04:46,240 --> 00:04:53,200
is also 1/8 for the neigbour points, and
6/8 for the center, i.e. the new edge point.


00:04:54,720 --> 00:04:57,661
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

120 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Open Polygons - Computing Weights of New Edge Points - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,400 --> 00:00:08,560
First off, we need the check for the new
edge points, if they are closed or not.


00:00:14,400 --> 00:00:16,879
So create a new wrangle node and connect it


00:00:16,879 --> 00:00:19,440
after the is_boundary_or_corner 
wrangle node.


00:00:20,400 --> 00:00:22,781
Copy the same code from this wrangle node,


00:00:22,781 --> 00:00:24,560
and paste it into the new wrangle node.


00:00:25,520 --> 00:00:27,564
Set the group to edge_pts group.


00:00:28,381 --> 00:00:30,096
You might thinkof adding the group


00:00:30,096 --> 00:00:32,764
to the is_boundary_or_corner 
wrangle node instead,


00:00:32,764 --> 00:00:36,595
but it has a lot more codethat we don't 
want to run for the new edge points.


00:00:37,624 --> 00:00:39,704
Now we will compute the new 
edge point weights.


00:00:40,880 --> 00:00:42,575
So create another wrangle node


00:00:42,653 --> 00:00:45,678
right after thecompute_catmull_clark_weights 
wrangle node.


00:00:46,640 --> 00:00:48,560
Set the group to edge_pts group.


00:00:50,240 --> 00:00:54,400
All we have to do is: if the
current edge point is not closed,


00:00:54,400 --> 00:00:58,160
then we gather the current edge point
with the value of control weight,


00:00:58,160 --> 00:01:01,440
and gather its neighbour points
with the value of the edge weight.


00:01:02,800 --> 00:01:06,320
So first check if the current
edge point is not closed.


00:01:06,411 --> 00:01:09,451
And if so, store the
edge_neighbours array in a variable.


00:01:10,625 --> 00:01:13,585
Now define the weights we figured
out in the previous lesson.


00:01:14,625 --> 00:01:20,190
The control weight is 6/8,
and the edge weight is 1/8.


00:01:21,680 --> 00:01:25,840
Create an integer array where the first
element is the current edge point.


00:01:26,480 --> 00:01:29,920
Create a float array where the
first element is the control weight.


00:01:33,120 --> 00:01:36,551
Now for each edge neighbour,
append them to the indices array,


00:01:40,117 --> 00:01:42,437
and append the edge weight 
to the weights array.


00:01:45,920 --> 00:01:48,553
Store these arrays on the current edge point.


00:01:54,000 --> 00:01:56,799
And create a new group 
called curve_edge_pts,


00:01:56,799 --> 00:01:58,960
and add the current edge point to this group.


00:02:07,520 --> 00:02:10,000
Duplicate the catmull_clark 
Attribute Interpolate


00:02:10,000 --> 00:02:12,598
node and connect it after the first one.


00:02:13,402 --> 00:02:15,708
Set the group to curve_edge_pts.


00:02:23,624 --> 00:02:26,984
And update the spare input to the
node that comes before this node.


00:02:30,800 --> 00:02:33,165
Our group only has the open edge points.


00:02:36,160 --> 00:02:38,640
Once we cook the second
attribute interpolate node,


00:02:38,640 --> 00:02:40,553
you can see the new edge points moved to


00:02:40,553 --> 00:02:43,021
where they are on the OpenSubdiv geometry.


00:02:57,200 --> 00:03:01,662
We have now perfectly matched
OpenSubdiv once again.


00:03:14,022 --> 00:03:18,000
But watch what happens when we increase
the depth of subdivision in OpenSubdiv.


00:03:32,080 --> 00:03:36,240
For the first iteration, every point is
changing positions. But after the first


00:03:36,240 --> 00:03:41,542
iteration, see how the points of the previous
iteration are fixed. They don't move at all.


00:03:42,480 --> 00:03:45,484
I am not sure if this is expected 
behaviour but tome


00:03:45,484 --> 00:03:47,477
this seems like it's not the right behaviour.


00:04:00,353 --> 00:04:03,315
If you apply the same 
OpenSubdiv subdivision2 times,


00:04:03,315 --> 00:04:05,796
1 iteration each, one after another,


00:04:05,796 --> 00:04:07,687
you will see that we perfectly match it


00:04:07,687 --> 00:04:10,623
using thesame number of iterations 
in our subdivide node.


00:04:11,520 --> 00:04:14,960
So the result of 2 subdivision
nodes at 1 iteration each,


00:04:14,960 --> 00:04:18,080
should be the same as 1
subdivision node at 2 iterations.


00:04:19,360 --> 00:04:23,023
As you know, that's already the
case with closed polygonal surfaces.


00:04:23,101 --> 00:04:25,476
Even when using OpenSubdiv Catmull-Clark.


00:04:26,640 --> 00:04:30,320
Because we don't have any special
logic wrt iterations in our code,


00:04:30,320 --> 00:04:33,360
our subdivide node perfectly
satisfies this condition.


00:04:34,480 --> 00:04:39,156
You can see the same thing happens at
higher subdivision levels at 3 and so on.


00:04:58,061 --> 00:05:00,861
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

121 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Handling Open Polygonal Curves

----------------------------------------------------------------------------------------------------



00:00:03,440 --> 00:00:08,800
This time we will look at open polygonal
curves. So create a simple polygonal curve,


00:00:08,800 --> 00:00:11,070
and connect it to both subdivide nodes.


00:00:37,920 --> 00:00:41,920
As you can see, the result is almost
the same, except the end points,


00:00:41,920 --> 00:00:43,840
which are fixed in OpenSubdiv.


00:00:44,489 --> 00:00:47,369
So we need to match the same
behaviour in our implementation.


00:00:48,400 --> 00:00:52,320
Select is_boundary_or_corner wrangle
node and add a new condition there.


00:00:53,520 --> 00:00:58,132
We will check if the current point is not
closed and if it has less than 2 neighbours.


00:01:01,420 --> 00:01:05,434
And if so, we exclude it from the
original points group, orig_pts.


00:01:07,040 --> 00:01:08,960
As soon as we commit this change,


00:01:08,960 --> 00:01:11,240
you can see the end points move


00:01:11,303 --> 00:01:14,383
to where theyshould be, 
like just in OpenSubdiv geometry.


00:01:15,360 --> 00:01:18,743
The same situation with
iterations is present here too.


00:01:43,760 --> 00:01:48,609
I will just play with the point positions on
the curve to compare both subdivide nodes.


00:01:59,680 --> 00:02:03,460
Now let's try if there is more to
the difference between OpenSubdiv


00:02:03,460 --> 00:02:07,229
and our subdivide node with regards to 
open polygonal curves.


00:02:08,080 --> 00:02:11,298
Does one offer advantage over 
the other in someway.


00:02:12,225 --> 00:02:14,780
In my testing I find it to be the case.


00:02:16,720 --> 00:02:20,633
So first off create a polygon 
circle with4 divisions


00:02:24,815 --> 00:02:27,500
and rotated in Z by 45 degrees.


00:02:28,552 --> 00:02:33,112
Add an Ends SOP to this circle, and set
Close-U to Unroll with Shared Points.


00:02:35,862 --> 00:02:38,502
Connect this geometry to both 
subdivide nodes.


00:02:43,289 --> 00:02:45,529
Set subdivision levels of both to 10.


00:02:46,543 --> 00:02:50,783
What we want to do is to compare each
subdivided circle to a real circle


00:02:51,760 --> 00:02:55,080
and see how much they differ
from an actual circle.


00:03:15,520 --> 00:03:19,539
I will scale down the circle to match
the radius of each subdivided one.


00:03:37,600 --> 00:03:41,650
Green is OpenSubdiv geometry,
and pink is the actual circle.


00:03:48,800 --> 00:03:50,670
Our subdivide node is yellow.


00:03:51,360 --> 00:03:53,932
I also scale down the OpenSubdiv circle


00:03:53,932 --> 00:03:56,641
soall of these circles have similar radii.


00:04:18,320 --> 00:04:21,650
As you can see our subdivided 
yellow polygoncurve


00:04:21,650 --> 00:04:23,680
is closer to the actual pink circle.


00:04:24,889 --> 00:04:27,376
OpenSubdiv polygon curve deviates more


00:04:27,376 --> 00:04:29,369
from the actual circle, than our circle.


00:04:31,794 --> 00:04:34,354
The difference is more in the 
corners as expected.


00:04:35,200 --> 00:04:39,360
That's why our solution of applying the
same operation without fixing points


00:04:39,360 --> 00:04:42,320
based on iterations produces 
smoother curves.


00:04:45,200 --> 00:04:47,834
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

122 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Handling Mixed Topology - Full Geometry

----------------------------------------------------------------------------------------------------



00:00:04,943 --> 00:00:08,185
Now let's take a look at mixed 
geometry where some edges are shared


00:00:08,185 --> 00:00:10,220
by both closed and open polygons.


00:00:11,026 --> 00:00:14,162
So first create a grid with 9 by 9 divisions.


00:00:17,090 --> 00:00:20,981
Create a primitive group that only 
contains one side of the geometry.


00:00:24,172 --> 00:00:25,863
Create another primitive group


00:00:25,863 --> 00:00:28,367
that contains a shrinked version 
of the previous group.


00:00:32,201 --> 00:00:34,891
Create an Ends SOP and connect 
it to this geometry


00:00:34,891 --> 00:00:37,169
and use the first group we have created.


00:00:38,284 --> 00:00:41,077
Set Close-U to Unroll with Shared Points.


00:00:41,898 --> 00:00:45,494
As you can see, we have both 
open and closed polygons


00:00:45,494 --> 00:00:48,487
where some edges share one of 
each type of polygon.


00:00:50,749 --> 00:00:53,162
Add a Mountain SOP before the Ends node.


00:00:58,568 --> 00:01:01,270
Connect this geometry to both 
subdivide nodes.


00:01:02,215 --> 00:01:04,917
As you can see OpenSubdiv subdivision


00:01:04,917 --> 00:01:07,926
completely ignores and deletes all 
the open polygons.


00:01:07,926 --> 00:01:09,351
Most definitely a bug.


00:01:10,141 --> 00:01:14,390
All OpenSubdiv algorithms seem to 
delete the open polygons in this case.


00:01:15,165 --> 00:01:18,220
If you switch to one of the
Houdini subdivision algorithm,


00:01:18,220 --> 00:01:21,201
then the open polygons are 
no longer deleted.


00:01:21,201 --> 00:01:23,755
Except the Houdini subdivision algorithms


00:01:23,755 --> 00:01:26,479
seem to produce some artifacts 
as you can see.


00:01:30,088 --> 00:01:32,845
Our subdivide node also 
seem to get confused


00:01:32,845 --> 00:01:37,886
around the parts that  share both an 
open and a closed polygon.So let's fix this.


00:01:38,661 --> 00:01:42,991
First off, change the templated 
subdivide node to use Houdini Catmull-Clark


00:01:42,991 --> 00:01:44,951
so we can see if we are close.


00:01:47,786 --> 00:01:51,090
Add a statement inside the 
handle_boundary_points wrangle node


00:01:51,090 --> 00:01:53,771
that checks if the current point is closed.


00:01:58,916 --> 00:02:02,835
As soon as we commit this change, 
the original points at the center fold


00:02:02,835 --> 00:02:05,086
are snapped back to where they should be.


00:02:09,935 --> 00:02:13,862
Now only the new edge points at the 
center fold seem to move incorrectly.


00:02:19,206 --> 00:02:23,289
Let's look at the new edge 
points: 139 and 156.


00:02:28,974 --> 00:02:31,263
I will first clean up some of the attributes.


00:02:54,362 --> 00:02:56,193
Focus on 139 first.


00:02:56,193 --> 00:03:00,562
As you can see, it has 2 neighbour 
points: 13 and 22.


00:03:01,275 --> 00:03:03,303
But it's still moving to a position


00:03:03,303 --> 00:03:06,388
that doesn't look like the mid-point 
of these 2 neighbour points.


00:03:06,558 --> 00:03:07,735
Why is that?


00:03:08,355 --> 00:03:11,917
If you disable the second 
Catmull-Clark attribute interpolate node,


00:03:11,917 --> 00:03:15,320
you can see that they immediately 
move to where they should be.


00:03:17,520 --> 00:03:19,172
As you can see for point 139,


00:03:19,172 --> 00:03:22,371
the points to interpolate from are 
actually more than 2.


00:03:22,371 --> 00:03:26,781
They are 139, 13, 88, 22.


00:03:26,781 --> 00:03:30,057
The extra point 88 is throwing things off.


00:03:30,847 --> 00:03:35,408
Not by its weights value but by shifting 
the weights value of the new edge point.


00:03:36,000 --> 00:03:37,915
We need to exclude that point.


00:03:38,659 --> 00:03:41,303
So create a new attribute wrangle 
right after the


00:03:41,303 --> 00:03:43,394
handle_boundary_points wrangle node.


00:03:47,623 --> 00:03:50,639
Set group to orig_pts and edge_pts.


00:03:51,414 --> 00:03:55,357
What we will do is: if the current 
point is not closed then:


00:03:55,357 --> 00:03:58,583
We create a new integer array to 
store new edge_points.


00:03:59,698 --> 00:04:01,996
Store a reference to edge_neighbours.


00:04:02,662 --> 00:04:04,548
So loop over the edge_neighbours.


00:04:08,684 --> 00:04:11,112
Get all the primitives of the 
current edge neighbour.


00:04:17,758 --> 00:04:21,354
If any of these primitives is not closed, 
then break the loop


00:04:21,354 --> 00:04:25,219
and append the current edge neighbour 
to open_edge_points array.


00:04:39,816 --> 00:04:43,350
Then replace the edge_neighbours 
array attribute with this array.


00:04:45,674 --> 00:04:47,467
As soon as we commit this change,


00:04:47,467 --> 00:04:50,520
you can see the edge points move 
to where they should be.


00:04:51,217 --> 00:04:53,898
It doesn't mirror Houdini 
Catmull-Clark completely


00:04:53,898 --> 00:04:56,927
because we consider open 
polygons independently.


00:04:57,144 --> 00:05:01,874
And as such we achieve a complete 
mirrored result for these open polygons,


00:05:01,874 --> 00:05:03,309
which we will see soon.


00:05:05,261 --> 00:05:08,405
As you can see, before we had 
3 neighbour points


00:05:08,405 --> 00:05:10,145
for each of these new edge points,


00:05:24,236 --> 00:05:26,656
but after the new VEX code we just wrote,


00:05:26,656 --> 00:05:30,515
we only have 2 neighbour points 
that are on the open polygons.


00:05:34,822 --> 00:05:37,859
If you look at other new edge points like 124,


00:05:40,539 --> 00:05:43,650
it only has 2 edge neighbour 
points as expected.


00:05:45,385 --> 00:05:47,824
Now let's compare both subdivide nodes.


00:05:48,815 --> 00:05:53,389
You can see both sides of the open polygons
mirror each other in our subdivide node.


00:05:54,380 --> 00:05:58,114
We also don't have the artifact that's 
seen in the default subdivide node.


00:05:58,842 --> 00:06:01,097
If you isolate open polygons alone,


00:06:16,169 --> 00:06:19,767
you can see they look exactly the 
same as their mirrored version.


00:06:30,681 --> 00:06:32,920
I think it's nice to have this consistency


00:06:32,920 --> 00:06:35,774
where open polygons are 
subdivided independently.


00:06:46,976 --> 00:06:48,796
As we increase the number of iterations,


00:06:48,796 --> 00:06:52,244
you can see both the closed polygons 
and the open polygons


00:06:52,244 --> 00:06:54,290
become smoother and smoother.


00:06:59,315 --> 00:07:03,526
The open polygons particularly look 
really cool at higher subdivision levels.


00:07:05,122 --> 00:07:08,034
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

123 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Handling Mixed Topology - Sub-Geometry

----------------------------------------------------------------------------------------------------



00:00:04,240 --> 00:00:06,000
I will first clean up the scene a bit.


00:00:06,800 --> 00:00:10,409
For this lesson we only need the
geometry from the previous lesson.


00:00:34,800 --> 00:00:37,782
First change the second primitive 
group to include


00:00:37,782 --> 00:00:40,240
all primitives except the outmost primitives.


00:00:41,360 --> 00:00:44,960
This time we will use this group, which
will complicate things a bit more.


00:00:46,000 --> 00:00:49,440
Unfortunately we can not use the
default subdivide with this group.


00:00:50,560 --> 00:00:54,800
It was immediately crashing Houdini
every single time, bug pending.


00:00:55,920 --> 00:00:58,580
So what I will do is to isolate this group


00:00:58,580 --> 00:01:01,710
andapply our subdivision to this 
to use as reference.


00:01:02,720 --> 00:01:05,418
This will be what we match in 
our implementation.


00:01:06,720 --> 00:01:11,374
As you can see, our geometry doesn't
perfectly match the isolated geometry.


00:01:16,000 --> 00:01:21,600
Take a look at point 107. It has
2 edge neighbours: 14 and 15.


00:01:22,744 --> 00:01:26,424
Both of these points are in positions
that's different than where they should be.


00:01:27,366 --> 00:01:28,966
Let's focus on point 14.


00:01:34,104 --> 00:01:36,104
You can see it has a lot of edge neighbours.


00:01:37,258 --> 00:01:41,578
I will just highlight these points using
a Group Create node for easier visibility.


00:01:49,360 --> 00:01:50,529
Something seems off,


00:01:51,289 --> 00:01:54,653
as we are gettingneighbours of 
neighbours for point 14.


00:01:54,653 --> 00:01:55,641
Or are we?


00:01:56,417 --> 00:01:58,803
On a closer look you can see,


00:01:58,803 --> 00:02:01,736
that theseextra points are not 
neighbours of neighbours


00:02:01,840 --> 00:02:04,747
but actual edge neighbours of point 14.


00:02:22,080 --> 00:02:25,760
Ok so what's happening is, because
we have split the input primitives,


00:02:25,760 --> 00:02:29,280
the adjacent primitives have not
yet been rebuilt at this stage.


00:02:29,920 --> 00:02:32,350
So we will rebuild these primitives before.


00:02:35,840 --> 00:02:39,840
First off move the 3 nodes related
to rebuilding neighbour primitives


00:02:39,840 --> 00:02:42,400
right after the bilinear
attribute interpolate node.


00:02:44,080 --> 00:02:46,240
But don't connect the Catmull-Clark 
branch to it.


00:02:47,280 --> 00:02:51,360
We want to execute these nodes
only if the algorithm is bilinear.


00:02:51,360 --> 00:02:55,633
Because we will use a duplicate of these
3 nodes in the above Catmull-Clark branch.


00:03:14,504 --> 00:03:16,245
So copy and paste these 3 nodes


00:03:16,245 --> 00:03:19,064
to run rightafter the
is_closed_edge_points wrangle node.


00:03:20,160 --> 00:03:22,027
Connect edge_neighbours_full_geometry


00:03:22,027 --> 00:03:24,320
wranglenode to the first input 
of the switch node.


00:03:25,520 --> 00:03:29,280
We will create another version of this
wrangle node for the sub-geometry,


00:03:29,280 --> 00:03:30,769
which is the second input.


00:03:32,560 --> 00:03:37,040
But for now let's do a quick test to see
if this change will get us what we want.


00:03:38,160 --> 00:03:40,652
So connect the duplicate 
create_neighbour_prims


00:03:40,652 --> 00:03:42,708
wrangle node to the first input of the


00:03:42,708 --> 00:03:44,744
edge_neighbours_full_geometry 
wrangle node.


00:03:45,680 --> 00:03:48,869
As you can see, point 14, 
no longer has those


00:03:48,869 --> 00:03:50,469
extra points as edge neighbours.


00:03:52,663 --> 00:03:55,850
Because at this point, the 
neighbour primitives have been rebuilt,


00:03:55,850 --> 00:03:58,613
and as such we get the correct 
edge neighbours.


00:04:00,761 --> 00:04:02,197
Revert back this connection


00:04:02,197 --> 00:04:02,983
and duplicate the


00:04:02,983 --> 00:04:05,081
edge_neighbours_full_geometry 
wrangle node.


00:04:06,000 --> 00:04:08,536
This time we will handle sub-geometry.


00:04:18,320 --> 00:04:20,571
If you look at original primitive points,


00:04:20,571 --> 00:04:23,399
you can see they are not including 
the other points


00:04:23,399 --> 00:04:25,129
that are still included


00:04:25,129 --> 00:04:27,840
as edgeneighbours currently 
for sub-geometry.


00:04:31,440 --> 00:04:34,337
So create a group promote node, 
right after theduplicate


00:04:34,337 --> 00:04:36,242
create_neighbour_prims wrangle node,


00:04:36,242 --> 00:04:40,955
to convert orig_prims group to a
point group called orig_prim_pts.


00:04:53,120 --> 00:04:57,360
If you look at point 14, you
can see it does include point 5,


00:04:57,360 --> 00:05:00,240
but it shouldn't as it's
outside the input primitives.


00:05:07,360 --> 00:05:11,200
So instead of storing edge neighbours
directly, we will loop over them,


00:05:11,200 --> 00:05:16,031
and check if the current edge neighbour
is inside the orig_prim_pts group.


00:05:28,480 --> 00:05:31,440
And if so, then we append it to 
the edge_pts array.


00:05:33,840 --> 00:05:37,840
Finally save this array as
edge_neighbours on the current point.


00:05:38,480 --> 00:05:40,993
After we commit this change, point 14,


00:05:40,993 --> 00:05:44,359
nolonger has point 5 as 
one of its neighbours.


00:06:03,040 --> 00:06:06,400
As you can see, we now perfectly
match the isolated geometry.


00:06:07,360 --> 00:06:12,929
I will organize the nodes a bit. I do it
live so you can see what we end up with.


00:06:34,800 --> 00:06:37,760
Now I will play with higher
iterations to demonstrate


00:06:37,760 --> 00:06:40,186
that it perfectly subdivides sub-geometry.


00:07:02,240 --> 00:07:04,170
Finally let's try another thing.


00:07:05,051 --> 00:07:08,704
Create a singlepoint and connect 
this to the subdivide nodes.


00:07:17,600 --> 00:07:21,213
As you can see, we get a warning
about invalid source_prims group.


00:07:22,320 --> 00:07:25,120
There is also an error about
invalid orig_prims group.


00:07:26,160 --> 00:07:28,240
We just have to initialize orig_prims,


00:07:28,240 --> 00:07:31,920
as right now it's only initialized
if source_prims group exists.


00:07:34,800 --> 00:07:38,640
So just create a detail wrangle right
after the inner for loop begin node.


00:07:39,680 --> 00:07:44,560
In this case we have a null called
GEO, so connect it to this node.


00:07:50,560 --> 00:07:54,000
Now, initialize the orig_prims group
using the setprimgroup function.


00:07:55,200 --> 00:07:57,512
This will create an empty primitive group.


00:08:09,280 --> 00:08:14,000
As we inspect more nodes, we see another
error about the invalid edge_pts group.


00:08:15,040 --> 00:08:18,400
This group is created in
create_new_edge_points wrangle node.


00:08:19,440 --> 00:08:22,880
So we just have to check if this
group has any points at this node.


00:08:24,000 --> 00:08:27,360
So select the final switch node
and add another spare input.


00:08:30,160 --> 00:08:32,720
Reference create_new_edge_points
wrangle node here.


00:08:33,920 --> 00:08:36,357
Update the switch expression to check if


00:08:36,357 --> 00:08:38,703
the number of points in edge_pts group,


00:08:38,703 --> 00:08:41,246
on the second spare input, 
has any points.


00:08:43,365 --> 00:08:46,413
After we commit this change,
there are no more errors.


00:08:48,640 --> 00:08:51,773
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

124 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Testing On Complex Geometry

----------------------------------------------------------------------------------------------------



00:00:04,156 --> 00:00:06,205
It's time to try our subdivide node


00:00:06,205 --> 00:00:09,319
on a complex model to check if 
we see any issues.


00:00:10,353 --> 00:00:14,397
I will use this geometry but feel free to 
bring in any geometry that you like.


00:00:23,478 --> 00:00:26,433
As you can see, we perfectly 
match OpenSubdiv.


00:00:27,718 --> 00:00:29,758
It's remarkable what one can do


00:00:29,758 --> 00:00:32,372
given enough knowledge and 
dedication with VEX.


00:00:33,121 --> 00:00:36,065
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

125 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Performance - Profiling

----------------------------------------------------------------------------------------------------



00:00:04,082 --> 00:00:06,367
Let's do a quick performance profiling


00:00:06,453 --> 00:00:09,608
to see if there are glaring 
bottlenecks in performance.


00:00:10,270 --> 00:00:11,962
Everything seems to look normal


00:00:11,962 --> 00:00:14,159
except the Group from Attribute 
Boundary node.


00:00:15,000 --> 00:00:17,764
It looks like it's taking a significant 
amount of time


00:00:17,764 --> 00:00:19,123
from the overall cook time.


00:00:19,895 --> 00:00:22,949
As this is an HDA, inside it's 
the Group Promote node


00:00:22,949 --> 00:00:24,605
that's tanking the performance.


00:00:25,279 --> 00:00:28,480
We need to replace this operation 
with a custom VEX solution.


00:00:29,179 --> 00:00:31,359
And with that said, 
let's dive in.






----------------------------------------------------------------------------------------------------

126 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Performance - Grouping Boundary Edges from Primitive Group - Concept

----------------------------------------------------------------------------------------------------



00:00:04,237 --> 00:00:08,272
We will use the non-manifold geometry 
to implement our custom solution.


00:00:08,902 --> 00:00:12,402
So let's first replicate the node 
version of it using the same nodes


00:00:12,402 --> 00:00:14,299
inside our custom subdivide node.


00:00:15,163 --> 00:00:19,833
We will use the group A, so create a 
primitive wrangle that sets an integer attribute


00:00:19,833 --> 00:00:21,736
for each primitive in this group.


00:00:22,725 --> 00:00:25,643
Connect a Group From Attribute 
Boundary node to this node,


00:00:26,167 --> 00:00:28,332
and use the attribute we have just created.


00:00:29,151 --> 00:00:30,414
Set group to A also.


00:00:31,728 --> 00:00:34,499
As you can see we have the 
border edges highlighted.


00:00:36,426 --> 00:00:38,341
There are 168 of them.


00:00:40,386 --> 00:00:42,840
We will use a few new half-edge functions.


00:00:43,711 --> 00:00:47,021
The first one is point_hedge function 
that will give us a half-edge,


00:00:47,021 --> 00:00:49,128
given a source and destination point.


00:00:50,043 --> 00:00:53,209
From this half-edge, we pass 
this to hedge_prim function


00:00:53,209 --> 00:00:57,154
that will give us the primitive that 
contains this particular half-edge.


00:00:57,929 --> 00:01:01,652
Then we find the next half-edge that's 
equivalent to the current half-edge


00:01:01,755 --> 00:01:04,230
using the hedge_nextequiv function.


00:01:05,131 --> 00:01:08,431
Once we have the next half-edge, 
we repeat the same thing,


00:01:08,431 --> 00:01:10,284
until we reach the first half-edge.


00:01:11,170 --> 00:01:15,805
As you recall equivalence is defined 
such that, 2 half-edges are equivalent


00:01:15,805 --> 00:01:18,692
if they are split from the same shared edges.


00:01:19,474 --> 00:01:24,034
This way we can find every primitive 
shared by a particular edge.


00:01:24,905 --> 00:01:26,927
If you look at the highlighted edges,


00:01:26,927 --> 00:01:29,563
you can see that each edge 
is only highlighted


00:01:29,563 --> 00:01:33,928
if the edge has only 1 primitive that's 
inside the input primitive group.


00:01:34,733 --> 00:01:37,800
And that's exactly what we are 
gonna check in the next lesson.






----------------------------------------------------------------------------------------------------

127 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Performance - Grouping Boundary Edges from Primitive Group - Implementation

----------------------------------------------------------------------------------------------------



00:00:04,775 --> 00:00:07,415
Let's start implementing our solution in VEX.


00:00:07,987 --> 00:00:11,094
So create an attribute wrangle 
and set group to A.


00:00:14,381 --> 00:00:18,563
First define a parameter for the group 
name that we will use as the input group,


00:00:19,802 --> 00:00:23,193
and another parameter for the 
new group name that we will create.


00:00:26,146 --> 00:00:29,423
Get the neighbour points of the 
current point, and loop over them.


00:00:34,163 --> 00:00:38,472
Inside the loop, create an integer array 
that will hold all the unique primitives.


00:00:39,437 --> 00:00:41,947
For every neighbour point, 
and the current point,


00:00:41,947 --> 00:00:46,619
first get the half-edge that's defined by 
them, using the point_hedge function.


00:00:47,417 --> 00:00:50,606
Store a copy of this that we will 
update inside a new loop.


00:00:51,356 --> 00:00:53,264
This time we will use a do_loop


00:00:53,264 --> 00:00:56,360
because we want to run the following 
code at least once.


00:00:57,000 --> 00:00:59,582
That is a difference between 
a while_loop and a do_loop.


00:01:00,000 --> 00:01:03,283
Where a do_loop ensures that the 
loop runs at least once,


00:01:03,283 --> 00:01:05,589
regardless of the result of the 
loop statement.


00:01:06,530 --> 00:01:10,076
So define a do_loop where inside 
the loop, get the primitive


00:01:10,076 --> 00:01:11,796
that belongs to the current half-edge.


00:01:12,677 --> 00:01:16,277
If this primitive is a valid primitive, 
and it's not in the primitive


00:01:16,277 --> 00:01:21,165
array we have created above, then 
append this primitive to this array.


00:01:21,868 --> 00:01:25,056
Replace the current half-edge with 
the next equivalent half-edge.


00:01:25,925 --> 00:01:29,416
In the while part of the loop, check 
if the current half-edge


00:01:29,416 --> 00:01:31,430
is equal to the first half-edge.


00:01:32,121 --> 00:01:35,045
We only continue the loop, if they 
are not equal.


00:01:35,676 --> 00:01:39,178
Otherwise that means, we looped through 
all the half-edges


00:01:39,178 --> 00:01:41,236
and came back to the first half-edge.


00:01:42,000 --> 00:01:45,573
Define an integer variable to store the 
total primitive count.


00:01:46,299 --> 00:01:50,802
Now we will check how many primitives 
that are in the array, are both closed,


00:01:50,802 --> 00:01:53,202
and are inside the input primitive group.


00:01:53,845 --> 00:01:57,451
They have to be closed because we 
don't want to consider open polygons


00:01:57,451 --> 00:01:58,705
for border edges.


00:01:59,455 --> 00:02:01,775
And they have to be inside the input 
primitive group,


00:02:01,775 --> 00:02:03,859
as that's how the borders are defined.


00:02:04,705 --> 00:02:09,122
If they satisfy both conditions, then 
we increment the primitive count by 1.


00:02:10,027 --> 00:02:14,133
If the primitive count is 1, then we 
add this edge to the new edge group.


00:02:23,008 --> 00:02:25,875
Link group_name to the input group 
of the wrangle node.


00:02:26,601 --> 00:02:30,973
Define the new edge group name inside 
the Output selection Group parameter,


00:02:30,973 --> 00:02:33,461
and link it to the new_group_name parameter.


00:02:34,271 --> 00:02:37,221
As you can see, we have the 
same edges highlighted.


00:02:37,221 --> 00:02:40,897
And the edge count is the same as 
the Group from Attribute Boundary node.


00:02:41,850 --> 00:02:45,046
Now let's replace the relevant 
nodes from our subdivide network


00:02:45,046 --> 00:02:46,600
with the new wrangle node.


00:03:13,139 --> 00:03:15,540
Because we create the new edge 
group in VEX,


00:03:15,540 --> 00:03:19,526
we also need to initialize it just like 
we initialized the orig_prims group.


00:03:20,372 --> 00:03:23,418
So create an empty edge group using 
the setedgegroup function


00:03:23,418 --> 00:03:27,778
that sets point 0 and 0 to be excluded 
from this group,


00:03:27,778 --> 00:03:29,392
which will create an empty group.


00:03:31,023 --> 00:03:32,519
We now have the edge group.


00:03:39,688 --> 00:03:43,457
As you can see the new solution takes 
about half the cook time of


00:03:43,600 --> 00:03:45,467
group from Attribute Boundary node.


00:03:46,324 --> 00:03:49,332
Keep in mind though the performance 
gains would be a lot more


00:03:49,332 --> 00:03:51,533
if the geometry had a lot more primitives


00:03:51,533 --> 00:03:55,688
and it would scale close to linear by 
the number of CPU cores you have.


00:03:56,510 --> 00:04:01,409
When we turn on compiling, we get 
another 7x speed up on my 8 core laptop.


00:04:02,243 --> 00:04:04,961
That's pretty remarkable for a 
carefully designed


00:04:04,961 --> 00:04:09,098
highly complex network of nodes 
doing various geometric operations.


00:04:10,599 --> 00:04:13,434
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

128 - Custom Subdivision Surfaces - Implementing Catmull-Clark Subdivision - Performance - VEX vs C++

----------------------------------------------------------------------------------------------------



00:00:04,173 --> 00:00:07,803
Now we can compare the performance 
of the final subdivide node


00:00:07,803 --> 00:00:11,460
against OpenSubdiv Catmull-Clark
and Houdini Catmull-Clark.


00:00:12,080 --> 00:00:15,463
They are both written in C++ 
but the algorithmic difference


00:00:15,463 --> 00:00:19,604
between OpenSubdiv Catmull-Clark and 
Houdini Catmull-Clark is significant.


00:00:20,382 --> 00:00:24,017
So we are comparing not only 
VEX vs C++ but also


00:00:24,091 --> 00:00:26,854
algorithmic differences 
among the 3 operators.


00:00:28,084 --> 00:00:32,595
As you can see, VEX Catmull-Clark 
subdivision took 1.7 seconds.


00:00:33,320 --> 00:00:37,032
OpenSubdiv Catmull-Clark
subdivision took 0.3 seconds


00:00:37,032 --> 00:00:40,619
and Houdini Catmull-Clark
subdivision took 0.7 seconds.


00:00:41,281 --> 00:00:44,746
OpenSubdiv Catmull-Clark is the fastest of all,


00:00:45,009 --> 00:00:47,734
that's 5 times faster than the VEX subdivide,


00:00:48,000 --> 00:00:51,228
while Houdini Catmull-Clark
subdivision is a bit more than


00:00:51,228 --> 00:00:53,207
2 times faster than the VEX subdivide.


00:00:54,000 --> 00:00:56,637
Both implementations are heavily 
single-threaded


00:00:56,637 --> 00:01:01,056
while the VEX Catmull-Clark subdivision 
is highly parallel in execution.


00:01:03,421 --> 00:01:07,861
Let's add some vertex attributes to 
complicate the input geometry significantly.


00:01:08,618 --> 00:01:11,906
I will just create 300 random 
vertex attributes.


00:01:58,000 --> 00:02:01,239
Now VEX Catmull-Clark subdivision 
is the fastest of all,


00:02:01,239 --> 00:02:05,073
beating even OpenSubdiv 
Catmull-Clark subdivision by 8 percent,


00:02:05,914 --> 00:02:10,409
14.9s vs 16.1s respectively.


00:02:11,229 --> 00:02:14,050
Houdini Catmull-Clark subdivision 
being the slowest,


00:02:14,050 --> 00:02:17,141
7 times slower than the VEX 
Catmull-Clark subdivision,


00:02:17,141 --> 00:02:21,425
while being 6.6 times slower than 
OpenSubdiv Catmull-Clark subdivision.


00:02:22,508 --> 00:02:26,145
With a large CPU core count such 
as 64 cores,


00:02:26,145 --> 00:02:30,331
it's very likely the performance gains 
of VEX Catmull-Clark subdivision,


00:02:30,331 --> 00:02:31,650
could go through the roof,


00:02:31,860 --> 00:02:35,170
leaving both algorithms of the 
default subdivide node in the dust.


00:02:35,948 --> 00:02:40,195
As you can see, the performance very 
much depends on the input geometry,


00:02:40,195 --> 00:02:43,893
and as such there is no clear 
winner in every single scenario.


00:02:44,145 --> 00:02:48,009
Though it's fair to assume 
OpenSubdiv Catmull-Clark subdivision


00:02:48,009 --> 00:02:51,064
is always faster than the Houdini 
Catmull-Clark subdivision.


00:02:51,064 --> 00:02:53,339
Though the result might not 
always be what you want.


00:02:54,254 --> 00:02:57,884
VEX might not be an automatic 
win with regards to performance


00:02:57,884 --> 00:03:00,171
every single time over C++.


00:03:00,907 --> 00:03:04,179
But it's truly capable of matching 
the speed of C++


00:03:04,179 --> 00:03:06,730
in some real world production scenario,


00:03:06,730 --> 00:03:09,936
especially if you make use 
of faster algorithms.


00:03:11,019 --> 00:03:14,338
This concludes implementing 
Catmull-Clark in VEX.


00:03:15,232 --> 00:03:18,352
And with that said, 
see you guys in the next lesson.






----------------------------------------------------------------------------------------------------

129 - Caustics - Introduction

----------------------------------------------------------------------------------------------------



00:00:04,880 --> 00:00:11,620
In this lesson we will replicate 2 types of
caustics, sea caustics and pool caustics.


00:00:12,160 --> 00:00:15,280
The difference is mostly in the 
input surfacewe will create.


00:00:16,099 --> 00:00:19,545
In sea caustics the surface has 
a lot more detail and cusps


00:00:20,075 --> 00:00:22,788
that willbe reflected in the caustics pattern.


00:00:31,600 --> 00:00:35,309
In optics, a caustic or caustic network


00:00:35,309 --> 00:00:40,592
is theenvelope of light rays reflected 
or refracted by a curved surface


00:00:40,682 --> 00:00:45,210
or object, or the projectionof that 
envelope of rays on another surface.


00:00:46,710 --> 00:00:48,790
Let's see the concept of the algorithm first.


00:00:51,920 --> 00:00:57,130
Imagine a water surface like this. We
will have photons shooting from above,


00:00:57,450 --> 00:01:00,410
perfectly perpendicular to the 
sea or pool floor.


00:01:06,960 --> 00:01:09,365
After these photos hit the water surface,


00:01:10,164 --> 00:01:12,377
theywill be refracted through the medium


00:01:12,377 --> 00:01:16,103
and change their travel path and 
end up somewhere on thefloor.


00:01:17,192 --> 00:01:21,317
Influenced by the shape of the water 
and the way refraction works,


00:01:21,627 --> 00:01:25,925
some photons will beconcentrated 
in certain regions more than others.


00:01:26,664 --> 00:01:30,902
And this phenomenon is what creates 
thesebeautiful patterns of light


00:01:30,902 --> 00:01:32,150
moving with the water.


00:01:33,360 --> 00:01:35,468
There are a few things we will perform


00:01:35,468 --> 00:01:38,214
toincrease the quality and 
improve performance.


00:01:39,343 --> 00:01:43,383
1. First optimization we will perform 
isprecomputing the points


00:01:43,383 --> 00:01:47,513
on the water surface instead of 
ray tracing them from above.


00:01:48,053 --> 00:01:51,306
Thiswill give us some performance 
gains but also require us to


00:01:51,306 --> 00:01:53,163
perform additional optimizations


00:01:53,163 --> 00:01:56,000
we will see when we implement it in Houdini.


00:01:57,479 --> 00:02:01,010
2. Because we have a limited number of
points we will use to model photons,


00:02:01,680 --> 00:02:04,530
we need to avoid using an ordered 
formationof points


00:02:04,610 --> 00:02:06,320
such as that on a grid geometry,


00:02:06,880 --> 00:02:10,600
as those would easily be
noticed in our caustics pattern.


00:02:11,689 --> 00:02:15,121
3. We also need to avoid having 
holes in ourdistribution,


00:02:15,121 --> 00:02:17,393
as those too would easily be recognizable


00:02:17,393 --> 00:02:20,336
in the caustics pattern in
the shape of holes,


00:02:20,536 --> 00:02:22,040
sometimes magnified


00:02:22,480 --> 00:02:25,960
and sometimes shrinked based 
onthe shape of the water plane.


00:02:27,039 --> 00:02:29,861
4. We need to use uniform 
randomdistribution, 


00:02:29,861 --> 00:02:32,840
similar to how we relaxed points 
in a previous lesson


00:02:32,840 --> 00:02:35,840
but thistime we will use a built-in 
SOP for this.


00:02:37,889 --> 00:02:42,430
5. Then we will rasterize our
points i.e. photons into a volume.


00:02:43,949 --> 00:02:48,800
6. Lastly we will use a
few tricks like power, bias


00:02:48,800 --> 00:02:52,960
and sharpen the volumes, to enhance
the look of our caustics even more.


00:02:56,160 --> 00:02:58,792
Let's see how to implement this in Houdini now.






----------------------------------------------------------------------------------------------------

130 - Caustics - Sea Caustics

----------------------------------------------------------------------------------------------------



00:00:16,480 --> 00:00:18,880
We will first create the sea caustics effect.


00:00:20,800 --> 00:00:23,763
First create a grid of standard 10 by 10,


00:00:23,763 --> 00:00:28,080
with2 by 2 divisions in the 
default XZ orientation.


00:00:29,360 --> 00:00:34,800
Then apply Remesh to this. We will use
this node to create a triangular surface


00:00:34,800 --> 00:00:36,560
with fairly uniform distribution.


00:00:39,120 --> 00:00:41,840
Set target size to 0.01.


00:00:43,520 --> 00:00:46,640
This gives us a bit over 1.1M points.


00:00:50,880 --> 00:00:52,851
Now create a mountain SOP.


00:00:58,160 --> 00:01:02,173
As you can see, the cook time is
almost 1 second, which is quite slow.


00:01:04,080 --> 00:01:08,640
There is something we can do to speed
up this and the subsequent operations,


00:01:08,640 --> 00:01:12,695
and that is to convert the input
geometry into a polygon soup.


00:01:24,240 --> 00:01:28,108
As you can see, it now takes 
0.7seconds to cook,


00:01:28,185 --> 00:01:30,077
which is a significant improvement.


00:01:31,520 --> 00:01:35,974
Now create the ground plane that is 90
percent of the size of the photons plane,


00:01:41,840 --> 00:01:43,703
with 2 by 2 divisions.


00:01:55,120 --> 00:02:00,000
Create a wrangle node. We will implement
the refraction and raytracing here.


00:02:00,800 --> 00:02:05,440
Note that in this particular case,
where we have a flat ground plane,


00:02:05,440 --> 00:02:08,055
we can avoid using the intersect function


00:02:08,055 --> 00:02:11,120
and perform a simpler,
point plane projection


00:02:11,680 --> 00:02:14,400
as we have seen in the Camera
Based Occlusion chapter,


00:02:15,120 --> 00:02:17,989
but I am choosing to use the 
intersect function


00:02:17,989 --> 00:02:20,763
in case you want to use a 
non-planar ground plane.


00:02:23,120 --> 00:02:28,400
First we have to define the index parameter,
which is a relative index of refraction,


00:02:28,400 --> 00:02:34,400
meaning it's the index of refraction
of the outside medium divided by,


00:02:34,400 --> 00:02:37,351
the index of refraction of 
the inside medium.


00:02:38,985 --> 00:02:41,520
Our direction is negative Y.


00:02:49,677 --> 00:02:52,707
We will refract this direction 
using the surfacenormal,


00:02:52,707 --> 00:02:55,197
with our relative index of refraction.


00:03:03,440 --> 00:03:06,880
Then we intersect the second
input, which is our ground plane,


00:03:06,880 --> 00:03:08,586
by the refracted normal.


00:03:09,105 --> 00:03:11,852
We multiply this by alarge number,


00:03:12,621 --> 00:03:16,126
as the length of this vector 
is used for intersection,


00:03:16,261 --> 00:03:19,015
so we have to makesure it will 
reach our ground plane.


00:03:20,237 --> 00:03:24,317
If there is an intersection, then
we set the current point position.


00:03:28,560 --> 00:03:31,840
Create an Add SOP to delete
everything but the points.


00:03:32,720 --> 00:03:34,400
Let's see how it looks in the viewport.


00:03:35,840 --> 00:03:38,277
As you can see, everything looks flat.


00:03:38,277 --> 00:03:40,782
We need toelevate our water plane.


00:03:41,705 --> 00:03:43,921
I will use 3 meters in Y.


00:03:45,495 --> 00:03:50,135
Note that height of the water plain will
 greatlyaffect the result of the caustics.


00:03:51,315 --> 00:03:54,097
Just like how moving a 
magnifying glass towards


00:03:54,097 --> 00:03:57,315
oraway from a surface can 
change the light patterns.


00:03:58,800 --> 00:04:00,147
Create parameters.


00:04:01,666 --> 00:04:04,440
We have to setthe relative 
index of refraction to 1


00:04:04,440 --> 00:04:07,733
divided by 1.33


00:04:07,868 --> 00:04:12,071
because we are moving fromair, 
which has an index of refraction of 1,


00:04:12,480 --> 00:04:17,040
into water, which has an
index of refraction of 1.33.


00:04:18,160 --> 00:04:22,839
I am gonna change the visualization of
points to pixels to see it more clearly.


00:04:36,080 --> 00:04:38,820
We will first rasterize the 
points into a volume.


00:04:39,243 --> 00:04:40,785
So create a volume SOP.


00:04:42,901 --> 00:04:46,199
Set it to 2d, in ZX plane.


00:04:48,336 --> 00:04:50,416
Set resolution to 500 for now.


00:04:51,840 --> 00:04:55,623
There are many ways to 
rasterize pointsinto a volume.


00:04:55,623 --> 00:04:59,494
In my experience Volume 
Rasterize Particles SOP


00:04:59,494 --> 00:05:01,572
is the fastest method to do it.


00:05:01,572 --> 00:05:03,019
So we'll use that.


00:05:03,920 --> 00:05:08,628
Adjust Coverage Scale and 
ParticleScale to get a better result.


00:05:11,040 --> 00:05:13,176
Now we will adjust the noise parameters


00:05:13,291 --> 00:05:15,875
toconverge to a more 
plausible caustics result.


00:05:16,720 --> 00:05:18,458
Feel free to play with the settings.


00:05:19,680 --> 00:05:23,875
Increase the element size, 
andset noise type to Perlin.


00:05:24,394 --> 00:05:26,074
Still doesn't look like caustics yet.


00:05:26,720 --> 00:05:29,911
Reduce Roughness. 
Now we have something.


00:05:31,040 --> 00:05:36,080
I will animate the time. Animating
the time of a 4 dimensional noise


00:05:36,080 --> 00:05:40,355
yields much better results, than
offsetting a noise in 3d space,


00:05:40,432 --> 00:05:42,173
to emulate motion.


00:05:44,000 --> 00:05:48,400
Create a volume wrangle node to apply
power operation to the caustics volume.


00:05:49,440 --> 00:05:51,840
This effectively modifies the contrast.


00:06:10,000 --> 00:06:13,197
You can see how it drastically 
changesthe result.


00:06:16,428 --> 00:06:18,189
I will disable it for now.


00:06:19,194 --> 00:06:20,554
Create a volume VOP node.


00:06:21,600 --> 00:06:24,915
We will apply bias VOP. Unfortunately this


00:06:24,915 --> 00:06:26,941
function is not directly exposed to VEX,


00:06:27,210 --> 00:06:30,800
but rather you have to include 
a library to call it.


00:06:31,185 --> 00:06:33,458
So I am just gonna use the 
VOP version instead.


00:06:33,997 --> 00:06:36,810
This is shifting the values towards one end.


00:06:37,600 --> 00:06:41,760
Now we will sharpen the caustics
volume using an unsharp mask,


00:06:41,760 --> 00:06:44,515
the same method we have 
seen in a previous lesson.


00:06:46,000 --> 00:06:48,184
Turn on Use Voxel Radius.


00:06:48,511 --> 00:06:50,406
5 iterations should be enough.


00:06:50,733 --> 00:06:54,295
This yields muchbetter results 
than using large kernels.


00:06:55,040 --> 00:06:58,897
Now create another volume wrangle
to implement unsharp mask.


00:07:25,660 --> 00:07:29,725
With full sharpening you can see 
how itreally brings out more details.


00:07:30,918 --> 00:07:34,476
It's such a cheap trick but produces 
quite remarkableresults,


00:07:34,591 --> 00:07:36,835
almost like using higher res volumes.


00:07:37,600 --> 00:07:41,394
Speaking of high res, let's 
revise ourvolume resolution.


00:07:42,317 --> 00:07:44,155
We have it at 500,


00:07:44,155 --> 00:07:46,311
but we have way more point detail,


00:07:46,311 --> 00:07:48,121
than the volumeis able to capture.


00:07:48,467 --> 00:07:50,147
So I will just double it up.


00:07:51,280 --> 00:07:53,520
Nothing like really increasing
the resolution though.


00:07:54,400 --> 00:07:57,475
Now the caustics pattern 
looks really detailed.


00:08:07,875 --> 00:08:11,075
When we sharpen the caustics
volume after we upressed it,


00:08:11,776 --> 00:08:14,416
you can really see so much 
more detail coming out.


00:08:16,960 --> 00:08:21,077
I will also show you the difference 
between Coverage Scale vs Particle Scale. 


00:08:21,635 --> 00:08:25,077
You can strike a good balance 
using either in combination.


00:08:26,539 --> 00:08:30,369
But overallyou can think of 
Coverage Scale as intensity,


00:08:30,640 --> 00:08:33,663
and Particle Scale as radius of influence,


00:08:34,682 --> 00:08:39,011
sothe higher the Particle Scale, the 
more blurry your volume would look like.


00:08:40,396 --> 00:08:44,734
So to have sharp details, you want to
use the smallest Particle Scale possible,


00:08:45,000 --> 00:08:49,457
as well as the smallest Coverage Scale,
so as not to blow out the values.


00:08:51,120 --> 00:08:55,279
Play around with them to get a better
sense of how they influence each other.


00:09:54,400 --> 00:09:56,954
Let's create a flipbook to see it in action.


00:10:45,600 --> 00:10:49,173
I think the sea caustics both look 
and feel veryconvincing.


00:10:51,616 --> 00:10:53,934
We can now move onto pool caustics.






----------------------------------------------------------------------------------------------------

131 - Caustics - Pool Caustics

----------------------------------------------------------------------------------------------------



00:00:16,960 --> 00:00:20,080
Make a copy of it and call it pool caustics.


00:00:21,200 --> 00:00:24,640
It will be the same setup except
we will use a different noise.


00:00:28,480 --> 00:00:31,944
Delete the mountain node and
replace it with an Attrib VOP.


00:00:40,540 --> 00:00:43,465
Create Anti Aliased Noise with 4d input.


00:00:48,480 --> 00:00:51,520
We will connect time as the
4th component to the noise.


00:00:56,080 --> 00:00:58,959
Create a multiplier for time as speed.


00:01:24,921 --> 00:01:27,081
I will set speed to 0.6.


00:01:30,317 --> 00:01:31,437
Lower the frequency.


00:01:33,520 --> 00:01:34,480
Lower the amplitude.


00:01:37,440 --> 00:01:39,179
Reduce max octaves.


00:01:40,083 --> 00:01:42,452
Now we havesomething that 
looks like pool caustics.


00:01:43,600 --> 00:01:46,000
We can play with the Volume Rasterization.


00:02:05,840 --> 00:02:07,487
I will leave power at 1.


00:02:22,077 --> 00:02:24,477
You can see the sharpen really 
helps us here too.


00:02:28,720 --> 00:02:30,680
Maybe set power to 1.2.


00:02:35,840 --> 00:02:40,543
I think 0.6 might be a bit
fast. I will set it to 0.3.


00:02:41,760 --> 00:02:43,137
Let's create a flipbook.


00:02:53,680 --> 00:02:56,331
I think this looks like very
convincing pool caustics.


00:02:58,922 --> 00:03:03,165
One idea i can give you is to try 
using ocean spectrums,


00:03:03,374 --> 00:03:06,376
which can create nice and interesting 
caustic patterns.


00:03:07,026 --> 00:03:10,146
But again, you just have to 
make sure you experiment and


00:03:10,378 --> 00:03:12,913
try to convert to a plausible caustics effect.


00:03:15,702 --> 00:03:19,221
If the frequency is too large 
or the water plane is too high


00:03:19,221 --> 00:03:21,391
or you have so much roughness,


00:03:21,391 --> 00:03:24,498
you might not get the desired 
result that you are looking for.


00:03:25,887 --> 00:03:28,664
I hope you take the time and try 
to come up with your own


00:03:28,664 --> 00:03:30,214
interesting caustic patterns.


00:03:33,280 --> 00:03:36,243
And with that said, 
see youguys in the next lesson.






----------------------------------------------------------------------------------------------------

132 - Conclusion

----------------------------------------------------------------------------------------------------



00:00:04,095 --> 00:00:06,499
As all goods must come to an end,


00:00:06,797 --> 00:00:08,820
we have finally reached the end 
of this course.


00:00:09,533 --> 00:00:15,455
We have covered countless concepts,
ideas, tips, tricks and the mindset


00:00:15,455 --> 00:00:20,548
to solving production problems using
practical and visually pleasing examples.


00:00:21,071 --> 00:00:24,986
I hope you enjoyed the series as 
much as I enjoyed creating them,


00:00:25,293 --> 00:00:29,148
and hopefully you are better now, 
than before you started this course.


00:00:30,000 --> 00:00:32,700
And as always thanks for watching.